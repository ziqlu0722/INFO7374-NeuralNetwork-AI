{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_GLOVE.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"Pbw_PwyYjJO1","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","import json\n","import numpy as np\n","from random import randrange\n","import zipfile"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Rsx9iwfj4Ak3","colab_type":"text"},"cell_type":"markdown","source":["## Experiment 1 - Eearnings Transcript - GLOVE\n"]},{"metadata":{"id":"CyWVTaPs2qJm","colab_type":"text"},"cell_type":"markdown","source":["###1.Preprocess Data"]},{"metadata":{"id":"d7wBjICwjQVq","colab_type":"code","colab":{}},"cell_type":"code","source":["def load_data(link):\n","  \n","  text = []\n","  sentiment = []\n","\n","  for filename in os.listdir(link):\n","    if filename != '.ipynb_checkpoints':\n","      file_path = os.path.join(link, filename)\n","      with open(file_path, encoding = 'unicode_escape') as f:\n","        dic = json.load(f)\n","\n","        for k, v in dic['text'].items():\n","          text.append(v)\n","        for k, v in dic['sentiment'].items():\n","          sentiment.append(v)\n","  return text, sentiment"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3dJEd10c37c_","colab_type":"code","colab":{}},"cell_type":"code","source":["def preprocess_data(text, sentiment):\n","  # 80-20 split\n","  combo = [list(i) for i in zip(text, sentiment)]\n","  \n","  combo_test = []\n","\n","  test_size = int(len(combo)*0.2)\n","  while len(combo_test) < test_size:\n","    index = randrange(len(combo))\n","    combo_test.append(combo.pop(index))\n","  \n","  x_train = []\n","  y_train = []\n","  x_test = []\n","  y_test = []\n","  # x, y split\n","  for line in combo:\n","    x_train.append(line[0])\n","    if line[1] in ['Positive', 'positive', 'postive']:\n","      y_train.append(2)\n","    elif line[1] in ['Neutral', 'neutral', 'neutra', 'neutra;', 'Neural']:\n","      y_train.append(1)\n","    elif line[1] in ['Negative', 'negative', 'Negetive']:\n","      y_train.append(0)\n","    else:\n","      print('error in sentiment label {}'.format(line[1]))\n","  for line in combo_test:\n","    x_test.append(line[0])\n","    if line[1] in ['Positive', 'positive', 'postive']:\n","      y_test.append(2)\n","    elif line[1] in ['Neutral', 'neutral', 'neutra', 'neutra;', 'Neural']:\n","      y_test.append(1)\n","    elif line[1] in ['Negative', 'negative', 'Negetive']:\n","      y_test.append(0)\n","    else:\n","      print('error in sentiment label {}'.format(line[1]))\n","      \n","  return (x_train, y_train), (x_test, y_test)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qufGKe6S204Z","colab_type":"code","colab":{}},"cell_type":"code","source":["link = 'drive/INFO7374_NeuralNetwork&AI/Assignment_3/updated-json-files'\n","text, sentiment = load_data(link)\n","(x_train, y_train), (x_test, y_test) = preprocess_data(text, sentiment)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5YZyfWNbd_hA","colab_type":"code","colab":{}},"cell_type":"code","source":["# convert to one-hot encoding\n","\n","from keras.utils import to_categorical\n","\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XuMk60uocOoF","colab_type":"code","outputId":"dd7d11d3-0795-4fe7-ad4a-82d7196d8868","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["len(x_train), len(x_test)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1316, 328)"]},"metadata":{"tags":[]},"execution_count":46}]},{"metadata":{"id":"E0U1ew2K-L2h","colab_type":"text"},"cell_type":"markdown","source":["### 2. Tokenize The Data"]},{"metadata":{"id":"G5vf1_Cv4V_M","colab_type":"code","outputId":"eb6119c0-4422-40e1-acaa-9895c0b72eb4","colab":{"base_uri":"https://localhost:8080/","height":68}},"cell_type":"code","source":["from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","\n","maxlen = 100  # We will cut reviews after 100 words\n","training_samples = 1000  # We will be training on 1000 samples\n","validation_samples = 316  # We will be validating on 316 samples\n","max_words = 10000  # We will only consider the top 10,000 words in the dataset\n","\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(x_train)\n","sequences = tokenizer.texts_to_sequences(x_train)\n","\n","sequences_test = tokenizer.texts_to_sequences(x_test)\n","x_test = pad_sequences(sequences_test, maxlen=maxlen)\n","y_test = np.asarray(y_test)\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","data = pad_sequences(sequences, maxlen=maxlen)\n","\n","labels = np.asarray(y_train)\n","print('Shape of data tensor:', data.shape)\n","print('Shape of label tensor:', labels.shape)\n","\n","# Split the data into a training set and a validation set\n","# But first, shuffle the data, since we started from data\n","# where sample are ordered (all negative first, then all positive).\n","indices = np.arange(data.shape[0])\n","np.random.shuffle(indices)\n","data = data[indices]\n","labels = labels[indices]\n","\n","x_train = data[:training_samples]\n","y_train = labels[:training_samples]\n","x_val = data[training_samples: training_samples + validation_samples]\n","y_val = labels[training_samples: training_samples + validation_samples]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Found 5794 unique tokens.\n","Shape of data tensor: (1316, 100)\n","Shape of label tensor: (1316, 3)\n"],"name":"stdout"}]},{"metadata":{"id":"01vPHPTXk9PF","colab_type":"text"},"cell_type":"markdown","source":["### Training with Freezed Embedding Layer"]},{"metadata":{"id":"sQWBwuvdcAM4","colab_type":"code","colab":{}},"cell_type":"code","source":["with zipfile.ZipFile('drive/INFO7374_NeuralNetwork&AI/Assignment_3/aclImdb.zip','r') as zip_ref:\n","    zip_ref.extractall('')\n","    \n","with zipfile.ZipFile('drive/INFO7374_NeuralNetwork&AI/Assignment_3/glove.6B.zip','r') as zip_ref:\n","    zip_ref.extractall('')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TnBC4QLh4V9E","colab_type":"code","outputId":"e344f320-3ce3-4b0e-be05-0b61521fc616","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["embeddings_index = {}\n","\n","with open('glove.6B.100d.txt') as f:\n","  for line in f:\n","      values = line.split()\n","      word = values[0]\n","      coefs = np.asarray(values[1:], dtype='float32')\n","      embeddings_index[word] = coefs\n","\n","print('Found %s word vectors.' % len(embeddings_index))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Found 400000 word vectors.\n"],"name":"stdout"}]},{"metadata":{"id":"_Aj2dxeEcuVN","colab_type":"code","colab":{}},"cell_type":"code","source":["embedding_dim = 100\n","\n","embedding_matrix = np.zeros((max_words, embedding_dim))\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if i < max_words:\n","        if embedding_vector is not None:\n","            # Words not found in embedding index will be all-zeros.\n","            embedding_matrix[i] = embedding_vector"],"execution_count":0,"outputs":[]},{"metadata":{"id":"c4pKjY5T4V6r","colab_type":"code","outputId":"60be706b-a827-4719-9e2f-7ee087131c48","colab":{"base_uri":"https://localhost:8080/","height":272}},"cell_type":"code","source":["from keras import optimizers\n","from keras.models import Sequential\n","from keras.layers import Embedding, Flatten, Dense\n","\n","model = Sequential()\n","model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n","model.add(Flatten())\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(3, activation='softmax'))\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_3 (Embedding)      (None, 100, 100)          1000000   \n","_________________________________________________________________\n","flatten_3 (Flatten)          (None, 10000)             0         \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 32)                320032    \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 3)                 99        \n","=================================================================\n","Total params: 1,320,131\n","Trainable params: 1,320,131\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"UlVWMhly4Vtg","colab_type":"code","colab":{}},"cell_type":"code","source":["model.layers[0].set_weights([embedding_matrix])\n","model.layers[0].trainable = False"],"execution_count":0,"outputs":[]},{"metadata":{"id":"glEFtKOGdMFI","colab_type":"code","outputId":"4c5fadef-6490-4d9f-af20-95d28c79e3cf","colab":{"base_uri":"https://localhost:8080/","height":6834}},"cell_type":"code","source":["optimizer = optimizers.SGD(lr=0.01, nesterov=True)\n","loss = 'categorical_crossentropy'\n","epochs = 200\n","batch_size = 32\n","\n","model.compile(optimizer= optimizer,\n","              loss = loss,\n","              metrics=['acc'])\n","\n","history_1 = model.fit(x_train, y_train,\n","                    epochs=epochs,\n","                    batch_size=batch_size,\n","                    validation_data=(x_val, y_val))\n","\n","model.save_weights('model_1.h5')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 1000 samples, validate on 316 samples\n","Epoch 1/200\n","1000/1000 [==============================] - 1s 612us/step - loss: 0.9729 - acc: 0.5610 - val_loss: 0.8864 - val_acc: 0.6108\n","Epoch 2/200\n","1000/1000 [==============================] - 0s 158us/step - loss: 0.8245 - acc: 0.6460 - val_loss: 0.8727 - val_acc: 0.6139\n","Epoch 3/200\n","1000/1000 [==============================] - 0s 156us/step - loss: 0.7355 - acc: 0.6990 - val_loss: 0.8478 - val_acc: 0.5949\n","Epoch 4/200\n","1000/1000 [==============================] - 0s 171us/step - loss: 0.6608 - acc: 0.7420 - val_loss: 1.0168 - val_acc: 0.6171\n","Epoch 5/200\n","1000/1000 [==============================] - 0s 197us/step - loss: 0.5787 - acc: 0.7910 - val_loss: 0.8436 - val_acc: 0.5981\n","Epoch 6/200\n","1000/1000 [==============================] - 0s 158us/step - loss: 0.5071 - acc: 0.8250 - val_loss: 0.9991 - val_acc: 0.5380\n","Epoch 7/200\n","1000/1000 [==============================] - 0s 148us/step - loss: 0.4516 - acc: 0.8610 - val_loss: 0.8979 - val_acc: 0.5538\n","Epoch 8/200\n","1000/1000 [==============================] - 0s 131us/step - loss: 0.3766 - acc: 0.9090 - val_loss: 0.9196 - val_acc: 0.6234\n","Epoch 9/200\n","1000/1000 [==============================] - 0s 161us/step - loss: 0.3340 - acc: 0.9170 - val_loss: 0.9141 - val_acc: 0.5759\n","Epoch 10/200\n","1000/1000 [==============================] - 0s 213us/step - loss: 0.3074 - acc: 0.9300 - val_loss: 0.8717 - val_acc: 0.6139\n","Epoch 11/200\n","1000/1000 [==============================] - 0s 164us/step - loss: 0.2637 - acc: 0.9510 - val_loss: 0.8761 - val_acc: 0.6234\n","Epoch 12/200\n","1000/1000 [==============================] - 0s 164us/step - loss: 0.2141 - acc: 0.9710 - val_loss: 1.0006 - val_acc: 0.6297\n","Epoch 13/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 0.1908 - acc: 0.9730 - val_loss: 0.9139 - val_acc: 0.6234\n","Epoch 14/200\n","1000/1000 [==============================] - 0s 165us/step - loss: 0.1683 - acc: 0.9780 - val_loss: 1.0477 - val_acc: 0.5665\n","Epoch 15/200\n","1000/1000 [==============================] - 0s 180us/step - loss: 0.1570 - acc: 0.9760 - val_loss: 1.0513 - val_acc: 0.6266\n","Epoch 16/200\n","1000/1000 [==============================] - 0s 178us/step - loss: 0.1399 - acc: 0.9820 - val_loss: 0.9335 - val_acc: 0.6171\n","Epoch 17/200\n","1000/1000 [==============================] - 0s 164us/step - loss: 0.1247 - acc: 0.9840 - val_loss: 0.9373 - val_acc: 0.6076\n","Epoch 18/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 0.1136 - acc: 0.9850 - val_loss: 0.9348 - val_acc: 0.6329\n","Epoch 19/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 0.1049 - acc: 0.9870 - val_loss: 0.9481 - val_acc: 0.6234\n","Epoch 20/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 0.0964 - acc: 0.9900 - val_loss: 0.9621 - val_acc: 0.6266\n","Epoch 21/200\n","1000/1000 [==============================] - 0s 170us/step - loss: 0.0890 - acc: 0.9900 - val_loss: 0.9932 - val_acc: 0.6234\n","Epoch 22/200\n","1000/1000 [==============================] - 0s 180us/step - loss: 0.0854 - acc: 0.9900 - val_loss: 0.9807 - val_acc: 0.6297\n","Epoch 23/200\n","1000/1000 [==============================] - 0s 193us/step - loss: 0.0807 - acc: 0.9890 - val_loss: 1.0279 - val_acc: 0.6297\n","Epoch 24/200\n","1000/1000 [==============================] - 0s 168us/step - loss: 0.0750 - acc: 0.9900 - val_loss: 1.5928 - val_acc: 0.6234\n","Epoch 25/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 0.0883 - acc: 0.9840 - val_loss: 1.0048 - val_acc: 0.6266\n","Epoch 26/200\n","1000/1000 [==============================] - 0s 168us/step - loss: 0.0695 - acc: 0.9900 - val_loss: 1.0087 - val_acc: 0.6361\n","Epoch 27/200\n","1000/1000 [==============================] - 0s 166us/step - loss: 0.0650 - acc: 0.9900 - val_loss: 1.0035 - val_acc: 0.6361\n","Epoch 28/200\n","1000/1000 [==============================] - 0s 184us/step - loss: 0.0623 - acc: 0.9890 - val_loss: 1.0120 - val_acc: 0.6139\n","Epoch 29/200\n","1000/1000 [==============================] - 0s 168us/step - loss: 0.0592 - acc: 0.9890 - val_loss: 1.0515 - val_acc: 0.6171\n","Epoch 30/200\n","1000/1000 [==============================] - 0s 171us/step - loss: 0.0576 - acc: 0.9900 - val_loss: 1.0475 - val_acc: 0.6297\n","Epoch 31/200\n","1000/1000 [==============================] - 0s 185us/step - loss: 0.0549 - acc: 0.9900 - val_loss: 1.0741 - val_acc: 0.6329\n","Epoch 32/200\n","1000/1000 [==============================] - 0s 165us/step - loss: 0.0524 - acc: 0.9900 - val_loss: 1.0648 - val_acc: 0.6234\n","Epoch 33/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 0.0505 - acc: 0.9920 - val_loss: 1.0545 - val_acc: 0.6297\n","Epoch 34/200\n","1000/1000 [==============================] - 0s 174us/step - loss: 0.0482 - acc: 0.9930 - val_loss: 1.1155 - val_acc: 0.6171\n","Epoch 35/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 0.0482 - acc: 0.9910 - val_loss: 1.0549 - val_acc: 0.6234\n","Epoch 36/200\n","1000/1000 [==============================] - 0s 179us/step - loss: 0.0456 - acc: 0.9910 - val_loss: 1.0800 - val_acc: 0.6297\n","Epoch 37/200\n","1000/1000 [==============================] - 0s 171us/step - loss: 0.0441 - acc: 0.9920 - val_loss: 1.0796 - val_acc: 0.6329\n","Epoch 38/200\n","1000/1000 [==============================] - 0s 159us/step - loss: 0.0430 - acc: 0.9910 - val_loss: 1.6429 - val_acc: 0.5506\n","Epoch 39/200\n","1000/1000 [==============================] - 0s 154us/step - loss: 0.0573 - acc: 0.9860 - val_loss: 1.0920 - val_acc: 0.6329\n","Epoch 40/200\n","1000/1000 [==============================] - 0s 173us/step - loss: 0.0420 - acc: 0.9930 - val_loss: 1.1616 - val_acc: 0.6234\n","Epoch 41/200\n","1000/1000 [==============================] - 0s 181us/step - loss: 0.0408 - acc: 0.9930 - val_loss: 1.1191 - val_acc: 0.6266\n","Epoch 42/200\n","1000/1000 [==============================] - 0s 165us/step - loss: 0.0385 - acc: 0.9920 - val_loss: 1.0931 - val_acc: 0.6329\n","Epoch 43/200\n","1000/1000 [==============================] - 0s 156us/step - loss: 0.0376 - acc: 0.9920 - val_loss: 1.1132 - val_acc: 0.6297\n","Epoch 44/200\n","1000/1000 [==============================] - 0s 166us/step - loss: 0.0363 - acc: 0.9920 - val_loss: 1.1133 - val_acc: 0.6266\n","Epoch 45/200\n","1000/1000 [==============================] - 0s 161us/step - loss: 0.0350 - acc: 0.9930 - val_loss: 1.1599 - val_acc: 0.6266\n","Epoch 46/200\n","1000/1000 [==============================] - 0s 168us/step - loss: 0.0341 - acc: 0.9940 - val_loss: 1.1280 - val_acc: 0.6266\n","Epoch 47/200\n","1000/1000 [==============================] - 0s 187us/step - loss: 0.0335 - acc: 0.9930 - val_loss: 1.1597 - val_acc: 0.6203\n","Epoch 48/200\n","1000/1000 [==============================] - 0s 169us/step - loss: 0.0326 - acc: 0.9950 - val_loss: 1.1567 - val_acc: 0.6203\n","Epoch 49/200\n","1000/1000 [==============================] - 0s 165us/step - loss: 0.0324 - acc: 0.9940 - val_loss: 1.1338 - val_acc: 0.6297\n","Epoch 50/200\n","1000/1000 [==============================] - 0s 167us/step - loss: 0.0304 - acc: 0.9950 - val_loss: 1.1419 - val_acc: 0.6297\n","Epoch 51/200\n","1000/1000 [==============================] - 0s 161us/step - loss: 0.0309 - acc: 0.9940 - val_loss: 1.1579 - val_acc: 0.6203\n","Epoch 52/200\n","1000/1000 [==============================] - 0s 178us/step - loss: 0.0299 - acc: 0.9940 - val_loss: 1.1804 - val_acc: 0.6203\n","Epoch 53/200\n","1000/1000 [==============================] - 0s 169us/step - loss: 0.0294 - acc: 0.9930 - val_loss: 1.1609 - val_acc: 0.6297\n","Epoch 54/200\n","1000/1000 [==============================] - 0s 163us/step - loss: 0.0291 - acc: 0.9940 - val_loss: 1.1812 - val_acc: 0.6171\n","Epoch 55/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 0.0287 - acc: 0.9930 - val_loss: 1.1792 - val_acc: 0.6171\n","Epoch 56/200\n","1000/1000 [==============================] - 0s 158us/step - loss: 0.0274 - acc: 0.9950 - val_loss: 1.1609 - val_acc: 0.6329\n","Epoch 57/200\n","1000/1000 [==============================] - 0s 179us/step - loss: 0.0273 - acc: 0.9940 - val_loss: 1.1603 - val_acc: 0.6266\n","Epoch 58/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 0.0266 - acc: 0.9950 - val_loss: 1.2081 - val_acc: 0.6171\n","Epoch 59/200\n","1000/1000 [==============================] - 0s 177us/step - loss: 0.0264 - acc: 0.9930 - val_loss: 1.1986 - val_acc: 0.6171\n","Epoch 60/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 0.0255 - acc: 0.9950 - val_loss: 1.1821 - val_acc: 0.6266\n","Epoch 61/200\n","1000/1000 [==============================] - 0s 167us/step - loss: 0.0250 - acc: 0.9940 - val_loss: 1.8230 - val_acc: 0.5506\n","Epoch 62/200\n","1000/1000 [==============================] - 0s 167us/step - loss: 0.0486 - acc: 0.9860 - val_loss: 1.1929 - val_acc: 0.6266\n","Epoch 63/200\n","1000/1000 [==============================] - 0s 183us/step - loss: 0.0259 - acc: 0.9950 - val_loss: 1.2394 - val_acc: 0.6139\n","Epoch 64/200\n","1000/1000 [==============================] - 0s 163us/step - loss: 0.0258 - acc: 0.9940 - val_loss: 1.2056 - val_acc: 0.6266\n","Epoch 65/200\n","1000/1000 [==============================] - 0s 176us/step - loss: 0.0241 - acc: 0.9940 - val_loss: 1.2144 - val_acc: 0.6266\n","Epoch 66/200\n","1000/1000 [==============================] - 0s 164us/step - loss: 0.0237 - acc: 0.9940 - val_loss: 1.2465 - val_acc: 0.6108\n","Epoch 67/200\n","1000/1000 [==============================] - 0s 167us/step - loss: 0.0229 - acc: 0.9950 - val_loss: 1.2186 - val_acc: 0.6234\n","Epoch 68/200\n","1000/1000 [==============================] - 0s 182us/step - loss: 0.0227 - acc: 0.9950 - val_loss: 1.2119 - val_acc: 0.6234\n","Epoch 69/200\n","1000/1000 [==============================] - 0s 159us/step - loss: 0.0222 - acc: 0.9930 - val_loss: 1.3052 - val_acc: 0.6266\n","Epoch 70/200\n","1000/1000 [==============================] - 0s 160us/step - loss: 0.0212 - acc: 0.9950 - val_loss: 1.2243 - val_acc: 0.6266\n","Epoch 71/200\n","1000/1000 [==============================] - 0s 181us/step - loss: 0.0215 - acc: 0.9950 - val_loss: 1.2217 - val_acc: 0.6234\n","Epoch 72/200\n","1000/1000 [==============================] - 0s 167us/step - loss: 0.0214 - acc: 0.9950 - val_loss: 1.2101 - val_acc: 0.6361\n","Epoch 73/200\n","1000/1000 [==============================] - 0s 182us/step - loss: 0.0219 - acc: 0.9960 - val_loss: 1.2445 - val_acc: 0.6203\n","Epoch 74/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 0.0213 - acc: 0.9950 - val_loss: 1.2242 - val_acc: 0.6203\n","Epoch 75/200\n","1000/1000 [==============================] - 0s 164us/step - loss: 0.0205 - acc: 0.9950 - val_loss: 1.2265 - val_acc: 0.6392\n","Epoch 76/200\n","1000/1000 [==============================] - 0s 164us/step - loss: 0.0198 - acc: 0.9970 - val_loss: 1.3091 - val_acc: 0.6266\n","Epoch 77/200\n","1000/1000 [==============================] - 0s 176us/step - loss: 0.0207 - acc: 0.9950 - val_loss: 1.2730 - val_acc: 0.6203\n","Epoch 78/200\n","1000/1000 [==============================] - 0s 174us/step - loss: 0.0185 - acc: 0.9970 - val_loss: 1.2471 - val_acc: 0.6266\n","Epoch 79/200\n","1000/1000 [==============================] - 0s 166us/step - loss: 0.0191 - acc: 0.9970 - val_loss: 1.3033 - val_acc: 0.6203\n","Epoch 80/200\n","1000/1000 [==============================] - 0s 165us/step - loss: 0.0190 - acc: 0.9960 - val_loss: 1.2612 - val_acc: 0.6203\n","Epoch 81/200\n","1000/1000 [==============================] - 0s 164us/step - loss: 0.0188 - acc: 0.9970 - val_loss: 1.2831 - val_acc: 0.6171\n","Epoch 82/200\n","1000/1000 [==============================] - 0s 165us/step - loss: 0.0189 - acc: 0.9960 - val_loss: 1.2682 - val_acc: 0.6203\n","Epoch 83/200\n","1000/1000 [==============================] - 0s 183us/step - loss: 0.0182 - acc: 0.9970 - val_loss: 1.2619 - val_acc: 0.6203\n","Epoch 84/200\n","1000/1000 [==============================] - 0s 172us/step - loss: 0.0184 - acc: 0.9960 - val_loss: 1.2801 - val_acc: 0.6171\n","Epoch 85/200\n","1000/1000 [==============================] - 0s 165us/step - loss: 0.0183 - acc: 0.9950 - val_loss: 1.2573 - val_acc: 0.6297\n","Epoch 86/200\n","1000/1000 [==============================] - 0s 164us/step - loss: 0.0180 - acc: 0.9960 - val_loss: 1.2558 - val_acc: 0.6329\n","Epoch 87/200\n","1000/1000 [==============================] - 0s 161us/step - loss: 0.0171 - acc: 0.9970 - val_loss: 1.3164 - val_acc: 0.6203\n","Epoch 88/200\n","1000/1000 [==============================] - 0s 167us/step - loss: 0.0176 - acc: 0.9950 - val_loss: 1.3172 - val_acc: 0.6234\n","Epoch 89/200\n","1000/1000 [==============================] - 0s 209us/step - loss: 0.0164 - acc: 0.9970 - val_loss: 1.2707 - val_acc: 0.6297\n","Epoch 90/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 0.0171 - acc: 0.9950 - val_loss: 1.2651 - val_acc: 0.6424\n","Epoch 91/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 0.0168 - acc: 0.9960 - val_loss: 1.2974 - val_acc: 0.6234\n","Epoch 92/200\n","1000/1000 [==============================] - 0s 160us/step - loss: 0.0172 - acc: 0.9950 - val_loss: 1.3047 - val_acc: 0.6139\n","Epoch 93/200\n","1000/1000 [==============================] - 0s 163us/step - loss: 0.0168 - acc: 0.9960 - val_loss: 1.3232 - val_acc: 0.6171\n","Epoch 94/200\n","1000/1000 [==============================] - 0s 196us/step - loss: 0.0162 - acc: 0.9960 - val_loss: 1.2892 - val_acc: 0.6266\n","Epoch 95/200\n","1000/1000 [==============================] - 0s 196us/step - loss: 0.0160 - acc: 0.9970 - val_loss: 1.3478 - val_acc: 0.6234\n","Epoch 96/200\n","1000/1000 [==============================] - 0s 170us/step - loss: 0.0169 - acc: 0.9960 - val_loss: 1.3262 - val_acc: 0.6108\n","Epoch 97/200\n","1000/1000 [==============================] - 0s 163us/step - loss: 0.0168 - acc: 0.9960 - val_loss: 1.3127 - val_acc: 0.6203\n","Epoch 98/200\n","1000/1000 [==============================] - 0s 164us/step - loss: 0.0163 - acc: 0.9950 - val_loss: 1.3187 - val_acc: 0.6234\n","Epoch 99/200\n","1000/1000 [==============================] - 0s 172us/step - loss: 0.0165 - acc: 0.9950 - val_loss: 1.3259 - val_acc: 0.6139\n","Epoch 100/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 0.0153 - acc: 0.9960 - val_loss: 1.3033 - val_acc: 0.6266\n","Epoch 101/200\n","1000/1000 [==============================] - 0s 181us/step - loss: 0.0158 - acc: 0.9950 - val_loss: 1.3152 - val_acc: 0.6234\n","Epoch 102/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 0.0148 - acc: 0.9960 - val_loss: 1.4360 - val_acc: 0.6266\n","Epoch 103/200\n","1000/1000 [==============================] - 0s 157us/step - loss: 0.0159 - acc: 0.9950 - val_loss: 1.3458 - val_acc: 0.6171\n","Epoch 104/200\n","1000/1000 [==============================] - 0s 168us/step - loss: 0.0139 - acc: 0.9960 - val_loss: 1.3189 - val_acc: 0.6266\n","Epoch 105/200\n","1000/1000 [==============================] - 0s 166us/step - loss: 0.0147 - acc: 0.9960 - val_loss: 1.3030 - val_acc: 0.6424\n","Epoch 106/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 0.0141 - acc: 0.9970 - val_loss: 1.3384 - val_acc: 0.6171\n","Epoch 107/200\n","1000/1000 [==============================] - 0s 179us/step - loss: 0.0143 - acc: 0.9980 - val_loss: 1.3239 - val_acc: 0.6203\n","Epoch 108/200\n","1000/1000 [==============================] - 0s 165us/step - loss: 0.0145 - acc: 0.9960 - val_loss: 1.3546 - val_acc: 0.6171\n","Epoch 109/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 0.0138 - acc: 0.9980 - val_loss: 1.3215 - val_acc: 0.6361\n","Epoch 110/200\n","1000/1000 [==============================] - 0s 160us/step - loss: 0.0136 - acc: 0.9960 - val_loss: 1.3563 - val_acc: 0.6171\n","Epoch 111/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 0.0148 - acc: 0.9970 - val_loss: 1.3963 - val_acc: 0.6203\n","Epoch 112/200\n","1000/1000 [==============================] - 0s 168us/step - loss: 0.0138 - acc: 0.9980 - val_loss: 1.3237 - val_acc: 0.6329\n","Epoch 113/200\n","1000/1000 [==============================] - 0s 181us/step - loss: 0.0136 - acc: 0.9970 - val_loss: 1.3788 - val_acc: 0.6203\n","Epoch 114/200\n","1000/1000 [==============================] - 0s 164us/step - loss: 0.0142 - acc: 0.9970 - val_loss: 1.4000 - val_acc: 0.6171\n","Epoch 115/200\n","1000/1000 [==============================] - 0s 178us/step - loss: 0.0142 - acc: 0.9960 - val_loss: 1.4069 - val_acc: 0.6234\n","Epoch 116/200\n","1000/1000 [==============================] - 0s 166us/step - loss: 0.0126 - acc: 0.9970 - val_loss: 1.3463 - val_acc: 0.6266\n","Epoch 117/200\n","1000/1000 [==============================] - 0s 167us/step - loss: 0.0130 - acc: 0.9980 - val_loss: 1.4685 - val_acc: 0.6297\n","Epoch 118/200\n","1000/1000 [==============================] - 0s 161us/step - loss: 0.0132 - acc: 0.9980 - val_loss: 1.3380 - val_acc: 0.6297\n","Epoch 119/200\n","1000/1000 [==============================] - 0s 190us/step - loss: 0.0121 - acc: 0.9980 - val_loss: 1.4182 - val_acc: 0.6297\n","Epoch 120/200\n","1000/1000 [==============================] - 0s 192us/step - loss: 0.0158 - acc: 0.9970 - val_loss: 1.4346 - val_acc: 0.6234\n","Epoch 121/200\n","1000/1000 [==============================] - 0s 178us/step - loss: 0.0133 - acc: 0.9980 - val_loss: 1.3462 - val_acc: 0.6234\n","Epoch 122/200\n","1000/1000 [==============================] - 0s 171us/step - loss: 0.0140 - acc: 0.9960 - val_loss: 1.3585 - val_acc: 0.6266\n","Epoch 123/200\n","1000/1000 [==============================] - 0s 168us/step - loss: 0.0134 - acc: 0.9960 - val_loss: 1.3448 - val_acc: 0.6392\n","Epoch 124/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 0.0128 - acc: 0.9980 - val_loss: 1.3796 - val_acc: 0.6171\n","Epoch 125/200\n","1000/1000 [==============================] - 0s 209us/step - loss: 0.0124 - acc: 0.9980 - val_loss: 1.3435 - val_acc: 0.6361\n","Epoch 126/200\n","1000/1000 [==============================] - 0s 168us/step - loss: 0.0127 - acc: 0.9970 - val_loss: 1.3994 - val_acc: 0.6139\n","Epoch 127/200\n","1000/1000 [==============================] - 0s 166us/step - loss: 0.0133 - acc: 0.9970 - val_loss: 1.4220 - val_acc: 0.6171\n","Epoch 128/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 0.0135 - acc: 0.9960 - val_loss: 1.4123 - val_acc: 0.6203\n","Epoch 129/200\n","1000/1000 [==============================] - 0s 161us/step - loss: 0.0134 - acc: 0.9970 - val_loss: 1.3966 - val_acc: 0.6171\n","Epoch 130/200\n","1000/1000 [==============================] - 0s 172us/step - loss: 0.0121 - acc: 0.9970 - val_loss: 1.3532 - val_acc: 0.6392\n","Epoch 131/200\n","1000/1000 [==============================] - 0s 182us/step - loss: 0.0121 - acc: 0.9980 - val_loss: 1.4604 - val_acc: 0.6266\n","Epoch 132/200\n","1000/1000 [==============================] - 0s 169us/step - loss: 0.0134 - acc: 0.9960 - val_loss: 1.4421 - val_acc: 0.6139\n","Epoch 133/200\n","1000/1000 [==============================] - 0s 165us/step - loss: 0.0126 - acc: 0.9970 - val_loss: 1.5096 - val_acc: 0.6234\n","Epoch 134/200\n","1000/1000 [==============================] - 0s 164us/step - loss: 0.0139 - acc: 0.9960 - val_loss: 1.4044 - val_acc: 0.6171\n","Epoch 135/200\n","1000/1000 [==============================] - 0s 168us/step - loss: 0.0113 - acc: 0.9970 - val_loss: 1.3721 - val_acc: 0.6297\n","Epoch 136/200\n","1000/1000 [==============================] - 0s 171us/step - loss: 0.0111 - acc: 0.9980 - val_loss: 1.4614 - val_acc: 0.6234\n","Epoch 137/200\n","1000/1000 [==============================] - 0s 181us/step - loss: 0.0132 - acc: 0.9960 - val_loss: 1.4181 - val_acc: 0.6203\n","Epoch 138/200\n","1000/1000 [==============================] - 0s 171us/step - loss: 0.0130 - acc: 0.9960 - val_loss: 1.4106 - val_acc: 0.6139\n","Epoch 139/200\n","1000/1000 [==============================] - 0s 161us/step - loss: 0.0114 - acc: 0.9970 - val_loss: 1.3842 - val_acc: 0.6329\n","Epoch 140/200\n","1000/1000 [==============================] - 0s 172us/step - loss: 0.0105 - acc: 0.9980 - val_loss: 1.5258 - val_acc: 0.6234\n","Epoch 141/200\n","1000/1000 [==============================] - 0s 163us/step - loss: 0.0114 - acc: 0.9980 - val_loss: 1.3882 - val_acc: 0.6234\n","Epoch 142/200\n","1000/1000 [==============================] - 0s 166us/step - loss: 0.0129 - acc: 0.9960 - val_loss: 1.3831 - val_acc: 0.6361\n","Epoch 143/200\n","1000/1000 [==============================] - 0s 185us/step - loss: 0.0110 - acc: 0.9980 - val_loss: 1.4531 - val_acc: 0.6108\n","Epoch 144/200\n","1000/1000 [==============================] - 0s 163us/step - loss: 0.0121 - acc: 0.9970 - val_loss: 1.5256 - val_acc: 0.6234\n","Epoch 145/200\n","1000/1000 [==============================] - 0s 165us/step - loss: 0.0131 - acc: 0.9970 - val_loss: 1.4286 - val_acc: 0.6171\n","Epoch 146/200\n","1000/1000 [==============================] - 0s 166us/step - loss: 0.0118 - acc: 0.9970 - val_loss: 1.4624 - val_acc: 0.6139\n","Epoch 147/200\n","1000/1000 [==============================] - 0s 163us/step - loss: 0.0106 - acc: 0.9970 - val_loss: 1.3918 - val_acc: 0.6329\n","Epoch 148/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 0.0106 - acc: 0.9980 - val_loss: 1.4335 - val_acc: 0.6171\n","Epoch 149/200\n","1000/1000 [==============================] - 0s 175us/step - loss: 0.0122 - acc: 0.9970 - val_loss: 1.4481 - val_acc: 0.6203\n","Epoch 150/200\n","1000/1000 [==============================] - 0s 159us/step - loss: 0.0109 - acc: 0.9980 - val_loss: 1.4113 - val_acc: 0.6297\n","Epoch 151/200\n","1000/1000 [==============================] - 0s 172us/step - loss: 0.0116 - acc: 0.9960 - val_loss: 1.3987 - val_acc: 0.6424\n","Epoch 152/200\n","1000/1000 [==============================] - 0s 159us/step - loss: 0.0104 - acc: 0.9970 - val_loss: 1.5476 - val_acc: 0.6203\n","Epoch 153/200\n","1000/1000 [==============================] - 0s 161us/step - loss: 0.0103 - acc: 0.9970 - val_loss: 1.4014 - val_acc: 0.6361\n","Epoch 154/200\n","1000/1000 [==============================] - 0s 160us/step - loss: 0.0118 - acc: 0.9970 - val_loss: 1.4001 - val_acc: 0.6361\n","Epoch 155/200\n","1000/1000 [==============================] - 0s 180us/step - loss: 0.0123 - acc: 0.9960 - val_loss: 1.4149 - val_acc: 0.6297\n","Epoch 156/200\n","1000/1000 [==============================] - 0s 174us/step - loss: 0.0104 - acc: 0.9970 - val_loss: 1.5389 - val_acc: 0.6266\n","Epoch 157/200\n","1000/1000 [==============================] - 0s 171us/step - loss: 0.0101 - acc: 0.9980 - val_loss: 2.5083 - val_acc: 0.5475\n","Epoch 158/200\n","1000/1000 [==============================] - 0s 160us/step - loss: 0.0260 - acc: 0.9930 - val_loss: 1.4677 - val_acc: 0.6297\n","Epoch 159/200\n","1000/1000 [==============================] - 0s 161us/step - loss: 0.0136 - acc: 0.9980 - val_loss: 1.5483 - val_acc: 0.6361\n","Epoch 160/200\n","1000/1000 [==============================] - 0s 163us/step - loss: 0.0124 - acc: 0.9970 - val_loss: 1.4233 - val_acc: 0.6139\n","Epoch 161/200\n","1000/1000 [==============================] - 0s 206us/step - loss: 0.0109 - acc: 0.9980 - val_loss: 1.4777 - val_acc: 0.6203\n","Epoch 162/200\n","1000/1000 [==============================] - 0s 168us/step - loss: 0.0114 - acc: 0.9960 - val_loss: 1.4574 - val_acc: 0.6171\n","Epoch 163/200\n","1000/1000 [==============================] - 0s 163us/step - loss: 0.0100 - acc: 0.9970 - val_loss: 1.4053 - val_acc: 0.6329\n","Epoch 164/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 0.0116 - acc: 0.9970 - val_loss: 1.4160 - val_acc: 0.6297\n","Epoch 165/200\n","1000/1000 [==============================] - 0s 168us/step - loss: 0.0119 - acc: 0.9960 - val_loss: 1.4216 - val_acc: 0.6266\n","Epoch 166/200\n","1000/1000 [==============================] - 0s 171us/step - loss: 0.0099 - acc: 0.9980 - val_loss: 1.5972 - val_acc: 0.6266\n","Epoch 167/200\n","1000/1000 [==============================] - 0s 194us/step - loss: 0.0097 - acc: 0.9980 - val_loss: 1.4271 - val_acc: 0.6329\n","Epoch 168/200\n","1000/1000 [==============================] - 0s 169us/step - loss: 0.0102 - acc: 0.9970 - val_loss: 1.4877 - val_acc: 0.6139\n","Epoch 169/200\n","1000/1000 [==============================] - 0s 165us/step - loss: 0.0099 - acc: 0.9980 - val_loss: 1.4285 - val_acc: 0.6266\n","Epoch 170/200\n","1000/1000 [==============================] - 0s 164us/step - loss: 0.0099 - acc: 0.9980 - val_loss: 1.5244 - val_acc: 0.6234\n","Epoch 171/200\n","1000/1000 [==============================] - 0s 165us/step - loss: 0.0116 - acc: 0.9960 - val_loss: 1.5226 - val_acc: 0.6266\n","Epoch 172/200\n","1000/1000 [==============================] - 0s 171us/step - loss: 0.0100 - acc: 0.9980 - val_loss: 1.4345 - val_acc: 0.6329\n","Epoch 173/200\n","1000/1000 [==============================] - 0s 178us/step - loss: 0.0097 - acc: 0.9980 - val_loss: 1.5023 - val_acc: 0.6139\n","Epoch 174/200\n","1000/1000 [==============================] - 0s 159us/step - loss: 0.0098 - acc: 0.9970 - val_loss: 1.4449 - val_acc: 0.6297\n","Epoch 175/200\n","1000/1000 [==============================] - 0s 163us/step - loss: 0.0110 - acc: 0.9970 - val_loss: 1.4302 - val_acc: 0.6361\n","Epoch 176/200\n","1000/1000 [==============================] - 0s 165us/step - loss: 0.0098 - acc: 0.9970 - val_loss: 1.5117 - val_acc: 0.6139\n","Epoch 177/200\n","1000/1000 [==============================] - 0s 169us/step - loss: 0.0095 - acc: 0.9970 - val_loss: 1.4378 - val_acc: 0.6361\n","Epoch 178/200\n","1000/1000 [==============================] - 0s 165us/step - loss: 0.0123 - acc: 0.9970 - val_loss: 1.4494 - val_acc: 0.6329\n","Epoch 179/200\n","1000/1000 [==============================] - 0s 186us/step - loss: 0.0114 - acc: 0.9960 - val_loss: 1.4461 - val_acc: 0.6297\n","Epoch 180/200\n","1000/1000 [==============================] - 0s 168us/step - loss: 0.0117 - acc: 0.9970 - val_loss: 1.4497 - val_acc: 0.6234\n","Epoch 181/200\n","1000/1000 [==============================] - 0s 163us/step - loss: 0.0115 - acc: 0.9970 - val_loss: 1.4553 - val_acc: 0.6234\n","Epoch 182/200\n","1000/1000 [==============================] - 0s 176us/step - loss: 0.0113 - acc: 0.9970 - val_loss: 1.4402 - val_acc: 0.6361\n","Epoch 183/200\n","1000/1000 [==============================] - 0s 170us/step - loss: 0.0118 - acc: 0.9970 - val_loss: 1.4588 - val_acc: 0.6266\n","Epoch 184/200\n","1000/1000 [==============================] - 0s 164us/step - loss: 0.0108 - acc: 0.9970 - val_loss: 2.5019 - val_acc: 0.5411\n","Epoch 185/200\n","1000/1000 [==============================] - 0s 170us/step - loss: 0.0358 - acc: 0.9880 - val_loss: 1.4950 - val_acc: 0.6203\n","Epoch 186/200\n","1000/1000 [==============================] - 0s 157us/step - loss: 0.0128 - acc: 0.9980 - val_loss: 1.5062 - val_acc: 0.6234\n","Epoch 187/200\n","1000/1000 [==============================] - 0s 174us/step - loss: 0.0102 - acc: 0.9970 - val_loss: 1.5053 - val_acc: 0.6203\n","Epoch 188/200\n","1000/1000 [==============================] - 0s 164us/step - loss: 0.0106 - acc: 0.9960 - val_loss: 1.5069 - val_acc: 0.6266\n","Epoch 189/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 0.0094 - acc: 0.9980 - val_loss: 1.4470 - val_acc: 0.6203\n","Epoch 190/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 0.0092 - acc: 0.9980 - val_loss: 1.5366 - val_acc: 0.6266\n","Epoch 191/200\n","1000/1000 [==============================] - 0s 184us/step - loss: 0.0094 - acc: 0.9980 - val_loss: 1.4495 - val_acc: 0.6234\n","Epoch 192/200\n","1000/1000 [==============================] - 0s 178us/step - loss: 0.0089 - acc: 0.9980 - val_loss: 1.5652 - val_acc: 0.6329\n","Epoch 193/200\n","1000/1000 [==============================] - 0s 178us/step - loss: 0.0085 - acc: 0.9980 - val_loss: 1.4491 - val_acc: 0.6266\n","Epoch 194/200\n","1000/1000 [==============================] - 0s 163us/step - loss: 0.0105 - acc: 0.9960 - val_loss: 1.4556 - val_acc: 0.6297\n","Epoch 195/200\n","1000/1000 [==============================] - 0s 167us/step - loss: 0.0102 - acc: 0.9960 - val_loss: 1.4668 - val_acc: 0.6329\n","Epoch 196/200\n","1000/1000 [==============================] - 0s 163us/step - loss: 0.0106 - acc: 0.9970 - val_loss: 1.4588 - val_acc: 0.6361\n","Epoch 197/200\n","1000/1000 [==============================] - 0s 184us/step - loss: 0.0092 - acc: 0.9970 - val_loss: 1.5360 - val_acc: 0.6266\n","Epoch 198/200\n","1000/1000 [==============================] - 0s 171us/step - loss: 0.0100 - acc: 0.9960 - val_loss: 1.5714 - val_acc: 0.6266\n","Epoch 199/200\n","1000/1000 [==============================] - 0s 163us/step - loss: 0.0081 - acc: 0.9980 - val_loss: 1.4840 - val_acc: 0.6234\n","Epoch 200/200\n","1000/1000 [==============================] - 0s 160us/step - loss: 0.0100 - acc: 0.9960 - val_loss: 1.4972 - val_acc: 0.6076\n"],"name":"stdout"}]},{"metadata":{"id":"Cwb0pKWLgyK8","colab_type":"code","outputId":"337164b2-a349-4b46-8be5-1a03b75211f1","colab":{"base_uri":"https://localhost:8080/","height":3434}},"cell_type":"code","source":["optimizer = optimizers.SGD(lr=0.001, nesterov=True)\n","loss = 'categorical_crossentropy'\n","epochs = 100\n","batch_size = 32\n","\n","model.compile(optimizer= optimizer,\n","              loss = loss,\n","              metrics=['acc'])\n","\n","history_2 = model.fit(x_train, y_train,\n","                    epochs=epochs,\n","                    batch_size=batch_size,\n","                    validation_data=(x_val, y_val))\n","\n","model.save_weights('model_2.h5')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 1000 samples, validate on 316 samples\n","Epoch 1/100\n","1000/1000 [==============================] - 1s 564us/step - loss: 0.0092 - acc: 0.9980 - val_loss: 1.4965 - val_acc: 0.6044\n","Epoch 2/100\n","1000/1000 [==============================] - 0s 170us/step - loss: 0.0090 - acc: 0.9980 - val_loss: 1.4970 - val_acc: 0.6044\n","Epoch 3/100\n","1000/1000 [==============================] - 0s 182us/step - loss: 0.0088 - acc: 0.9980 - val_loss: 1.4971 - val_acc: 0.6044\n","Epoch 4/100\n","1000/1000 [==============================] - 0s 136us/step - loss: 0.0086 - acc: 0.9980 - val_loss: 1.4992 - val_acc: 0.6044\n","Epoch 5/100\n","1000/1000 [==============================] - 0s 156us/step - loss: 0.0085 - acc: 0.9980 - val_loss: 1.5011 - val_acc: 0.6044\n","Epoch 6/100\n","1000/1000 [==============================] - 0s 164us/step - loss: 0.0083 - acc: 0.9980 - val_loss: 1.5020 - val_acc: 0.6076\n","Epoch 7/100\n","1000/1000 [==============================] - 0s 135us/step - loss: 0.0082 - acc: 0.9980 - val_loss: 1.4998 - val_acc: 0.6044\n","Epoch 8/100\n","1000/1000 [==============================] - 0s 136us/step - loss: 0.0081 - acc: 0.9980 - val_loss: 1.5014 - val_acc: 0.6044\n","Epoch 9/100\n","1000/1000 [==============================] - 0s 133us/step - loss: 0.0080 - acc: 0.9980 - val_loss: 1.5000 - val_acc: 0.6044\n","Epoch 10/100\n","1000/1000 [==============================] - 0s 151us/step - loss: 0.0079 - acc: 0.9980 - val_loss: 1.5001 - val_acc: 0.6044\n","Epoch 11/100\n","1000/1000 [==============================] - 0s 149us/step - loss: 0.0079 - acc: 0.9980 - val_loss: 1.5016 - val_acc: 0.6044\n","Epoch 12/100\n","1000/1000 [==============================] - 0s 168us/step - loss: 0.0078 - acc: 0.9980 - val_loss: 1.5030 - val_acc: 0.6044\n","Epoch 13/100\n","1000/1000 [==============================] - 0s 166us/step - loss: 0.0078 - acc: 0.9980 - val_loss: 1.5014 - val_acc: 0.6044\n","Epoch 14/100\n","1000/1000 [==============================] - 0s 177us/step - loss: 0.0077 - acc: 0.9980 - val_loss: 1.5006 - val_acc: 0.6044\n","Epoch 15/100\n","1000/1000 [==============================] - 0s 160us/step - loss: 0.0076 - acc: 0.9980 - val_loss: 1.5009 - val_acc: 0.6044\n","Epoch 16/100\n","1000/1000 [==============================] - 0s 182us/step - loss: 0.0076 - acc: 0.9980 - val_loss: 1.5013 - val_acc: 0.6108\n","Epoch 17/100\n","1000/1000 [==============================] - 0s 162us/step - loss: 0.0075 - acc: 0.9980 - val_loss: 1.5036 - val_acc: 0.6108\n","Epoch 18/100\n","1000/1000 [==============================] - 0s 166us/step - loss: 0.0075 - acc: 0.9980 - val_loss: 1.5051 - val_acc: 0.6108\n","Epoch 19/100\n","1000/1000 [==============================] - 0s 180us/step - loss: 0.0074 - acc: 0.9970 - val_loss: 1.5044 - val_acc: 0.6171\n","Epoch 20/100\n","1000/1000 [==============================] - 0s 164us/step - loss: 0.0074 - acc: 0.9980 - val_loss: 1.5063 - val_acc: 0.6171\n","Epoch 21/100\n","1000/1000 [==============================] - 0s 163us/step - loss: 0.0074 - acc: 0.9970 - val_loss: 1.5058 - val_acc: 0.6171\n","Epoch 22/100\n","1000/1000 [==============================] - 0s 181us/step - loss: 0.0073 - acc: 0.9970 - val_loss: 1.5061 - val_acc: 0.6171\n","Epoch 23/100\n","1000/1000 [==============================] - 0s 167us/step - loss: 0.0073 - acc: 0.9970 - val_loss: 1.5062 - val_acc: 0.6171\n","Epoch 24/100\n","1000/1000 [==============================] - 0s 200us/step - loss: 0.0073 - acc: 0.9970 - val_loss: 1.5064 - val_acc: 0.6171\n","Epoch 25/100\n","1000/1000 [==============================] - 0s 165us/step - loss: 0.0073 - acc: 0.9980 - val_loss: 1.5082 - val_acc: 0.6171\n","Epoch 26/100\n","1000/1000 [==============================] - 0s 163us/step - loss: 0.0072 - acc: 0.9980 - val_loss: 1.5100 - val_acc: 0.6171\n","Epoch 27/100\n","1000/1000 [==============================] - 0s 170us/step - loss: 0.0072 - acc: 0.9970 - val_loss: 1.5089 - val_acc: 0.6171\n","Epoch 28/100\n","1000/1000 [==============================] - 0s 182us/step - loss: 0.0072 - acc: 0.9970 - val_loss: 1.5088 - val_acc: 0.6171\n","Epoch 29/100\n","1000/1000 [==============================] - 0s 193us/step - loss: 0.0072 - acc: 0.9970 - val_loss: 1.5097 - val_acc: 0.6203\n","Epoch 30/100\n","1000/1000 [==============================] - 0s 174us/step - loss: 0.0072 - acc: 0.9980 - val_loss: 1.5118 - val_acc: 0.6171\n","Epoch 31/100\n","1000/1000 [==============================] - 0s 161us/step - loss: 0.0071 - acc: 0.9980 - val_loss: 1.5140 - val_acc: 0.6171\n","Epoch 32/100\n","1000/1000 [==============================] - 0s 163us/step - loss: 0.0071 - acc: 0.9980 - val_loss: 1.5151 - val_acc: 0.6171\n","Epoch 33/100\n","1000/1000 [==============================] - 0s 163us/step - loss: 0.0071 - acc: 0.9970 - val_loss: 1.5159 - val_acc: 0.6171\n","Epoch 34/100\n","1000/1000 [==============================] - 0s 210us/step - loss: 0.0071 - acc: 0.9970 - val_loss: 1.5165 - val_acc: 0.6171\n","Epoch 35/100\n","1000/1000 [==============================] - 0s 160us/step - loss: 0.0071 - acc: 0.9980 - val_loss: 1.5150 - val_acc: 0.6171\n","Epoch 36/100\n","1000/1000 [==============================] - 0s 157us/step - loss: 0.0071 - acc: 0.9980 - val_loss: 1.5139 - val_acc: 0.6171\n","Epoch 37/100\n","1000/1000 [==============================] - 0s 160us/step - loss: 0.0071 - acc: 0.9980 - val_loss: 1.5148 - val_acc: 0.6171\n","Epoch 38/100\n","1000/1000 [==============================] - 0s 163us/step - loss: 0.0071 - acc: 0.9970 - val_loss: 1.5159 - val_acc: 0.6171\n","Epoch 39/100\n","1000/1000 [==============================] - 0s 166us/step - loss: 0.0071 - acc: 0.9980 - val_loss: 1.5151 - val_acc: 0.6171\n","Epoch 40/100\n","1000/1000 [==============================] - 0s 181us/step - loss: 0.0071 - acc: 0.9970 - val_loss: 1.5146 - val_acc: 0.6171\n","Epoch 41/100\n","1000/1000 [==============================] - 0s 158us/step - loss: 0.0071 - acc: 0.9970 - val_loss: 1.5142 - val_acc: 0.6171\n","Epoch 42/100\n","1000/1000 [==============================] - 0s 165us/step - loss: 0.0071 - acc: 0.9970 - val_loss: 1.5139 - val_acc: 0.6203\n","Epoch 43/100\n","1000/1000 [==============================] - 0s 168us/step - loss: 0.0071 - acc: 0.9980 - val_loss: 1.5153 - val_acc: 0.6171\n","Epoch 44/100\n","1000/1000 [==============================] - 0s 163us/step - loss: 0.0071 - acc: 0.9980 - val_loss: 1.5161 - val_acc: 0.6171\n","Epoch 45/100\n","1000/1000 [==============================] - 0s 172us/step - loss: 0.0071 - acc: 0.9970 - val_loss: 1.5170 - val_acc: 0.6171\n","Epoch 46/100\n","1000/1000 [==============================] - 0s 183us/step - loss: 0.0071 - acc: 0.9970 - val_loss: 1.5183 - val_acc: 0.6171\n","Epoch 47/100\n","1000/1000 [==============================] - 0s 164us/step - loss: 0.0071 - acc: 0.9980 - val_loss: 1.5168 - val_acc: 0.6171\n","Epoch 48/100\n","1000/1000 [==============================] - 0s 171us/step - loss: 0.0070 - acc: 0.9970 - val_loss: 1.5175 - val_acc: 0.6171\n","Epoch 49/100\n","1000/1000 [==============================] - 0s 163us/step - loss: 0.0070 - acc: 0.9970 - val_loss: 1.5179 - val_acc: 0.6171\n","Epoch 50/100\n","1000/1000 [==============================] - 0s 201us/step - loss: 0.0070 - acc: 0.9970 - val_loss: 1.5180 - val_acc: 0.6171\n","Epoch 51/100\n","1000/1000 [==============================] - 0s 194us/step - loss: 0.0070 - acc: 0.9970 - val_loss: 1.5183 - val_acc: 0.6171\n","Epoch 52/100\n","1000/1000 [==============================] - 0s 197us/step - loss: 0.0070 - acc: 0.9970 - val_loss: 1.5186 - val_acc: 0.6171\n","Epoch 53/100\n","1000/1000 [==============================] - 0s 173us/step - loss: 0.0070 - acc: 0.9970 - val_loss: 1.5183 - val_acc: 0.6171\n","Epoch 54/100\n","1000/1000 [==============================] - 0s 170us/step - loss: 0.0070 - acc: 0.9980 - val_loss: 1.5170 - val_acc: 0.6203\n","Epoch 55/100\n","1000/1000 [==============================] - 0s 173us/step - loss: 0.0070 - acc: 0.9970 - val_loss: 1.5176 - val_acc: 0.6171\n","Epoch 56/100\n","1000/1000 [==============================] - 0s 170us/step - loss: 0.0070 - acc: 0.9980 - val_loss: 1.5164 - val_acc: 0.6203\n","Epoch 57/100\n","1000/1000 [==============================] - 0s 163us/step - loss: 0.0070 - acc: 0.9980 - val_loss: 1.5174 - val_acc: 0.6203\n","Epoch 58/100\n","1000/1000 [==============================] - 0s 182us/step - loss: 0.0070 - acc: 0.9970 - val_loss: 1.5178 - val_acc: 0.6203\n","Epoch 59/100\n","1000/1000 [==============================] - 0s 169us/step - loss: 0.0070 - acc: 0.9980 - val_loss: 1.5169 - val_acc: 0.6203\n","Epoch 60/100\n","1000/1000 [==============================] - 0s 173us/step - loss: 0.0070 - acc: 0.9970 - val_loss: 1.5180 - val_acc: 0.6203\n","Epoch 61/100\n","1000/1000 [==============================] - 0s 177us/step - loss: 0.0070 - acc: 0.9980 - val_loss: 1.5172 - val_acc: 0.6203\n","Epoch 62/100\n","1000/1000 [==============================] - 0s 167us/step - loss: 0.0070 - acc: 0.9970 - val_loss: 1.5160 - val_acc: 0.6203\n","Epoch 63/100\n","1000/1000 [==============================] - 0s 165us/step - loss: 0.0070 - acc: 0.9980 - val_loss: 1.5173 - val_acc: 0.6203\n","Epoch 64/100\n","1000/1000 [==============================] - 0s 175us/step - loss: 0.0070 - acc: 0.9970 - val_loss: 1.5185 - val_acc: 0.6203\n","Epoch 65/100\n","1000/1000 [==============================] - 0s 160us/step - loss: 0.0070 - acc: 0.9970 - val_loss: 1.5195 - val_acc: 0.6203\n","Epoch 66/100\n","1000/1000 [==============================] - 0s 159us/step - loss: 0.0070 - acc: 0.9980 - val_loss: 1.5181 - val_acc: 0.6203\n","Epoch 67/100\n","1000/1000 [==============================] - 0s 157us/step - loss: 0.0070 - acc: 0.9970 - val_loss: 1.5173 - val_acc: 0.6203\n","Epoch 68/100\n","1000/1000 [==============================] - 0s 160us/step - loss: 0.0070 - acc: 0.9970 - val_loss: 1.5167 - val_acc: 0.6203\n","Epoch 69/100\n","1000/1000 [==============================] - 0s 160us/step - loss: 0.0070 - acc: 0.9970 - val_loss: 1.5169 - val_acc: 0.6203\n","Epoch 70/100\n","1000/1000 [==============================] - 0s 171us/step - loss: 0.0070 - acc: 0.9980 - val_loss: 1.5181 - val_acc: 0.6203\n","Epoch 71/100\n","1000/1000 [==============================] - 0s 158us/step - loss: 0.0070 - acc: 0.9970 - val_loss: 1.5176 - val_acc: 0.6203\n","Epoch 72/100\n","1000/1000 [==============================] - 0s 161us/step - loss: 0.0070 - acc: 0.9980 - val_loss: 1.5193 - val_acc: 0.6203\n","Epoch 73/100\n","1000/1000 [==============================] - 0s 161us/step - loss: 0.0069 - acc: 0.9970 - val_loss: 1.5183 - val_acc: 0.6203\n","Epoch 74/100\n","1000/1000 [==============================] - 0s 160us/step - loss: 0.0069 - acc: 0.9970 - val_loss: 1.5180 - val_acc: 0.6203\n","Epoch 75/100\n","1000/1000 [==============================] - 0s 166us/step - loss: 0.0069 - acc: 0.9970 - val_loss: 1.5185 - val_acc: 0.6234\n","Epoch 76/100\n","1000/1000 [==============================] - 0s 179us/step - loss: 0.0069 - acc: 0.9980 - val_loss: 1.5206 - val_acc: 0.6203\n","Epoch 77/100\n","1000/1000 [==============================] - 0s 158us/step - loss: 0.0069 - acc: 0.9970 - val_loss: 1.5204 - val_acc: 0.6203\n","Epoch 78/100\n","1000/1000 [==============================] - 0s 159us/step - loss: 0.0069 - acc: 0.9970 - val_loss: 1.5201 - val_acc: 0.6203\n","Epoch 79/100\n","1000/1000 [==============================] - 0s 167us/step - loss: 0.0068 - acc: 0.9980 - val_loss: 1.5210 - val_acc: 0.6203\n","Epoch 80/100\n","1000/1000 [==============================] - 0s 161us/step - loss: 0.0069 - acc: 0.9970 - val_loss: 1.5201 - val_acc: 0.6203\n","Epoch 81/100\n","1000/1000 [==============================] - 0s 161us/step - loss: 0.0069 - acc: 0.9970 - val_loss: 1.5202 - val_acc: 0.6203\n","Epoch 82/100\n","1000/1000 [==============================] - 0s 164us/step - loss: 0.0069 - acc: 0.9980 - val_loss: 1.5219 - val_acc: 0.6203\n","Epoch 83/100\n","1000/1000 [==============================] - 0s 162us/step - loss: 0.0069 - acc: 0.9980 - val_loss: 1.5231 - val_acc: 0.6203\n","Epoch 84/100\n","1000/1000 [==============================] - 0s 157us/step - loss: 0.0069 - acc: 0.9970 - val_loss: 1.5236 - val_acc: 0.6203\n","Epoch 85/100\n","1000/1000 [==============================] - 0s 162us/step - loss: 0.0069 - acc: 0.9960 - val_loss: 1.5241 - val_acc: 0.6203\n","Epoch 86/100\n","1000/1000 [==============================] - 0s 158us/step - loss: 0.0069 - acc: 0.9960 - val_loss: 1.5242 - val_acc: 0.6203\n","Epoch 87/100\n","1000/1000 [==============================] - 0s 160us/step - loss: 0.0069 - acc: 0.9960 - val_loss: 1.5242 - val_acc: 0.6203\n","Epoch 88/100\n","1000/1000 [==============================] - 0s 161us/step - loss: 0.0069 - acc: 0.9970 - val_loss: 1.5066 - val_acc: 0.6266\n","Epoch 89/100\n","1000/1000 [==============================] - 0s 172us/step - loss: 0.0071 - acc: 0.9980 - val_loss: 1.5105 - val_acc: 0.6266\n","Epoch 90/100\n","1000/1000 [==============================] - 0s 160us/step - loss: 0.0070 - acc: 0.9980 - val_loss: 1.5147 - val_acc: 0.6266\n","Epoch 91/100\n","1000/1000 [==============================] - 0s 157us/step - loss: 0.0069 - acc: 0.9980 - val_loss: 1.5188 - val_acc: 0.6234\n","Epoch 92/100\n","1000/1000 [==============================] - 0s 157us/step - loss: 0.0069 - acc: 0.9980 - val_loss: 1.5215 - val_acc: 0.6203\n","Epoch 93/100\n","1000/1000 [==============================] - 0s 160us/step - loss: 0.0069 - acc: 0.9970 - val_loss: 1.5222 - val_acc: 0.6203\n","Epoch 94/100\n","1000/1000 [==============================] - 0s 160us/step - loss: 0.0069 - acc: 0.9970 - val_loss: 1.5239 - val_acc: 0.6203\n","Epoch 95/100\n","1000/1000 [==============================] - 0s 172us/step - loss: 0.0068 - acc: 0.9980 - val_loss: 1.5238 - val_acc: 0.6203\n","Epoch 96/100\n","1000/1000 [==============================] - 0s 158us/step - loss: 0.0069 - acc: 0.9980 - val_loss: 1.5249 - val_acc: 0.6203\n","Epoch 97/100\n","1000/1000 [==============================] - 0s 161us/step - loss: 0.0069 - acc: 0.9960 - val_loss: 1.5238 - val_acc: 0.6203\n","Epoch 98/100\n","1000/1000 [==============================] - 0s 158us/step - loss: 0.0068 - acc: 0.9980 - val_loss: 1.5248 - val_acc: 0.6203\n","Epoch 99/100\n","1000/1000 [==============================] - 0s 159us/step - loss: 0.0069 - acc: 0.9980 - val_loss: 1.5259 - val_acc: 0.6171\n","Epoch 100/100\n","1000/1000 [==============================] - 0s 158us/step - loss: 0.0069 - acc: 0.9980 - val_loss: 1.5244 - val_acc: 0.6203\n"],"name":"stdout"}]},{"metadata":{"id":"ognjWcIFhRZ1","colab_type":"code","outputId":"b79d153c-abc4-4781-f96a-91b9a88f49e4","colab":{"base_uri":"https://localhost:8080/","height":3434}},"cell_type":"code","source":["optimizer = optimizers.Adam(lr=0.001)\n","loss = 'categorical_crossentropy'\n","epochs = 100\n","batch_size = 32\n","\n","model.compile(optimizer= optimizer,\n","              loss = loss,\n","              metrics=['acc'])\n","\n","history_3 = model.fit(x_train, y_train,\n","                    epochs=epochs,\n","                    batch_size=batch_size,\n","                    validation_data=(x_val, y_val))\n","\n","model.save_weights('model_3.h5')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 1000 samples, validate on 316 samples\n","Epoch 1/100\n","1000/1000 [==============================] - 1s 691us/step - loss: 0.2624 - acc: 0.9300 - val_loss: 1.3887 - val_acc: 0.6013\n","Epoch 2/100\n","1000/1000 [==============================] - 0s 175us/step - loss: 0.0329 - acc: 0.9960 - val_loss: 1.5014 - val_acc: 0.6108\n","Epoch 3/100\n","1000/1000 [==============================] - 0s 188us/step - loss: 0.0181 - acc: 0.9960 - val_loss: 1.4749 - val_acc: 0.6203\n","Epoch 4/100\n","1000/1000 [==============================] - 0s 171us/step - loss: 0.0122 - acc: 0.9980 - val_loss: 1.4864 - val_acc: 0.6234\n","Epoch 5/100\n","1000/1000 [==============================] - 0s 190us/step - loss: 0.0174 - acc: 0.9970 - val_loss: 1.4974 - val_acc: 0.6013\n","Epoch 6/100\n","1000/1000 [==============================] - 0s 192us/step - loss: 0.0204 - acc: 0.9960 - val_loss: 1.5164 - val_acc: 0.6203\n","Epoch 7/100\n","1000/1000 [==============================] - 0s 177us/step - loss: 0.0123 - acc: 0.9980 - val_loss: 1.5969 - val_acc: 0.6139\n","Epoch 8/100\n","1000/1000 [==============================] - 0s 182us/step - loss: 0.0171 - acc: 0.9980 - val_loss: 1.5476 - val_acc: 0.6266\n","Epoch 9/100\n","1000/1000 [==============================] - 0s 182us/step - loss: 0.0127 - acc: 0.9980 - val_loss: 1.6339 - val_acc: 0.6234\n","Epoch 10/100\n","1000/1000 [==============================] - 0s 179us/step - loss: 0.0103 - acc: 0.9980 - val_loss: 1.5730 - val_acc: 0.6171\n","Epoch 11/100\n","1000/1000 [==============================] - 0s 197us/step - loss: 0.0173 - acc: 0.9980 - val_loss: 1.7588 - val_acc: 0.6108\n","Epoch 12/100\n","1000/1000 [==============================] - 0s 192us/step - loss: 0.0048 - acc: 0.9990 - val_loss: 1.6888 - val_acc: 0.6139\n","Epoch 13/100\n","1000/1000 [==============================] - 0s 195us/step - loss: 0.0174 - acc: 0.9970 - val_loss: 1.7553 - val_acc: 0.6044\n","Epoch 14/100\n","1000/1000 [==============================] - 0s 181us/step - loss: 0.0136 - acc: 0.9970 - val_loss: 1.6822 - val_acc: 0.6266\n","Epoch 15/100\n","1000/1000 [==============================] - 0s 189us/step - loss: 0.0170 - acc: 0.9980 - val_loss: 1.7615 - val_acc: 0.6108\n","Epoch 16/100\n","1000/1000 [==============================] - 0s 189us/step - loss: 0.0173 - acc: 0.9980 - val_loss: 1.6349 - val_acc: 0.5949\n","Epoch 17/100\n","1000/1000 [==============================] - 0s 187us/step - loss: 0.0216 - acc: 0.9970 - val_loss: 1.9350 - val_acc: 0.6266\n","Epoch 18/100\n","1000/1000 [==============================] - 0s 176us/step - loss: 0.0180 - acc: 0.9970 - val_loss: 1.7040 - val_acc: 0.6108\n","Epoch 19/100\n","1000/1000 [==============================] - 0s 177us/step - loss: 0.0119 - acc: 0.9970 - val_loss: 1.7728 - val_acc: 0.6203\n","Epoch 20/100\n","1000/1000 [==============================] - 0s 179us/step - loss: 0.0150 - acc: 0.9970 - val_loss: 1.8707 - val_acc: 0.6203\n","Epoch 21/100\n","1000/1000 [==============================] - 0s 190us/step - loss: 0.0105 - acc: 0.9980 - val_loss: 1.7682 - val_acc: 0.6266\n","Epoch 22/100\n","1000/1000 [==============================] - 0s 198us/step - loss: 0.0162 - acc: 0.9980 - val_loss: 1.8936 - val_acc: 0.6297\n","Epoch 23/100\n","1000/1000 [==============================] - 0s 183us/step - loss: 0.0075 - acc: 0.9980 - val_loss: 1.7849 - val_acc: 0.6044\n","Epoch 24/100\n","1000/1000 [==============================] - 0s 184us/step - loss: 0.0128 - acc: 0.9980 - val_loss: 1.7292 - val_acc: 0.5981\n","Epoch 25/100\n","1000/1000 [==============================] - 0s 178us/step - loss: 0.0093 - acc: 0.9970 - val_loss: 1.7746 - val_acc: 0.6234\n","Epoch 26/100\n","1000/1000 [==============================] - 0s 180us/step - loss: 0.0080 - acc: 0.9970 - val_loss: 1.7557 - val_acc: 0.6108\n","Epoch 27/100\n","1000/1000 [==============================] - 0s 197us/step - loss: 0.0068 - acc: 0.9980 - val_loss: 1.8615 - val_acc: 0.6108\n","Epoch 28/100\n","1000/1000 [==============================] - 0s 197us/step - loss: 0.0094 - acc: 0.9970 - val_loss: 1.8134 - val_acc: 0.6108\n","Epoch 29/100\n","1000/1000 [==============================] - 0s 189us/step - loss: 0.0064 - acc: 0.9980 - val_loss: 1.8330 - val_acc: 0.6171\n","Epoch 30/100\n","1000/1000 [==============================] - 0s 191us/step - loss: 0.0067 - acc: 0.9980 - val_loss: 1.8986 - val_acc: 0.6297\n","Epoch 31/100\n","1000/1000 [==============================] - 0s 181us/step - loss: 0.0063 - acc: 0.9980 - val_loss: 1.8265 - val_acc: 0.6139\n","Epoch 32/100\n","1000/1000 [==============================] - 0s 177us/step - loss: 0.0060 - acc: 0.9980 - val_loss: 1.9563 - val_acc: 0.6203\n","Epoch 33/100\n","1000/1000 [==============================] - 0s 190us/step - loss: 0.0086 - acc: 0.9970 - val_loss: 1.8658 - val_acc: 0.6171\n","Epoch 34/100\n","1000/1000 [==============================] - 0s 180us/step - loss: 0.0062 - acc: 0.9970 - val_loss: 1.8761 - val_acc: 0.6171\n","Epoch 35/100\n","1000/1000 [==============================] - 0s 187us/step - loss: 0.0052 - acc: 0.9980 - val_loss: 1.9371 - val_acc: 0.6203\n","Epoch 36/100\n","1000/1000 [==============================] - 0s 182us/step - loss: 0.0078 - acc: 0.9970 - val_loss: 1.9185 - val_acc: 0.6171\n","Epoch 37/100\n","1000/1000 [==============================] - 0s 177us/step - loss: 0.0049 - acc: 0.9980 - val_loss: 1.8782 - val_acc: 0.6171\n","Epoch 38/100\n","1000/1000 [==============================] - 0s 190us/step - loss: 0.0083 - acc: 0.9970 - val_loss: 1.8931 - val_acc: 0.6171\n","Epoch 39/100\n","1000/1000 [==============================] - 0s 189us/step - loss: 0.0057 - acc: 0.9980 - val_loss: 1.9933 - val_acc: 0.6139\n","Epoch 40/100\n","1000/1000 [==============================] - 0s 184us/step - loss: 0.0064 - acc: 0.9980 - val_loss: 1.9379 - val_acc: 0.6171\n","Epoch 41/100\n","1000/1000 [==============================] - 0s 178us/step - loss: 0.0049 - acc: 0.9970 - val_loss: 1.9073 - val_acc: 0.6108\n","Epoch 42/100\n","1000/1000 [==============================] - 0s 177us/step - loss: 0.0064 - acc: 0.9970 - val_loss: 1.8895 - val_acc: 0.6139\n","Epoch 43/100\n","1000/1000 [==============================] - 0s 179us/step - loss: 0.0052 - acc: 0.9980 - val_loss: 1.9767 - val_acc: 0.6139\n","Epoch 44/100\n","1000/1000 [==============================] - 0s 194us/step - loss: 0.0063 - acc: 0.9980 - val_loss: 1.8808 - val_acc: 0.6139\n","Epoch 45/100\n","1000/1000 [==============================] - 0s 186us/step - loss: 0.0115 - acc: 0.9970 - val_loss: 1.8814 - val_acc: 0.6171\n","Epoch 46/100\n","1000/1000 [==============================] - 0s 191us/step - loss: 0.0067 - acc: 0.9980 - val_loss: 1.9164 - val_acc: 0.6203\n","Epoch 47/100\n","1000/1000 [==============================] - 0s 181us/step - loss: 0.0047 - acc: 0.9980 - val_loss: 1.9971 - val_acc: 0.6108\n","Epoch 48/100\n","1000/1000 [==============================] - 0s 177us/step - loss: 0.0052 - acc: 0.9980 - val_loss: 1.9358 - val_acc: 0.6139\n","Epoch 49/100\n","1000/1000 [==============================] - 0s 185us/step - loss: 0.0042 - acc: 0.9970 - val_loss: 1.9268 - val_acc: 0.6139\n","Epoch 50/100\n","1000/1000 [==============================] - 0s 195us/step - loss: 0.0038 - acc: 0.9980 - val_loss: 1.9417 - val_acc: 0.6203\n","Epoch 51/100\n","1000/1000 [==============================] - 0s 186us/step - loss: 0.0042 - acc: 0.9980 - val_loss: 2.1619 - val_acc: 0.6171\n","Epoch 52/100\n","1000/1000 [==============================] - 0s 178us/step - loss: 0.0126 - acc: 0.9960 - val_loss: 1.8760 - val_acc: 0.6013\n","Epoch 53/100\n","1000/1000 [==============================] - 0s 175us/step - loss: 0.0038 - acc: 0.9970 - val_loss: 1.9359 - val_acc: 0.6139\n","Epoch 54/100\n","1000/1000 [==============================] - 0s 176us/step - loss: 0.0038 - acc: 0.9970 - val_loss: 1.9578 - val_acc: 0.6139\n","Epoch 55/100\n","1000/1000 [==============================] - 0s 201us/step - loss: 0.0039 - acc: 0.9980 - val_loss: 1.9663 - val_acc: 0.6171\n","Epoch 56/100\n","1000/1000 [==============================] - 0s 180us/step - loss: 0.0037 - acc: 0.9980 - val_loss: 1.9861 - val_acc: 0.6203\n","Epoch 57/100\n","1000/1000 [==============================] - 0s 177us/step - loss: 0.0037 - acc: 0.9980 - val_loss: 1.9893 - val_acc: 0.6203\n","Epoch 58/100\n","1000/1000 [==============================] - 0s 176us/step - loss: 0.0042 - acc: 0.9970 - val_loss: 1.9990 - val_acc: 0.6108\n","Epoch 59/100\n","1000/1000 [==============================] - 0s 179us/step - loss: 0.0038 - acc: 0.9980 - val_loss: 1.9984 - val_acc: 0.6171\n","Epoch 60/100\n","1000/1000 [==============================] - 0s 180us/step - loss: 0.0140 - acc: 0.9960 - val_loss: 1.9410 - val_acc: 0.6203\n","Epoch 61/100\n","1000/1000 [==============================] - 0s 209us/step - loss: 0.0235 - acc: 0.9960 - val_loss: 1.9970 - val_acc: 0.5981\n","Epoch 62/100\n","1000/1000 [==============================] - 0s 193us/step - loss: 0.0188 - acc: 0.9970 - val_loss: 1.9650 - val_acc: 0.6044\n","Epoch 63/100\n","1000/1000 [==============================] - 0s 193us/step - loss: 0.0209 - acc: 0.9970 - val_loss: 2.0749 - val_acc: 0.6203\n","Epoch 64/100\n","1000/1000 [==============================] - 0s 193us/step - loss: 0.1663 - acc: 0.9570 - val_loss: 2.9077 - val_acc: 0.6044\n","Epoch 65/100\n","1000/1000 [==============================] - 0s 177us/step - loss: 0.5490 - acc: 0.8750 - val_loss: 2.9616 - val_acc: 0.6266\n","Epoch 66/100\n","1000/1000 [==============================] - 0s 194us/step - loss: 0.2597 - acc: 0.9230 - val_loss: 2.2597 - val_acc: 0.6487\n","Epoch 67/100\n","1000/1000 [==============================] - 0s 189us/step - loss: 0.0732 - acc: 0.9810 - val_loss: 2.3366 - val_acc: 0.6013\n","Epoch 68/100\n","1000/1000 [==============================] - 0s 179us/step - loss: 0.0420 - acc: 0.9900 - val_loss: 2.4056 - val_acc: 0.6139\n","Epoch 69/100\n","1000/1000 [==============================] - 0s 175us/step - loss: 0.0317 - acc: 0.9940 - val_loss: 2.2591 - val_acc: 0.6139\n","Epoch 70/100\n","1000/1000 [==============================] - 0s 180us/step - loss: 0.0199 - acc: 0.9980 - val_loss: 2.2221 - val_acc: 0.6139\n","Epoch 71/100\n","1000/1000 [==============================] - 0s 193us/step - loss: 0.0190 - acc: 0.9980 - val_loss: 2.2197 - val_acc: 0.6076\n","Epoch 72/100\n","1000/1000 [==============================] - 0s 187us/step - loss: 0.0186 - acc: 0.9980 - val_loss: 2.2227 - val_acc: 0.6044\n","Epoch 73/100\n","1000/1000 [==============================] - 0s 183us/step - loss: 0.0188 - acc: 0.9970 - val_loss: 2.2264 - val_acc: 0.6076\n","Epoch 74/100\n","1000/1000 [==============================] - 0s 175us/step - loss: 0.0185 - acc: 0.9980 - val_loss: 2.2325 - val_acc: 0.6076\n","Epoch 75/100\n","1000/1000 [==============================] - 0s 178us/step - loss: 0.0185 - acc: 0.9980 - val_loss: 2.2331 - val_acc: 0.6108\n","Epoch 76/100\n","1000/1000 [==============================] - 0s 204us/step - loss: 0.0183 - acc: 0.9970 - val_loss: 2.2345 - val_acc: 0.6108\n","Epoch 77/100\n","1000/1000 [==============================] - 0s 202us/step - loss: 0.0182 - acc: 0.9980 - val_loss: 2.2367 - val_acc: 0.6076\n","Epoch 78/100\n","1000/1000 [==============================] - 0s 184us/step - loss: 0.0191 - acc: 0.9980 - val_loss: 2.2261 - val_acc: 0.5981\n","Epoch 79/100\n","1000/1000 [==============================] - 0s 178us/step - loss: 0.0189 - acc: 0.9980 - val_loss: 2.2418 - val_acc: 0.6013\n","Epoch 80/100\n","1000/1000 [==============================] - 0s 181us/step - loss: 0.0186 - acc: 0.9980 - val_loss: 2.2447 - val_acc: 0.6013\n","Epoch 81/100\n","1000/1000 [==============================] - 0s 191us/step - loss: 0.0184 - acc: 0.9980 - val_loss: 2.2515 - val_acc: 0.5981\n","Epoch 82/100\n","1000/1000 [==============================] - 0s 197us/step - loss: 0.0183 - acc: 0.9980 - val_loss: 2.2555 - val_acc: 0.6013\n","Epoch 83/100\n","1000/1000 [==============================] - 0s 226us/step - loss: 0.0182 - acc: 0.9980 - val_loss: 2.2588 - val_acc: 0.6013\n","Epoch 84/100\n","1000/1000 [==============================] - 0s 191us/step - loss: 0.0180 - acc: 0.9980 - val_loss: 2.2598 - val_acc: 0.6013\n","Epoch 85/100\n","1000/1000 [==============================] - 0s 193us/step - loss: 0.0180 - acc: 0.9980 - val_loss: 2.2627 - val_acc: 0.6013\n","Epoch 86/100\n","1000/1000 [==============================] - 0s 200us/step - loss: 0.0179 - acc: 0.9980 - val_loss: 2.2665 - val_acc: 0.6013\n","Epoch 87/100\n","1000/1000 [==============================] - 0s 230us/step - loss: 0.0179 - acc: 0.9980 - val_loss: 2.2665 - val_acc: 0.6013\n","Epoch 88/100\n","1000/1000 [==============================] - 0s 183us/step - loss: 0.0180 - acc: 0.9980 - val_loss: 2.2674 - val_acc: 0.6013\n","Epoch 89/100\n","1000/1000 [==============================] - 0s 192us/step - loss: 0.0179 - acc: 0.9970 - val_loss: 2.2852 - val_acc: 0.6108\n","Epoch 90/100\n","1000/1000 [==============================] - 0s 217us/step - loss: 0.0179 - acc: 0.9980 - val_loss: 2.2775 - val_acc: 0.6013\n","Epoch 91/100\n","1000/1000 [==============================] - 0s 181us/step - loss: 0.0179 - acc: 0.9980 - val_loss: 2.2774 - val_acc: 0.6013\n","Epoch 92/100\n","1000/1000 [==============================] - 0s 203us/step - loss: 0.0178 - acc: 0.9980 - val_loss: 2.2768 - val_acc: 0.6013\n","Epoch 93/100\n","1000/1000 [==============================] - 0s 196us/step - loss: 0.0178 - acc: 0.9980 - val_loss: 2.2783 - val_acc: 0.6013\n","Epoch 94/100\n","1000/1000 [==============================] - 0s 223us/step - loss: 0.0178 - acc: 0.9980 - val_loss: 2.2824 - val_acc: 0.6013\n","Epoch 95/100\n","1000/1000 [==============================] - 0s 182us/step - loss: 0.0178 - acc: 0.9980 - val_loss: 2.2829 - val_acc: 0.6013\n","Epoch 96/100\n","1000/1000 [==============================] - 0s 187us/step - loss: 0.0178 - acc: 0.9980 - val_loss: 2.2854 - val_acc: 0.6013\n","Epoch 97/100\n","1000/1000 [==============================] - 0s 213us/step - loss: 0.0178 - acc: 0.9980 - val_loss: 2.2881 - val_acc: 0.6013\n","Epoch 98/100\n","1000/1000 [==============================] - 0s 240us/step - loss: 0.0178 - acc: 0.9980 - val_loss: 2.2897 - val_acc: 0.6013\n","Epoch 99/100\n","1000/1000 [==============================] - 0s 193us/step - loss: 0.0178 - acc: 0.9980 - val_loss: 2.2921 - val_acc: 0.6013\n","Epoch 100/100\n","1000/1000 [==============================] - 0s 181us/step - loss: 0.0178 - acc: 0.9980 - val_loss: 2.2945 - val_acc: 0.6013\n"],"name":"stdout"}]},{"metadata":{"id":"xeBGKg6Ni77a","colab_type":"code","outputId":"af293682-8e4b-4354-8c70-70f20ee54b73","colab":{"base_uri":"https://localhost:8080/","height":3434}},"cell_type":"code","source":["optimizer = optimizers.Adam(lr=0.01)\n","loss = 'categorical_crossentropy'\n","epochs = 100\n","batch_size = 32\n","\n","model.compile(optimizer= optimizer,\n","              loss = loss,\n","              metrics=['acc'])\n","\n","history_4 = model.fit(x_train, y_train,\n","                    epochs=epochs,\n","                    batch_size=batch_size,\n","                    validation_data=(x_val, y_val))\n","\n","model.save_weights('model_4.h5')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 1000 samples, validate on 316 samples\n","Epoch 1/100\n","1000/1000 [==============================] - 1s 797us/step - loss: 1.0035 - acc: 0.8360 - val_loss: 3.1758 - val_acc: 0.6044\n","Epoch 2/100\n","1000/1000 [==============================] - 0s 208us/step - loss: 0.3880 - acc: 0.9040 - val_loss: 2.9876 - val_acc: 0.6108\n","Epoch 3/100\n","1000/1000 [==============================] - 0s 191us/step - loss: 0.3823 - acc: 0.9170 - val_loss: 1.5926 - val_acc: 0.5981\n","Epoch 4/100\n","1000/1000 [==============================] - 0s 223us/step - loss: 0.2498 - acc: 0.9170 - val_loss: 1.7561 - val_acc: 0.5791\n","Epoch 5/100\n","1000/1000 [==============================] - 0s 181us/step - loss: 0.1642 - acc: 0.9460 - val_loss: 3.5818 - val_acc: 0.6297\n","Epoch 6/100\n","1000/1000 [==============================] - 0s 179us/step - loss: 0.0977 - acc: 0.9750 - val_loss: 3.0370 - val_acc: 0.6424\n","Epoch 7/100\n","1000/1000 [==============================] - 0s 217us/step - loss: 0.1032 - acc: 0.9720 - val_loss: 2.6455 - val_acc: 0.5823\n","Epoch 8/100\n","1000/1000 [==============================] - 0s 201us/step - loss: 0.0580 - acc: 0.9860 - val_loss: 3.6078 - val_acc: 0.6487\n","Epoch 9/100\n","1000/1000 [==============================] - 0s 202us/step - loss: 0.0550 - acc: 0.9890 - val_loss: 3.2551 - val_acc: 0.6487\n","Epoch 10/100\n","1000/1000 [==============================] - 0s 194us/step - loss: 0.0743 - acc: 0.9840 - val_loss: 3.6238 - val_acc: 0.6297\n","Epoch 11/100\n","1000/1000 [==============================] - 0s 227us/step - loss: 0.0450 - acc: 0.9900 - val_loss: 3.0682 - val_acc: 0.6266\n","Epoch 12/100\n","1000/1000 [==============================] - 0s 211us/step - loss: 0.0202 - acc: 0.9980 - val_loss: 3.1587 - val_acc: 0.6297\n","Epoch 13/100\n","1000/1000 [==============================] - 0s 181us/step - loss: 0.0186 - acc: 0.9980 - val_loss: 3.1978 - val_acc: 0.6171\n","Epoch 14/100\n","1000/1000 [==============================] - 0s 212us/step - loss: 0.0187 - acc: 0.9980 - val_loss: 3.2188 - val_acc: 0.6234\n","Epoch 15/100\n","1000/1000 [==============================] - 0s 203us/step - loss: 0.0185 - acc: 0.9980 - val_loss: 3.2304 - val_acc: 0.6392\n","Epoch 16/100\n","1000/1000 [==============================] - 0s 198us/step - loss: 0.0184 - acc: 0.9980 - val_loss: 3.2388 - val_acc: 0.6392\n","Epoch 17/100\n","1000/1000 [==============================] - 0s 217us/step - loss: 0.0186 - acc: 0.9970 - val_loss: 3.2593 - val_acc: 0.6329\n","Epoch 18/100\n","1000/1000 [==============================] - 0s 225us/step - loss: 0.0186 - acc: 0.9970 - val_loss: 3.2584 - val_acc: 0.6361\n","Epoch 19/100\n","1000/1000 [==============================] - 0s 190us/step - loss: 0.0184 - acc: 0.9970 - val_loss: 3.2658 - val_acc: 0.6392\n","Epoch 20/100\n","1000/1000 [==============================] - 0s 185us/step - loss: 0.0185 - acc: 0.9980 - val_loss: 3.2735 - val_acc: 0.6392\n","Epoch 21/100\n","1000/1000 [==============================] - 0s 206us/step - loss: 0.0183 - acc: 0.9980 - val_loss: 3.2815 - val_acc: 0.6361\n","Epoch 22/100\n","1000/1000 [==============================] - 0s 229us/step - loss: 0.0184 - acc: 0.9980 - val_loss: 3.2888 - val_acc: 0.6361\n","Epoch 23/100\n","1000/1000 [==============================] - 0s 200us/step - loss: 0.0186 - acc: 0.9980 - val_loss: 3.2936 - val_acc: 0.6361\n","Epoch 24/100\n","1000/1000 [==============================] - 0s 195us/step - loss: 0.0183 - acc: 0.9980 - val_loss: 3.3011 - val_acc: 0.6361\n","Epoch 25/100\n","1000/1000 [==============================] - 0s 232us/step - loss: 0.0183 - acc: 0.9970 - val_loss: 3.3065 - val_acc: 0.6361\n","Epoch 26/100\n","1000/1000 [==============================] - 0s 188us/step - loss: 0.0188 - acc: 0.9980 - val_loss: 3.3108 - val_acc: 0.6361\n","Epoch 27/100\n","1000/1000 [==============================] - 0s 203us/step - loss: 0.0185 - acc: 0.9980 - val_loss: 3.3134 - val_acc: 0.6361\n","Epoch 28/100\n","1000/1000 [==============================] - 0s 208us/step - loss: 0.0189 - acc: 0.9980 - val_loss: 3.3239 - val_acc: 0.6361\n","Epoch 29/100\n","1000/1000 [==============================] - 0s 207us/step - loss: 0.0186 - acc: 0.9980 - val_loss: 3.3285 - val_acc: 0.6392\n","Epoch 30/100\n","1000/1000 [==============================] - 0s 199us/step - loss: 0.0186 - acc: 0.9980 - val_loss: 3.3357 - val_acc: 0.6361\n","Epoch 31/100\n","1000/1000 [==============================] - 0s 194us/step - loss: 0.0184 - acc: 0.9980 - val_loss: 3.3352 - val_acc: 0.6361\n","Epoch 32/100\n","1000/1000 [==============================] - 0s 264us/step - loss: 0.0182 - acc: 0.9980 - val_loss: 3.3410 - val_acc: 0.6361\n","Epoch 33/100\n","1000/1000 [==============================] - 0s 200us/step - loss: 0.0186 - acc: 0.9980 - val_loss: 3.3435 - val_acc: 0.6361\n","Epoch 34/100\n","1000/1000 [==============================] - 0s 223us/step - loss: 0.0184 - acc: 0.9980 - val_loss: 3.3484 - val_acc: 0.6329\n","Epoch 35/100\n","1000/1000 [==============================] - 0s 210us/step - loss: 0.0188 - acc: 0.9980 - val_loss: 3.3514 - val_acc: 0.6361\n","Epoch 36/100\n","1000/1000 [==============================] - 0s 229us/step - loss: 0.0184 - acc: 0.9980 - val_loss: 3.3538 - val_acc: 0.6361\n","Epoch 37/100\n","1000/1000 [==============================] - 0s 204us/step - loss: 0.0184 - acc: 0.9980 - val_loss: 3.3594 - val_acc: 0.6361\n","Epoch 38/100\n","1000/1000 [==============================] - 0s 219us/step - loss: 0.0184 - acc: 0.9980 - val_loss: 3.3622 - val_acc: 0.6361\n","Epoch 39/100\n","1000/1000 [==============================] - 0s 213us/step - loss: 0.0185 - acc: 0.9980 - val_loss: 3.3655 - val_acc: 0.6361\n","Epoch 40/100\n","1000/1000 [==============================] - 0s 209us/step - loss: 0.0185 - acc: 0.9980 - val_loss: 3.3681 - val_acc: 0.6361\n","Epoch 41/100\n","1000/1000 [==============================] - 0s 213us/step - loss: 0.0185 - acc: 0.9980 - val_loss: 3.3721 - val_acc: 0.6361\n","Epoch 42/100\n","1000/1000 [==============================] - 0s 218us/step - loss: 0.0186 - acc: 0.9970 - val_loss: 3.3719 - val_acc: 0.6392\n","Epoch 43/100\n","1000/1000 [==============================] - 0s 197us/step - loss: 0.0182 - acc: 0.9980 - val_loss: 3.3810 - val_acc: 0.6329\n","Epoch 44/100\n","1000/1000 [==============================] - 0s 197us/step - loss: 0.0182 - acc: 0.9980 - val_loss: 3.3813 - val_acc: 0.6392\n","Epoch 45/100\n","1000/1000 [==============================] - 0s 213us/step - loss: 0.0190 - acc: 0.9970 - val_loss: 3.3736 - val_acc: 0.6392\n","Epoch 46/100\n","1000/1000 [==============================] - 0s 220us/step - loss: 0.0185 - acc: 0.9980 - val_loss: 3.3917 - val_acc: 0.6424\n","Epoch 47/100\n","1000/1000 [==============================] - 0s 185us/step - loss: 0.0185 - acc: 0.9980 - val_loss: 3.3909 - val_acc: 0.6392\n","Epoch 48/100\n","1000/1000 [==============================] - 0s 189us/step - loss: 0.0184 - acc: 0.9980 - val_loss: 3.3899 - val_acc: 0.6392\n","Epoch 49/100\n","1000/1000 [==============================] - 0s 246us/step - loss: 0.0184 - acc: 0.9980 - val_loss: 3.3909 - val_acc: 0.6392\n","Epoch 50/100\n","1000/1000 [==============================] - 0s 177us/step - loss: 0.0184 - acc: 0.9970 - val_loss: 3.3949 - val_acc: 0.6392\n","Epoch 51/100\n","1000/1000 [==============================] - 0s 206us/step - loss: 0.0181 - acc: 0.9980 - val_loss: 3.3984 - val_acc: 0.6361\n","Epoch 52/100\n","1000/1000 [==============================] - 0s 229us/step - loss: 0.0185 - acc: 0.9980 - val_loss: 3.3977 - val_acc: 0.6392\n","Epoch 53/100\n","1000/1000 [==============================] - 0s 188us/step - loss: 0.0185 - acc: 0.9970 - val_loss: 3.3973 - val_acc: 0.6392\n","Epoch 54/100\n","1000/1000 [==============================] - 0s 196us/step - loss: 0.0184 - acc: 0.9980 - val_loss: 3.4107 - val_acc: 0.6392\n","Epoch 55/100\n","1000/1000 [==============================] - 0s 195us/step - loss: 0.0186 - acc: 0.9980 - val_loss: 3.4068 - val_acc: 0.6392\n","Epoch 56/100\n","1000/1000 [==============================] - 0s 240us/step - loss: 0.0191 - acc: 0.9970 - val_loss: 3.3917 - val_acc: 0.6392\n","Epoch 57/100\n","1000/1000 [==============================] - 0s 190us/step - loss: 0.0186 - acc: 0.9980 - val_loss: 3.4092 - val_acc: 0.6392\n","Epoch 58/100\n","1000/1000 [==============================] - 0s 199us/step - loss: 0.0185 - acc: 0.9980 - val_loss: 3.4093 - val_acc: 0.6392\n","Epoch 59/100\n","1000/1000 [==============================] - 0s 196us/step - loss: 0.0183 - acc: 0.9980 - val_loss: 3.4095 - val_acc: 0.6424\n","Epoch 60/100\n","1000/1000 [==============================] - 0s 208us/step - loss: 0.0186 - acc: 0.9980 - val_loss: 3.4143 - val_acc: 0.6424\n","Epoch 61/100\n","1000/1000 [==============================] - 0s 207us/step - loss: 0.0185 - acc: 0.9980 - val_loss: 3.4128 - val_acc: 0.6392\n","Epoch 62/100\n","1000/1000 [==============================] - 0s 208us/step - loss: 0.0184 - acc: 0.9980 - val_loss: 3.4152 - val_acc: 0.6392\n","Epoch 63/100\n","1000/1000 [==============================] - 0s 218us/step - loss: 0.0181 - acc: 0.9980 - val_loss: 3.4217 - val_acc: 0.6361\n","Epoch 64/100\n","1000/1000 [==============================] - 0s 194us/step - loss: 0.0188 - acc: 0.9970 - val_loss: 3.4154 - val_acc: 0.6424\n","Epoch 65/100\n","1000/1000 [==============================] - 0s 206us/step - loss: 0.0182 - acc: 0.9980 - val_loss: 3.4252 - val_acc: 0.6424\n","Epoch 66/100\n","1000/1000 [==============================] - 0s 225us/step - loss: 0.0193 - acc: 0.9980 - val_loss: 3.4177 - val_acc: 0.6424\n","Epoch 67/100\n","1000/1000 [==============================] - 0s 207us/step - loss: 0.0189 - acc: 0.9980 - val_loss: 3.3881 - val_acc: 0.6329\n","Epoch 68/100\n","1000/1000 [==============================] - 0s 210us/step - loss: 0.0189 - acc: 0.9980 - val_loss: 3.3860 - val_acc: 0.6329\n","Epoch 69/100\n","1000/1000 [==============================] - 0s 195us/step - loss: 0.0188 - acc: 0.9980 - val_loss: 3.3869 - val_acc: 0.6329\n","Epoch 70/100\n","1000/1000 [==============================] - 0s 226us/step - loss: 0.0188 - acc: 0.9980 - val_loss: 3.3880 - val_acc: 0.6361\n","Epoch 71/100\n","1000/1000 [==============================] - 0s 222us/step - loss: 0.0188 - acc: 0.9980 - val_loss: 3.3898 - val_acc: 0.6329\n","Epoch 72/100\n","1000/1000 [==============================] - 0s 187us/step - loss: 0.0188 - acc: 0.9980 - val_loss: 3.3923 - val_acc: 0.6329\n","Epoch 73/100\n","1000/1000 [==============================] - 0s 215us/step - loss: 0.0188 - acc: 0.9980 - val_loss: 3.3942 - val_acc: 0.6329\n","Epoch 74/100\n","1000/1000 [==============================] - 0s 194us/step - loss: 0.0188 - acc: 0.9980 - val_loss: 3.3966 - val_acc: 0.6329\n","Epoch 75/100\n","1000/1000 [==============================] - 0s 210us/step - loss: 0.0187 - acc: 0.9980 - val_loss: 3.3990 - val_acc: 0.6329\n","Epoch 76/100\n","1000/1000 [==============================] - 0s 211us/step - loss: 0.0187 - acc: 0.9980 - val_loss: 3.4008 - val_acc: 0.6329\n","Epoch 77/100\n","1000/1000 [==============================] - 0s 232us/step - loss: 0.0187 - acc: 0.9980 - val_loss: 3.4021 - val_acc: 0.6329\n","Epoch 78/100\n","1000/1000 [==============================] - 0s 214us/step - loss: 0.0187 - acc: 0.9980 - val_loss: 3.4047 - val_acc: 0.6329\n","Epoch 79/100\n","1000/1000 [==============================] - 0s 188us/step - loss: 0.0187 - acc: 0.9980 - val_loss: 3.4063 - val_acc: 0.6329\n","Epoch 80/100\n","1000/1000 [==============================] - 0s 222us/step - loss: 0.0187 - acc: 0.9980 - val_loss: 3.4079 - val_acc: 0.6329\n","Epoch 81/100\n","1000/1000 [==============================] - 0s 226us/step - loss: 0.0187 - acc: 0.9980 - val_loss: 3.4093 - val_acc: 0.6297\n","Epoch 82/100\n","1000/1000 [==============================] - 0s 193us/step - loss: 0.0186 - acc: 0.9980 - val_loss: 3.4109 - val_acc: 0.6297\n","Epoch 83/100\n","1000/1000 [==============================] - 0s 200us/step - loss: 0.0186 - acc: 0.9980 - val_loss: 3.4129 - val_acc: 0.6297\n","Epoch 84/100\n","1000/1000 [==============================] - 0s 238us/step - loss: 0.0186 - acc: 0.9980 - val_loss: 3.4143 - val_acc: 0.6297\n","Epoch 85/100\n","1000/1000 [==============================] - 0s 206us/step - loss: 0.0186 - acc: 0.9980 - val_loss: 3.4153 - val_acc: 0.6297\n","Epoch 86/100\n","1000/1000 [==============================] - 0s 211us/step - loss: 0.0186 - acc: 0.9980 - val_loss: 3.4181 - val_acc: 0.6297\n","Epoch 87/100\n","1000/1000 [==============================] - 0s 226us/step - loss: 0.0186 - acc: 0.9980 - val_loss: 3.4195 - val_acc: 0.6297\n","Epoch 88/100\n","1000/1000 [==============================] - 0s 210us/step - loss: 0.0186 - acc: 0.9980 - val_loss: 3.4211 - val_acc: 0.6297\n","Epoch 89/100\n","1000/1000 [==============================] - 0s 184us/step - loss: 0.0185 - acc: 0.9980 - val_loss: 3.4222 - val_acc: 0.6297\n","Epoch 90/100\n","1000/1000 [==============================] - 0s 227us/step - loss: 0.0185 - acc: 0.9980 - val_loss: 3.4239 - val_acc: 0.6297\n","Epoch 91/100\n","1000/1000 [==============================] - 0s 207us/step - loss: 0.0185 - acc: 0.9980 - val_loss: 3.4258 - val_acc: 0.6297\n","Epoch 92/100\n","1000/1000 [==============================] - 0s 192us/step - loss: 0.0185 - acc: 0.9980 - val_loss: 3.4270 - val_acc: 0.6297\n","Epoch 93/100\n","1000/1000 [==============================] - 0s 193us/step - loss: 0.0185 - acc: 0.9980 - val_loss: 3.4284 - val_acc: 0.6297\n","Epoch 94/100\n","1000/1000 [==============================] - 0s 247us/step - loss: 0.0185 - acc: 0.9980 - val_loss: 3.4303 - val_acc: 0.6297\n","Epoch 95/100\n","1000/1000 [==============================] - 0s 210us/step - loss: 0.0185 - acc: 0.9980 - val_loss: 3.4315 - val_acc: 0.6297\n","Epoch 96/100\n","1000/1000 [==============================] - 0s 178us/step - loss: 0.0185 - acc: 0.9980 - val_loss: 3.4332 - val_acc: 0.6297\n","Epoch 97/100\n","1000/1000 [==============================] - 0s 217us/step - loss: 0.0185 - acc: 0.9980 - val_loss: 3.4341 - val_acc: 0.6297\n","Epoch 98/100\n","1000/1000 [==============================] - 0s 217us/step - loss: 0.0185 - acc: 0.9980 - val_loss: 3.4353 - val_acc: 0.6297\n","Epoch 99/100\n","1000/1000 [==============================] - 0s 190us/step - loss: 0.0184 - acc: 0.9980 - val_loss: 3.4367 - val_acc: 0.6297\n","Epoch 100/100\n","1000/1000 [==============================] - 0s 209us/step - loss: 0.0185 - acc: 0.9980 - val_loss: 3.4383 - val_acc: 0.6297\n"],"name":"stdout"}]},{"metadata":{"id":"JGFdyUWvjTAz","colab_type":"code","outputId":"007b6406-b134-493e-febc-1d110f8237c6","colab":{"base_uri":"https://localhost:8080/","height":3434}},"cell_type":"code","source":["optimizer = optimizers.Adam(lr=0.1)\n","loss = 'categorical_crossentropy'\n","epochs = 100\n","batch_size = 32\n","\n","model.compile(optimizer= optimizer,\n","              loss = loss,\n","              metrics=['acc'])\n","\n","history_5 = model.fit(x_train, y_train,\n","                    epochs=epochs,\n","                    batch_size=batch_size,\n","                    validation_data=(x_val, y_val))\n","\n","model.save_weights('model_5.h5')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 1000 samples, validate on 316 samples\n","Epoch 1/100\n","1000/1000 [==============================] - 1s 841us/step - loss: 7.5744 - acc: 0.5260 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 2/100\n","1000/1000 [==============================] - 0s 235us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 3/100\n","1000/1000 [==============================] - 0s 193us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 4/100\n","1000/1000 [==============================] - 0s 206us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 5/100\n","1000/1000 [==============================] - 0s 181us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 6/100\n","1000/1000 [==============================] - 0s 239us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 7/100\n","1000/1000 [==============================] - 0s 222us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 8/100\n","1000/1000 [==============================] - 0s 200us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 9/100\n","1000/1000 [==============================] - 0s 210us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 10/100\n","1000/1000 [==============================] - 0s 230us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 11/100\n","1000/1000 [==============================] - 0s 218us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 12/100\n","1000/1000 [==============================] - 0s 220us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 13/100\n","1000/1000 [==============================] - 0s 216us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 14/100\n","1000/1000 [==============================] - 0s 240us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 15/100\n","1000/1000 [==============================] - 0s 212us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 16/100\n","1000/1000 [==============================] - 0s 214us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 17/100\n","1000/1000 [==============================] - 0s 202us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 18/100\n","1000/1000 [==============================] - 0s 220us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 19/100\n","1000/1000 [==============================] - 0s 203us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 20/100\n","1000/1000 [==============================] - 0s 205us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 21/100\n","1000/1000 [==============================] - 0s 221us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 22/100\n","1000/1000 [==============================] - 0s 213us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 23/100\n","1000/1000 [==============================] - 0s 226us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 24/100\n","1000/1000 [==============================] - 0s 209us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 25/100\n","1000/1000 [==============================] - 0s 184us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 26/100\n","1000/1000 [==============================] - 0s 212us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 27/100\n","1000/1000 [==============================] - 0s 214us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 28/100\n","1000/1000 [==============================] - 0s 194us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 29/100\n","1000/1000 [==============================] - 0s 200us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 30/100\n","1000/1000 [==============================] - 0s 195us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 31/100\n","1000/1000 [==============================] - 0s 234us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 32/100\n","1000/1000 [==============================] - 0s 193us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 33/100\n","1000/1000 [==============================] - 0s 200us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 34/100\n","1000/1000 [==============================] - 0s 192us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 35/100\n","1000/1000 [==============================] - 0s 209us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 36/100\n","1000/1000 [==============================] - 0s 202us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 37/100\n","1000/1000 [==============================] - 0s 189us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 38/100\n","1000/1000 [==============================] - 0s 205us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 39/100\n","1000/1000 [==============================] - 0s 201us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 40/100\n","1000/1000 [==============================] - 0s 248us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 41/100\n","1000/1000 [==============================] - 0s 205us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 42/100\n","1000/1000 [==============================] - 0s 182us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 43/100\n","1000/1000 [==============================] - 0s 183us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 44/100\n","1000/1000 [==============================] - 0s 219us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 45/100\n","1000/1000 [==============================] - 0s 203us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 46/100\n","1000/1000 [==============================] - 0s 205us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 47/100\n","1000/1000 [==============================] - 0s 195us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 48/100\n","1000/1000 [==============================] - 0s 223us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 49/100\n","1000/1000 [==============================] - 0s 207us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 50/100\n","1000/1000 [==============================] - 0s 195us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 51/100\n","1000/1000 [==============================] - 0s 215us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 52/100\n","1000/1000 [==============================] - 0s 213us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 53/100\n","1000/1000 [==============================] - 0s 208us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 54/100\n","1000/1000 [==============================] - 0s 186us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 55/100\n","1000/1000 [==============================] - 0s 194us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 56/100\n","1000/1000 [==============================] - 0s 211us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 57/100\n","1000/1000 [==============================] - 0s 219us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 58/100\n","1000/1000 [==============================] - 0s 198us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 59/100\n","1000/1000 [==============================] - 0s 198us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 60/100\n","1000/1000 [==============================] - 0s 200us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 61/100\n","1000/1000 [==============================] - 0s 234us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 62/100\n","1000/1000 [==============================] - 0s 197us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 63/100\n","1000/1000 [==============================] - 0s 191us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 64/100\n","1000/1000 [==============================] - 0s 187us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 65/100\n","1000/1000 [==============================] - 0s 221us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 66/100\n","1000/1000 [==============================] - 0s 215us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 67/100\n","1000/1000 [==============================] - 0s 195us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 68/100\n","1000/1000 [==============================] - 0s 200us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 69/100\n","1000/1000 [==============================] - 0s 194us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 70/100\n","1000/1000 [==============================] - 0s 223us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 71/100\n","1000/1000 [==============================] - 0s 213us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 72/100\n","1000/1000 [==============================] - 0s 195us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 73/100\n","1000/1000 [==============================] - 0s 185us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 74/100\n","1000/1000 [==============================] - 0s 226us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 75/100\n","1000/1000 [==============================] - 0s 194us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 76/100\n","1000/1000 [==============================] - 0s 223us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 77/100\n","1000/1000 [==============================] - 0s 185us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 78/100\n","1000/1000 [==============================] - 0s 213us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 79/100\n","1000/1000 [==============================] - 0s 189us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 80/100\n","1000/1000 [==============================] - 0s 191us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 81/100\n","1000/1000 [==============================] - 0s 213us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 82/100\n","1000/1000 [==============================] - 0s 194us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 83/100\n","1000/1000 [==============================] - 0s 211us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 84/100\n","1000/1000 [==============================] - 0s 187us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 85/100\n","1000/1000 [==============================] - 0s 184us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 86/100\n","1000/1000 [==============================] - 0s 192us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 87/100\n","1000/1000 [==============================] - 0s 231us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 88/100\n","1000/1000 [==============================] - 0s 180us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 89/100\n","1000/1000 [==============================] - 0s 178us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 90/100\n","1000/1000 [==============================] - 0s 184us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 91/100\n","1000/1000 [==============================] - 0s 201us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 92/100\n","1000/1000 [==============================] - 0s 218us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 93/100\n","1000/1000 [==============================] - 0s 180us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 94/100\n","1000/1000 [==============================] - 0s 182us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 95/100\n","1000/1000 [==============================] - 0s 196us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 96/100\n","1000/1000 [==============================] - 0s 235us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 97/100\n","1000/1000 [==============================] - 0s 204us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 98/100\n","1000/1000 [==============================] - 0s 192us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 99/100\n","1000/1000 [==============================] - 0s 184us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 100/100\n","1000/1000 [==============================] - 0s 205us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n"],"name":"stdout"}]},{"metadata":{"id":"ib0qbP4vjnAZ","colab_type":"code","outputId":"a45415d5-c66e-4099-f203-c609c9fc8a91","colab":{"base_uri":"https://localhost:8080/","height":6834}},"cell_type":"code","source":["optimizer = optimizers.Adam(lr=0.1)\n","loss = 'categorical_crossentropy'\n","epochs = 200\n","batch_size = 54\n","\n","model.compile(optimizer= optimizer,\n","              loss = loss,\n","              metrics=['acc'])\n","\n","history_6 = model.fit(x_train, y_train,\n","                    epochs=epochs,\n","                    batch_size=batch_size,\n","                    validation_data=(x_val, y_val))\n","\n","model.save_weights('model_6.h5')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 1000 samples, validate on 316 samples\n","Epoch 1/200\n","1000/1000 [==============================] - 1s 763us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 2/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 3/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 4/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 5/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 6/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 7/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 8/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 9/200\n","1000/1000 [==============================] - 0s 91us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 10/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 11/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 12/200\n","1000/1000 [==============================] - 0s 120us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 13/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 14/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 15/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 16/200\n","1000/1000 [==============================] - 0s 121us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 17/200\n","1000/1000 [==============================] - 0s 136us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 18/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 19/200\n","1000/1000 [==============================] - 0s 149us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 20/200\n","1000/1000 [==============================] - 0s 118us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 21/200\n","1000/1000 [==============================] - 0s 123us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 22/200\n","1000/1000 [==============================] - 0s 119us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 23/200\n","1000/1000 [==============================] - 0s 121us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 24/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 25/200\n","1000/1000 [==============================] - 0s 127us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 26/200\n","1000/1000 [==============================] - 0s 126us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 27/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 28/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 29/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 30/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 31/200\n","1000/1000 [==============================] - 0s 118us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 32/200\n","1000/1000 [==============================] - 0s 118us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 33/200\n","1000/1000 [==============================] - 0s 156us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 34/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 35/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 36/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 37/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 38/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 39/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 40/200\n","1000/1000 [==============================] - 0s 147us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 41/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 42/200\n","1000/1000 [==============================] - 0s 126us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 43/200\n","1000/1000 [==============================] - 0s 119us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 44/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 45/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 46/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 47/200\n","1000/1000 [==============================] - 0s 125us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 48/200\n","1000/1000 [==============================] - 0s 125us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 49/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 50/200\n","1000/1000 [==============================] - 0s 133us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 51/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 52/200\n","1000/1000 [==============================] - 0s 125us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 53/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 54/200\n","1000/1000 [==============================] - 0s 126us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 55/200\n","1000/1000 [==============================] - 0s 126us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 56/200\n","1000/1000 [==============================] - 0s 147us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 57/200\n","1000/1000 [==============================] - 0s 123us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 58/200\n","1000/1000 [==============================] - 0s 136us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 59/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 60/200\n","1000/1000 [==============================] - 0s 121us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 61/200\n","1000/1000 [==============================] - 0s 147us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 62/200\n","1000/1000 [==============================] - 0s 119us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 63/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 64/200\n","1000/1000 [==============================] - 0s 118us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 65/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 66/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 67/200\n","1000/1000 [==============================] - 0s 123us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 68/200\n","1000/1000 [==============================] - 0s 129us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 69/200\n","1000/1000 [==============================] - 0s 124us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 70/200\n","1000/1000 [==============================] - 0s 120us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 71/200\n","1000/1000 [==============================] - 0s 117us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 72/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 73/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 74/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 75/200\n","1000/1000 [==============================] - 0s 134us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 76/200\n","1000/1000 [==============================] - 0s 148us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 77/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 78/200\n","1000/1000 [==============================] - 0s 121us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 79/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 80/200\n","1000/1000 [==============================] - 0s 127us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 81/200\n","1000/1000 [==============================] - 0s 124us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 82/200\n","1000/1000 [==============================] - 0s 128us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 83/200\n","1000/1000 [==============================] - 0s 153us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 84/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 85/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 86/200\n","1000/1000 [==============================] - 0s 120us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 87/200\n","1000/1000 [==============================] - 0s 117us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 88/200\n","1000/1000 [==============================] - 0s 118us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 89/200\n","1000/1000 [==============================] - 0s 117us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 90/200\n","1000/1000 [==============================] - 0s 143us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 91/200\n","1000/1000 [==============================] - 0s 122us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 92/200\n","1000/1000 [==============================] - 0s 125us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 93/200\n","1000/1000 [==============================] - 0s 122us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 94/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 95/200\n","1000/1000 [==============================] - 0s 120us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 96/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 97/200\n","1000/1000 [==============================] - 0s 146us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 98/200\n","1000/1000 [==============================] - 0s 117us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 99/200\n","1000/1000 [==============================] - 0s 117us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 100/200\n","1000/1000 [==============================] - 0s 138us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 101/200\n","1000/1000 [==============================] - 0s 117us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 102/200\n","1000/1000 [==============================] - 0s 118us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 103/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 104/200\n","1000/1000 [==============================] - 0s 142us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 105/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 106/200\n","1000/1000 [==============================] - 0s 120us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 107/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 108/200\n","1000/1000 [==============================] - 0s 140us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 109/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 110/200\n","1000/1000 [==============================] - 0s 119us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 111/200\n","1000/1000 [==============================] - 0s 146us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 112/200\n","1000/1000 [==============================] - 0s 117us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 113/200\n","1000/1000 [==============================] - 0s 120us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 114/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 115/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 116/200\n","1000/1000 [==============================] - 0s 130us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 117/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 118/200\n","1000/1000 [==============================] - 0s 130us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 119/200\n","1000/1000 [==============================] - 0s 125us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 120/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 121/200\n","1000/1000 [==============================] - 0s 120us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 122/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 123/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 124/200\n","1000/1000 [==============================] - 0s 122us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 125/200\n","1000/1000 [==============================] - 0s 154us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 126/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 127/200\n","1000/1000 [==============================] - 0s 120us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 128/200\n","1000/1000 [==============================] - 0s 118us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 129/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 130/200\n","1000/1000 [==============================] - 0s 121us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 131/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 132/200\n","1000/1000 [==============================] - 0s 133us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 133/200\n","1000/1000 [==============================] - 0s 143us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 134/200\n","1000/1000 [==============================] - 0s 119us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 135/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 136/200\n","1000/1000 [==============================] - 0s 123us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 137/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 138/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 139/200\n","1000/1000 [==============================] - 0s 152us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 140/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 141/200\n","1000/1000 [==============================] - 0s 123us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 142/200\n","1000/1000 [==============================] - 0s 122us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 143/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 144/200\n","1000/1000 [==============================] - 0s 122us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 145/200\n","1000/1000 [==============================] - 0s 132us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 146/200\n","1000/1000 [==============================] - 0s 146us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 147/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 148/200\n","1000/1000 [==============================] - 0s 130us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 149/200\n","1000/1000 [==============================] - 0s 143us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 150/200\n","1000/1000 [==============================] - 0s 117us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 151/200\n","1000/1000 [==============================] - 0s 120us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 152/200\n","1000/1000 [==============================] - 0s 126us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 153/200\n","1000/1000 [==============================] - 0s 141us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 154/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 155/200\n","1000/1000 [==============================] - 0s 119us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 156/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 157/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 158/200\n","1000/1000 [==============================] - 0s 142us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 159/200\n","1000/1000 [==============================] - 0s 127us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 160/200\n","1000/1000 [==============================] - 0s 141us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 161/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 162/200\n","1000/1000 [==============================] - 0s 122us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 163/200\n","1000/1000 [==============================] - 0s 121us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 164/200\n","1000/1000 [==============================] - 0s 118us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 165/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 166/200\n","1000/1000 [==============================] - 0s 159us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 167/200\n","1000/1000 [==============================] - 0s 128us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 168/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 169/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 170/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 171/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 172/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 173/200\n","1000/1000 [==============================] - 0s 119us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 174/200\n","1000/1000 [==============================] - 0s 155us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 175/200\n","1000/1000 [==============================] - 0s 122us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 176/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 177/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 178/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 179/200\n","1000/1000 [==============================] - 0s 120us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 180/200\n","1000/1000 [==============================] - 0s 124us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 181/200\n","1000/1000 [==============================] - 0s 154us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 182/200\n","1000/1000 [==============================] - 0s 136us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 183/200\n","1000/1000 [==============================] - 0s 117us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 184/200\n","1000/1000 [==============================] - 0s 126us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 185/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 186/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 187/200\n","1000/1000 [==============================] - 0s 118us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 188/200\n","1000/1000 [==============================] - 0s 154us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 189/200\n","1000/1000 [==============================] - 0s 118us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 190/200\n","1000/1000 [==============================] - 0s 144us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 191/200\n","1000/1000 [==============================] - 0s 120us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 192/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 193/200\n","1000/1000 [==============================] - 0s 128us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 194/200\n","1000/1000 [==============================] - 0s 132us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 195/200\n","1000/1000 [==============================] - 0s 124us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 196/200\n","1000/1000 [==============================] - 0s 119us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 197/200\n","1000/1000 [==============================] - 0s 119us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 198/200\n","1000/1000 [==============================] - 0s 138us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 199/200\n","1000/1000 [==============================] - 0s 120us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 200/200\n","1000/1000 [==============================] - 0s 142us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n"],"name":"stdout"}]},{"metadata":{"id":"psNtoKXXoJNA","colab_type":"text"},"cell_type":"markdown","source":["### Plot Confusion Matrix"]},{"metadata":{"id":"Pq-wQIxceaEg","colab_type":"text"},"cell_type":"markdown","source":["### Model 4 delivers the best accuracy"]},{"metadata":{"id":"NQ6p2CMQeVps","colab_type":"code","outputId":"c0ffe569-3cf4-4d33-b43f-aa6ea9a45198","colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["model.load_weights('model_4.h5')\n","model.evaluate(x_test, y_test)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["328/328 [==============================] - 0s 71us/step\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[4.163353454775926, 0.5762195121951219]"]},"metadata":{"tags":[]},"execution_count":63}]},{"metadata":{"id":"Rp3cfiBUmiRT","colab_type":"code","outputId":"ca62d7ce-944b-4628-eede-caf9f211bc3c","colab":{"base_uri":"https://localhost:8080/","height":364}},"cell_type":"code","source":["import seaborn as sns\n","from sklearn import metrics\n","\n","y_pred = model.predict(x_test)\n","matrix = metrics.confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n","sns.heatmap(matrix,annot=True,fmt='.5g')"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f6e8649d908>"]},"metadata":{"tags":[]},"execution_count":64},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAcEAAAFKCAYAAABlzOTzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 576x396 with 2 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"MFnflbeukYQx","colab_type":"text"},"cell_type":"markdown","source":["### Trainig From Scratch"]},{"metadata":{"id":"Dc3z2RBwkM31","colab_type":"code","outputId":"6adbf6ea-e785-463d-ecfb-69029621d16f","colab":{"base_uri":"https://localhost:8080/","height":7089}},"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Embedding, Flatten, Dense\n","\n","model = Sequential()\n","model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n","model.add(Flatten())\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(3, activation='softmax'))\n","model.summary()\n","\n","optimizer = optimizers.Adam(lr=0.1)\n","loss = 'categorical_crossentropy'\n","epochs = 200\n","batch_size = 54\n","\n","model.compile(optimizer= optimizer,\n","              loss = loss,\n","              metrics=['acc'])\n","\n","history_6 = model.fit(x_train, y_train,\n","                    epochs=epochs,\n","                    batch_size=batch_size,\n","                    validation_data=(x_val, y_val))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_4 (Embedding)      (None, 100, 100)          1000000   \n","_________________________________________________________________\n","flatten_4 (Flatten)          (None, 10000)             0         \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 32)                320032    \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 3)                 99        \n","=================================================================\n","Total params: 1,320,131\n","Trainable params: 1,320,131\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 1000 samples, validate on 316 samples\n","Epoch 1/200\n","1000/1000 [==============================] - 1s 947us/step - loss: 8.9635 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 2/200\n","1000/1000 [==============================] - 0s 152us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 3/200\n","1000/1000 [==============================] - 0s 127us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 4/200\n","1000/1000 [==============================] - 0s 132us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 5/200\n","1000/1000 [==============================] - 0s 126us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 6/200\n","1000/1000 [==============================] - 0s 141us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 7/200\n","1000/1000 [==============================] - 0s 136us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 8/200\n","1000/1000 [==============================] - 0s 123us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 9/200\n","1000/1000 [==============================] - 0s 157us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 10/200\n","1000/1000 [==============================] - 0s 154us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 11/200\n","1000/1000 [==============================] - 0s 152us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 12/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 13/200\n","1000/1000 [==============================] - 0s 156us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 14/200\n","1000/1000 [==============================] - 0s 180us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 15/200\n","1000/1000 [==============================] - 0s 148us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 16/200\n","1000/1000 [==============================] - 0s 148us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 17/200\n","1000/1000 [==============================] - 0s 167us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 18/200\n","1000/1000 [==============================] - 0s 153us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 19/200\n","1000/1000 [==============================] - 0s 151us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 20/200\n","1000/1000 [==============================] - 0s 197us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 21/200\n","1000/1000 [==============================] - 0s 158us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 22/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 23/200\n","1000/1000 [==============================] - 0s 156us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 24/200\n","1000/1000 [==============================] - 0s 150us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 25/200\n","1000/1000 [==============================] - 0s 180us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 26/200\n","1000/1000 [==============================] - 0s 167us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 27/200\n","1000/1000 [==============================] - 0s 168us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 28/200\n","1000/1000 [==============================] - 0s 156us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 29/200\n","1000/1000 [==============================] - 0s 150us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 30/200\n","1000/1000 [==============================] - 0s 171us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 31/200\n","1000/1000 [==============================] - 0s 160us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 32/200\n","1000/1000 [==============================] - 0s 168us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 33/200\n","1000/1000 [==============================] - 0s 161us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 34/200\n","1000/1000 [==============================] - 0s 153us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 35/200\n","1000/1000 [==============================] - 0s 155us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 36/200\n","1000/1000 [==============================] - 0s 182us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 37/200\n","1000/1000 [==============================] - 0s 160us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 38/200\n","1000/1000 [==============================] - 0s 159us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 39/200\n","1000/1000 [==============================] - 0s 160us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 40/200\n","1000/1000 [==============================] - 0s 148us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 41/200\n","1000/1000 [==============================] - 0s 166us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 42/200\n","1000/1000 [==============================] - 0s 176us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 43/200\n","1000/1000 [==============================] - 0s 147us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 44/200\n","1000/1000 [==============================] - 0s 149us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 45/200\n","1000/1000 [==============================] - 0s 164us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 46/200\n","1000/1000 [==============================] - 0s 151us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 47/200\n","1000/1000 [==============================] - 0s 168us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 48/200\n","1000/1000 [==============================] - 0s 159us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 49/200\n","1000/1000 [==============================] - 0s 150us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 50/200\n","1000/1000 [==============================] - 0s 159us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 51/200\n","1000/1000 [==============================] - 0s 156us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 52/200\n","1000/1000 [==============================] - 0s 183us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 53/200\n","1000/1000 [==============================] - 0s 163us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 54/200\n","1000/1000 [==============================] - 0s 145us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 55/200\n","1000/1000 [==============================] - 0s 149us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 56/200\n","1000/1000 [==============================] - 0s 149us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 57/200\n","1000/1000 [==============================] - 0s 152us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 58/200\n","1000/1000 [==============================] - 0s 207us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 59/200\n","1000/1000 [==============================] - 0s 152us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 60/200\n","1000/1000 [==============================] - 0s 149us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 61/200\n","1000/1000 [==============================] - 0s 150us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 62/200\n","1000/1000 [==============================] - 0s 146us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 63/200\n","1000/1000 [==============================] - 0s 195us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 64/200\n","1000/1000 [==============================] - 0s 170us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 65/200\n","1000/1000 [==============================] - 0s 150us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 66/200\n","1000/1000 [==============================] - 0s 146us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 67/200\n","1000/1000 [==============================] - 0s 149us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 68/200\n","1000/1000 [==============================] - 0s 168us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 69/200\n","1000/1000 [==============================] - 0s 171us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 70/200\n","1000/1000 [==============================] - 0s 152us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 71/200\n","1000/1000 [==============================] - 0s 164us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 72/200\n","1000/1000 [==============================] - 0s 154us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 73/200\n","1000/1000 [==============================] - 0s 165us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 74/200\n","1000/1000 [==============================] - 0s 183us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 75/200\n","1000/1000 [==============================] - 0s 155us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 76/200\n","1000/1000 [==============================] - 0s 149us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 77/200\n","1000/1000 [==============================] - 0s 168us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 78/200\n","1000/1000 [==============================] - 0s 155us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 79/200\n","1000/1000 [==============================] - 0s 160us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 80/200\n","1000/1000 [==============================] - 0s 151us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 81/200\n","1000/1000 [==============================] - 0s 155us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 82/200\n","1000/1000 [==============================] - 0s 151us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 83/200\n","1000/1000 [==============================] - 0s 148us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 84/200\n","1000/1000 [==============================] - 0s 171us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 85/200\n","1000/1000 [==============================] - 0s 167us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 86/200\n","1000/1000 [==============================] - 0s 152us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 87/200\n","1000/1000 [==============================] - 0s 152us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 88/200\n","1000/1000 [==============================] - 0s 152us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 89/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 90/200\n","1000/1000 [==============================] - 0s 181us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 91/200\n","1000/1000 [==============================] - 0s 147us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 92/200\n","1000/1000 [==============================] - 0s 156us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 93/200\n","1000/1000 [==============================] - 0s 155us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 94/200\n","1000/1000 [==============================] - 0s 163us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 95/200\n","1000/1000 [==============================] - 0s 155us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 96/200\n","1000/1000 [==============================] - 0s 164us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 97/200\n","1000/1000 [==============================] - 0s 161us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 98/200\n","1000/1000 [==============================] - 0s 152us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 99/200\n","1000/1000 [==============================] - 0s 160us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 100/200\n","1000/1000 [==============================] - 0s 157us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 101/200\n","1000/1000 [==============================] - 0s 159us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 102/200\n","1000/1000 [==============================] - 0s 148us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 103/200\n","1000/1000 [==============================] - 0s 165us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 104/200\n","1000/1000 [==============================] - 0s 154us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 105/200\n","1000/1000 [==============================] - 0s 166us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 106/200\n","1000/1000 [==============================] - 0s 148us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 107/200\n","1000/1000 [==============================] - 0s 159us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 108/200\n","1000/1000 [==============================] - 0s 150us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 109/200\n","1000/1000 [==============================] - 0s 148us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 110/200\n","1000/1000 [==============================] - 0s 174us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 111/200\n","1000/1000 [==============================] - 0s 149us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 112/200\n","1000/1000 [==============================] - 0s 157us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 113/200\n","1000/1000 [==============================] - 0s 159us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 114/200\n","1000/1000 [==============================] - 0s 153us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 115/200\n","1000/1000 [==============================] - 0s 171us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 116/200\n","1000/1000 [==============================] - 0s 165us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 117/200\n","1000/1000 [==============================] - 0s 149us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 118/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 119/200\n","1000/1000 [==============================] - 0s 151us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 120/200\n","1000/1000 [==============================] - 0s 156us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 121/200\n","1000/1000 [==============================] - 0s 165us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 122/200\n","1000/1000 [==============================] - 0s 154us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 123/200\n","1000/1000 [==============================] - 0s 160us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 124/200\n","1000/1000 [==============================] - 0s 150us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 125/200\n","1000/1000 [==============================] - 0s 149us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 126/200\n","1000/1000 [==============================] - 0s 159us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 127/200\n","1000/1000 [==============================] - 0s 148us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 128/200\n","1000/1000 [==============================] - 0s 148us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 129/200\n","1000/1000 [==============================] - 0s 178us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 130/200\n","1000/1000 [==============================] - 0s 150us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 131/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 132/200\n","1000/1000 [==============================] - 0s 153us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 133/200\n","1000/1000 [==============================] - 0s 152us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 134/200\n","1000/1000 [==============================] - 0s 161us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 135/200\n","1000/1000 [==============================] - 0s 165us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 136/200\n","1000/1000 [==============================] - 0s 177us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 137/200\n","1000/1000 [==============================] - 0s 160us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 138/200\n","1000/1000 [==============================] - 0s 151us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 139/200\n","1000/1000 [==============================] - 0s 155us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 140/200\n","1000/1000 [==============================] - 0s 177us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 141/200\n","1000/1000 [==============================] - 0s 168us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 142/200\n","1000/1000 [==============================] - 0s 164us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 143/200\n","1000/1000 [==============================] - 0s 148us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 144/200\n","1000/1000 [==============================] - 0s 161us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 145/200\n","1000/1000 [==============================] - 0s 163us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 146/200\n","1000/1000 [==============================] - 0s 161us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 147/200\n","1000/1000 [==============================] - 0s 161us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 148/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 149/200\n","1000/1000 [==============================] - 0s 152us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 150/200\n","1000/1000 [==============================] - 0s 159us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 151/200\n","1000/1000 [==============================] - 0s 166us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 152/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 153/200\n","1000/1000 [==============================] - 0s 153us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 154/200\n","1000/1000 [==============================] - 0s 147us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 155/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 156/200\n","1000/1000 [==============================] - 0s 167us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 157/200\n","1000/1000 [==============================] - 0s 169us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 158/200\n","1000/1000 [==============================] - 0s 152us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 159/200\n","1000/1000 [==============================] - 0s 150us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 160/200\n","1000/1000 [==============================] - 0s 150us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 161/200\n","1000/1000 [==============================] - 0s 175us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 162/200\n","1000/1000 [==============================] - 0s 197us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 163/200\n","1000/1000 [==============================] - 0s 157us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 164/200\n","1000/1000 [==============================] - 0s 152us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 165/200\n","1000/1000 [==============================] - 0s 146us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 166/200\n","1000/1000 [==============================] - 0s 156us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 167/200\n","1000/1000 [==============================] - 0s 198us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 168/200\n","1000/1000 [==============================] - 0s 154us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 169/200\n","1000/1000 [==============================] - 0s 149us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 170/200\n","1000/1000 [==============================] - 0s 145us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 171/200\n","1000/1000 [==============================] - 0s 144us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 172/200\n","1000/1000 [==============================] - 0s 188us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 173/200\n","1000/1000 [==============================] - 0s 154us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 174/200\n","1000/1000 [==============================] - 0s 164us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 175/200\n","1000/1000 [==============================] - 0s 148us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 176/200\n","1000/1000 [==============================] - 0s 156us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 177/200\n","1000/1000 [==============================] - 0s 168us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 178/200\n","1000/1000 [==============================] - 0s 170us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 179/200\n","1000/1000 [==============================] - 0s 150us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 180/200\n","1000/1000 [==============================] - 0s 163us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 181/200\n","1000/1000 [==============================] - 0s 153us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 182/200\n","1000/1000 [==============================] - 0s 158us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 183/200\n","1000/1000 [==============================] - 0s 188us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 184/200\n","1000/1000 [==============================] - 0s 151us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 185/200\n","1000/1000 [==============================] - 0s 148us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 186/200\n","1000/1000 [==============================] - 0s 151us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 187/200\n","1000/1000 [==============================] - 0s 172us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 188/200\n","1000/1000 [==============================] - 0s 176us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 189/200\n","1000/1000 [==============================] - 0s 155us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 190/200\n","1000/1000 [==============================] - 0s 149us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 191/200\n","1000/1000 [==============================] - 0s 156us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 192/200\n","1000/1000 [==============================] - 0s 150us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 193/200\n","1000/1000 [==============================] - 0s 189us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 194/200\n","1000/1000 [==============================] - 0s 162us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 195/200\n","1000/1000 [==============================] - 0s 147us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 196/200\n","1000/1000 [==============================] - 0s 155us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 197/200\n","1000/1000 [==============================] - 0s 154us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 198/200\n","1000/1000 [==============================] - 0s 170us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 199/200\n","1000/1000 [==============================] - 0s 175us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n","Epoch 200/200\n","1000/1000 [==============================] - 0s 148us/step - loss: 9.6870 - acc: 0.3990 - val_loss: 9.7933 - val_acc: 0.3924\n"],"name":"stdout"}]},{"metadata":{"id":"5LKfBluQoRc0","colab_type":"text"},"cell_type":"markdown","source":["#### Training From Scratch seems not a good idea"]},{"metadata":{"id":"KeTwa-jxTgV2","colab_type":"text"},"cell_type":"markdown","source":["## Experiment 2 - Transfer Learning From IMDB - GLOVE"]},{"metadata":{"id":"BUU46I78vB8b","colab_type":"text"},"cell_type":"markdown","source":["### 1. IMBD - Glove"]},{"metadata":{"id":"5JpurVMJGqDJ","colab_type":"text"},"cell_type":"markdown","source":["####1. Load IMBD as Raw Text"]},{"metadata":{"colab_type":"code","id":"2DQyVllMe1Wl","colab":{}},"cell_type":"code","source":["with zipfile.ZipFile('drive/INFO7374_NeuralNetwork&AI/Assignment_3/aclImdb.zip','r') as zip_ref:\n","    zip_ref.extractall('')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kBMYgs36gPAc","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","\n","imdb_dir = 'aclImdb'\n","train_dir = os.path.join(imdb_dir, 'train')\n","\n","labels = []\n","texts = []\n","\n","for label_type in ['neg', 'pos']:\n","    dir_name = os.path.join(train_dir, label_type)\n","    for fname in os.listdir(dir_name):\n","        if fname[-4:] == '.txt':\n","            f = open(os.path.join(dir_name, fname))\n","            texts.append(f.read())\n","            f.close()\n","            if label_type == 'neg':\n","                labels.append(0)\n","            else:\n","                labels.append(1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"C-l2RAA1Nmz-","colab_type":"code","outputId":"285bb1f2-80c3-42bc-ded2-72a46682e94a","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["len(texts)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["25000"]},"metadata":{"tags":[]},"execution_count":68}]},{"metadata":{"id":"K9fhHZAdG2mI","colab_type":"text"},"cell_type":"markdown","source":["####2. Tokenize Data"]},{"metadata":{"id":"Xckgyz2MG5Fj","colab_type":"code","outputId":"effce8af-e75d-4dc3-f8a7-5b6c86493bb4","colab":{"base_uri":"https://localhost:8080/","height":68}},"cell_type":"code","source":["from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","\n","maxlen = 100  # We will cut reviews after 100 words\n","training_samples = 10000  # We will be training on 200 samples\n","validation_samples = 200  # We will be validating on 10000 samples\n","max_words = 10000  # We will only consider the top 10,000 words in the dataset\n","\n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(texts)\n","sequences = tokenizer.texts_to_sequences(texts)\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","data = pad_sequences(sequences, maxlen=maxlen)\n","\n","labels = np.asarray(labels)\n","print('Shape of data tensor:', data.shape)\n","print('Shape of label tensor:', labels.shape)\n","\n","# Split the data into a training set and a validation set\n","# But first, shuffle the data, since we started from data\n","# where sample are ordered (all negative first, then all positive).\n","indices = np.arange(data.shape[0])\n","np.random.shuffle(indices)\n","data = data[indices]\n","labels = labels[indices]\n","\n","x_train_mv = data[:training_samples]\n","y_train_mv = labels[:training_samples]\n","x_val_mv = data[training_samples: training_samples + validation_samples]\n","y_val_mv = labels[training_samples: training_samples + validation_samples]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Found 88582 unique tokens.\n","Shape of data tensor: (25000, 100)\n","Shape of label tensor: (25000,)\n"],"name":"stdout"}]},{"metadata":{"id":"bf9l-hUPIXlw","colab_type":"text"},"cell_type":"markdown","source":["####3. Download and Preprocess GloVe"]},{"metadata":{"id":"yewqiz6jG48_","colab_type":"code","outputId":"add85c6a-505f-455e-c25d-3de6e1b29bba","colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["embeddings_index = {}\n","\n","with open('glove.6B.100d.txt') as f:\n","  for line in f:\n","      values = line.split()\n","      word = values[0]\n","      coefs = np.asarray(values[1:], dtype='float32')\n","      embeddings_index[word] = coefs\n","\n","print('Found %s word vectors.' % len(embeddings_index))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Found 400000 word vectors.\n"],"name":"stdout"}]},{"metadata":{"id":"EoMGct58LnJE","colab_type":"code","colab":{}},"cell_type":"code","source":["embedding_dim = 100\n","\n","embedding_matrix = np.zeros((max_words, embedding_dim))\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if i < max_words:\n","        if embedding_vector is not None:\n","            # Words not found in embedding index will be all-zeros.\n","            embedding_matrix[i] = embedding_vector"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zg5pir-TMcGh","colab_type":"text"},"cell_type":"markdown","source":["####4. Train Model - Freeze Embedding Layer"]},{"metadata":{"id":"kHDCMaQ7Mcfx","colab_type":"code","outputId":"0755e490-c9b1-4acb-aff6-44cbdbbc5261","colab":{"base_uri":"https://localhost:8080/","height":272}},"cell_type":"code","source":["from keras import optimizers\n","from keras.models import Sequential\n","from keras.layers import Embedding, Flatten, Dense\n","\n","model = Sequential()\n","model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n","model.add(Flatten())\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_5 (Embedding)      (None, 100, 100)          1000000   \n","_________________________________________________________________\n","flatten_5 (Flatten)          (None, 10000)             0         \n","_________________________________________________________________\n","dense_9 (Dense)              (None, 32)                320032    \n","_________________________________________________________________\n","dense_10 (Dense)             (None, 1)                 33        \n","=================================================================\n","Total params: 1,320,065\n","Trainable params: 1,320,065\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"Cu_Lm5IGMvaA","colab_type":"text"},"cell_type":"markdown","source":["#### 5. Load the GloVe embeddings in the model"]},{"metadata":{"id":"AS3ZNUIXMuqS","colab_type":"code","colab":{}},"cell_type":"code","source":["model.layers[0].set_weights([embedding_matrix])\n","model.layers[0].trainable = False"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SxXK_5U-M3QO","colab_type":"text"},"cell_type":"markdown","source":["####6. Train and Evaluate - freeze embedding"]},{"metadata":{"id":"X0cRleI2M5bO","colab_type":"code","outputId":"ac0d1edd-3898-4098-f062-83772f7f415d","colab":{"base_uri":"https://localhost:8080/","height":3434}},"cell_type":"code","source":["model.compile(optimizer='rmsprop',\n","              loss='binary_crossentropy',\n","              metrics=['acc'])\n","\n","history_7 = model.fit(x_train_mv, y_train_mv,\n","                    epochs=100,\n","                    batch_size=32,\n","                    validation_data=(x_val_mv, y_val_mv))\n","model.save_weights('pre_trained_glove_model.h5')\n","\n","base_model = model"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 10000 samples, validate on 200 samples\n","Epoch 1/100\n","10000/10000 [==============================] - 2s 228us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 2/100\n","10000/10000 [==============================] - 2s 152us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 3/100\n","10000/10000 [==============================] - 2s 151us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 4/100\n","10000/10000 [==============================] - 2s 150us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 5/100\n","10000/10000 [==============================] - 2s 159us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 6/100\n","10000/10000 [==============================] - 2s 154us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 7/100\n","10000/10000 [==============================] - 1s 147us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 8/100\n","10000/10000 [==============================] - 1s 150us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 9/100\n","10000/10000 [==============================] - 1s 149us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 10/100\n","10000/10000 [==============================] - 1s 147us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 11/100\n","10000/10000 [==============================] - 1s 149us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 12/100\n","10000/10000 [==============================] - 2s 151us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 13/100\n","10000/10000 [==============================] - 1s 147us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 14/100\n","10000/10000 [==============================] - 1s 150us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 15/100\n","10000/10000 [==============================] - 1s 145us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 16/100\n","10000/10000 [==============================] - 1s 148us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 17/100\n","10000/10000 [==============================] - 2s 150us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 18/100\n","10000/10000 [==============================] - 2s 155us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 19/100\n","10000/10000 [==============================] - 1s 149us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 20/100\n","10000/10000 [==============================] - 1s 149us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 21/100\n","10000/10000 [==============================] - 1s 144us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 22/100\n","10000/10000 [==============================] - 1s 142us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 23/100\n","10000/10000 [==============================] - 1s 143us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 24/100\n","10000/10000 [==============================] - 1s 147us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 25/100\n","10000/10000 [==============================] - 1s 147us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 26/100\n","10000/10000 [==============================] - 1s 147us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 27/100\n","10000/10000 [==============================] - 1s 145us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 28/100\n","10000/10000 [==============================] - 1s 145us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 29/100\n","10000/10000 [==============================] - 1s 143us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 30/100\n","10000/10000 [==============================] - 1s 145us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 31/100\n","10000/10000 [==============================] - 1s 150us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 32/100\n","10000/10000 [==============================] - 1s 145us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 33/100\n","10000/10000 [==============================] - 1s 146us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 34/100\n","10000/10000 [==============================] - 1s 139us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 35/100\n","10000/10000 [==============================] - 1s 137us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 36/100\n","10000/10000 [==============================] - 1s 141us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 37/100\n","10000/10000 [==============================] - 1s 138us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 38/100\n","10000/10000 [==============================] - 1s 138us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 39/100\n","10000/10000 [==============================] - 1s 138us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 40/100\n","10000/10000 [==============================] - 1s 138us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 41/100\n","10000/10000 [==============================] - 1s 141us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 42/100\n","10000/10000 [==============================] - 1s 150us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 43/100\n","10000/10000 [==============================] - 2s 172us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 44/100\n","10000/10000 [==============================] - 2s 176us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 45/100\n","10000/10000 [==============================] - 1s 146us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 46/100\n","10000/10000 [==============================] - 1s 149us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 47/100\n","10000/10000 [==============================] - 1s 149us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 48/100\n","10000/10000 [==============================] - 1s 143us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 49/100\n","10000/10000 [==============================] - 1s 147us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 50/100\n","10000/10000 [==============================] - 1s 144us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 51/100\n","10000/10000 [==============================] - 1s 146us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 52/100\n","10000/10000 [==============================] - 2s 152us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 53/100\n","10000/10000 [==============================] - 1s 148us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 54/100\n","10000/10000 [==============================] - 2s 152us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 55/100\n","10000/10000 [==============================] - 1s 146us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 56/100\n","10000/10000 [==============================] - 1s 144us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 57/100\n","10000/10000 [==============================] - 2s 151us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 58/100\n","10000/10000 [==============================] - 2s 151us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 59/100\n","10000/10000 [==============================] - 1s 148us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 60/100\n","10000/10000 [==============================] - 2s 152us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 61/100\n","10000/10000 [==============================] - 2s 157us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 62/100\n","10000/10000 [==============================] - 1s 149us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 63/100\n","10000/10000 [==============================] - 1s 150us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 64/100\n","10000/10000 [==============================] - 2s 152us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 65/100\n","10000/10000 [==============================] - 1s 145us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 66/100\n","10000/10000 [==============================] - 1s 147us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 67/100\n","10000/10000 [==============================] - 1s 146us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 68/100\n","10000/10000 [==============================] - 1s 148us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 69/100\n","10000/10000 [==============================] - 2s 152us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 70/100\n","10000/10000 [==============================] - 1s 147us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 71/100\n","10000/10000 [==============================] - 2s 151us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 72/100\n","10000/10000 [==============================] - 2s 156us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 73/100\n","10000/10000 [==============================] - 2s 157us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 74/100\n","10000/10000 [==============================] - 2s 150us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 75/100\n","10000/10000 [==============================] - 2s 152us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 76/100\n","10000/10000 [==============================] - 2s 150us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 77/100\n","10000/10000 [==============================] - 2s 151us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 78/100\n","10000/10000 [==============================] - 2s 157us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 79/100\n","10000/10000 [==============================] - 2s 154us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 80/100\n","10000/10000 [==============================] - 2s 153us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 81/100\n","10000/10000 [==============================] - 2s 151us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 82/100\n","10000/10000 [==============================] - 2s 153us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 83/100\n","10000/10000 [==============================] - 2s 159us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 84/100\n","10000/10000 [==============================] - 2s 153us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 85/100\n","10000/10000 [==============================] - 2s 150us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 86/100\n","10000/10000 [==============================] - 2s 157us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 87/100\n","10000/10000 [==============================] - 2s 160us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 88/100\n","10000/10000 [==============================] - 2s 163us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 89/100\n","10000/10000 [==============================] - 2s 151us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 90/100\n","10000/10000 [==============================] - 2s 151us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 91/100\n","10000/10000 [==============================] - 2s 156us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 92/100\n","10000/10000 [==============================] - 1s 150us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 93/100\n","10000/10000 [==============================] - 2s 153us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 94/100\n","10000/10000 [==============================] - 2s 160us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 95/100\n","10000/10000 [==============================] - 2s 157us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 96/100\n","10000/10000 [==============================] - 2s 154us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 97/100\n","10000/10000 [==============================] - 1s 148us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 98/100\n","10000/10000 [==============================] - 2s 160us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 99/100\n","10000/10000 [==============================] - 2s 165us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n","Epoch 100/100\n","10000/10000 [==============================] - 2s 170us/step - loss: 8.0015 - acc: 0.4981 - val_loss: 8.3698 - val_acc: 0.4750\n"],"name":"stdout"}]},{"metadata":{"id":"aCqVn98npNSb","colab_type":"text"},"cell_type":"markdown","source":["#### Build a model for Earnings Script Training Based on IMDB"]},{"metadata":{"id":"09OMz2eTp7CM","colab_type":"code","outputId":"88f0df7e-73c7-4b6f-b98a-80b0a34af3fd","colab":{"base_uri":"https://localhost:8080/","height":272}},"cell_type":"code","source":["base_model.load_weights('pre_trained_glove_model.h5')\n","base_model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_5 (Embedding)      (None, 100, 100)          1000000   \n","_________________________________________________________________\n","flatten_5 (Flatten)          (None, 10000)             0         \n","_________________________________________________________________\n","dense_9 (Dense)              (None, 32)                320032    \n","_________________________________________________________________\n","dense_10 (Dense)             (None, 1)                 33        \n","=================================================================\n","Total params: 1,320,065\n","Trainable params: 320,065\n","Non-trainable params: 1,000,000\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"ptgLhdgKpLMp","colab_type":"code","outputId":"1bd833f0-dede-4420-cedf-223fd36a1c21","colab":{"base_uri":"https://localhost:8080/","height":306}},"cell_type":"code","source":["model = Sequential()\n","for layer in base_model.layers[:-1]:\n","    model.add(layer)\n","    \n","for layer in model.layers:\n","  layer.trainable = False\n","  \n","\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(3, activation='softmax'))\n","\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_5 (Embedding)      (None, 100, 100)          1000000   \n","_________________________________________________________________\n","flatten_5 (Flatten)          (None, 10000)             0         \n","_________________________________________________________________\n","dense_9 (Dense)              (None, 32)                320032    \n","_________________________________________________________________\n","dense_11 (Dense)             (None, 32)                1056      \n","_________________________________________________________________\n","dense_12 (Dense)             (None, 3)                 99        \n","=================================================================\n","Total params: 1,321,187\n","Trainable params: 1,155\n","Non-trainable params: 1,320,032\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"oJMsTGycuqjR","colab_type":"text"},"cell_type":"markdown","source":["#### Train use Financial Data"]},{"metadata":{"id":"sJGIjBHrtY5N","colab_type":"code","outputId":"d4c3a1d4-7170-4413-8f3c-3f50fce6328d","colab":{"base_uri":"https://localhost:8080/","height":6834}},"cell_type":"code","source":["optimizer = optimizers.Adam(lr=0.1)\n","loss = 'categorical_crossentropy'\n","epochs = 200\n","batch_size = 54\n","\n","model.compile(optimizer= optimizer,\n","              loss = loss,\n","              metrics=['acc'])\n","\n","history_7 = model.fit(x_train, y_train,\n","                    epochs=epochs,\n","                    batch_size=batch_size,\n","                    validation_data=(x_val, y_val))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 1000 samples, validate on 316 samples\n","Epoch 1/200\n","1000/1000 [==============================] - 1s 977us/step - loss: 8.4017 - acc: 0.4760 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 2/200\n","1000/1000 [==============================] - 0s 89us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 3/200\n","1000/1000 [==============================] - 0s 94us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 4/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 5/200\n","1000/1000 [==============================] - 0s 89us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 6/200\n","1000/1000 [==============================] - 0s 88us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 7/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 8/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 9/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 10/200\n","1000/1000 [==============================] - 0s 91us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 11/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 12/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 13/200\n","1000/1000 [==============================] - 0s 126us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 14/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 15/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 16/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 17/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 18/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 19/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 20/200\n","1000/1000 [==============================] - 0s 173us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 21/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 22/200\n","1000/1000 [==============================] - 0s 117us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 23/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 24/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 25/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 26/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 27/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 28/200\n","1000/1000 [==============================] - 0s 150us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 29/200\n","1000/1000 [==============================] - 0s 123us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 30/200\n","1000/1000 [==============================] - 0s 137us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 31/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 32/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 33/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 34/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 35/200\n","1000/1000 [==============================] - 0s 147us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 36/200\n","1000/1000 [==============================] - 0s 175us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 37/200\n","1000/1000 [==============================] - 0s 149us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 38/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 39/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 40/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 41/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 42/200\n","1000/1000 [==============================] - 0s 195us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 43/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 44/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 45/200\n","1000/1000 [==============================] - 0s 147us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 46/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 47/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 48/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 49/200\n","1000/1000 [==============================] - 0s 177us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 50/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 51/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 52/200\n","1000/1000 [==============================] - 0s 119us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 53/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 54/200\n","1000/1000 [==============================] - 0s 128us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 55/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 56/200\n","1000/1000 [==============================] - 0s 172us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 57/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 58/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 59/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 60/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 61/200\n","1000/1000 [==============================] - 0s 127us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 62/200\n","1000/1000 [==============================] - 0s 124us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 63/200\n","1000/1000 [==============================] - 0s 141us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 64/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 65/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 66/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 67/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 68/200\n","1000/1000 [==============================] - 0s 118us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 69/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 70/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 71/200\n","1000/1000 [==============================] - 0s 165us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 72/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 73/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 74/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 75/200\n","1000/1000 [==============================] - 0s 117us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 76/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 77/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 78/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 79/200\n","1000/1000 [==============================] - 0s 142us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 80/200\n","1000/1000 [==============================] - 0s 121us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 81/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 82/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 83/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 84/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 85/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 86/200\n","1000/1000 [==============================] - 0s 152us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 87/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 88/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 89/200\n","1000/1000 [==============================] - 0s 122us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 90/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 91/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 92/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 93/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 94/200\n","1000/1000 [==============================] - 0s 155us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 95/200\n","1000/1000 [==============================] - 0s 124us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 96/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 97/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 98/200\n","1000/1000 [==============================] - 0s 117us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 99/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 100/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 101/200\n","1000/1000 [==============================] - 0s 155us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 102/200\n","1000/1000 [==============================] - 0s 121us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 103/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 104/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 105/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 106/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 107/200\n","1000/1000 [==============================] - 0s 121us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 108/200\n","1000/1000 [==============================] - 0s 122us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 109/200\n","1000/1000 [==============================] - 0s 164us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 110/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 111/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 112/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 113/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 114/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 115/200\n","1000/1000 [==============================] - 0s 135us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 116/200\n","1000/1000 [==============================] - 0s 147us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 117/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 118/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 119/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 120/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 121/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 122/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 123/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 124/200\n","1000/1000 [==============================] - 0s 154us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 125/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 126/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 127/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 128/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 129/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 130/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 131/200\n","1000/1000 [==============================] - 0s 148us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 132/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 133/200\n","1000/1000 [==============================] - 0s 123us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 134/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 135/200\n","1000/1000 [==============================] - 0s 119us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 136/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 137/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 138/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 139/200\n","1000/1000 [==============================] - 0s 126us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 140/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 141/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 142/200\n","1000/1000 [==============================] - 0s 124us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 143/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 144/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 145/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 146/200\n","1000/1000 [==============================] - 0s 136us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 147/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 148/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 149/200\n","1000/1000 [==============================] - 0s 120us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 150/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 151/200\n","1000/1000 [==============================] - 0s 122us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 152/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 153/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 154/200\n","1000/1000 [==============================] - 0s 159us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 155/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 156/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 157/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 158/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 159/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 160/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 161/200\n","1000/1000 [==============================] - 0s 156us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 162/200\n","1000/1000 [==============================] - 0s 123us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 163/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 164/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 165/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 166/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 167/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 168/200\n","1000/1000 [==============================] - 0s 140us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 169/200\n","1000/1000 [==============================] - 0s 168us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 170/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 171/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 172/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 173/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 174/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 175/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 176/200\n","1000/1000 [==============================] - 0s 130us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 177/200\n","1000/1000 [==============================] - 0s 141us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 178/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 179/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 180/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 181/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 182/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 183/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 184/200\n","1000/1000 [==============================] - 0s 153us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 185/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 186/200\n","1000/1000 [==============================] - 0s 126us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 187/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 188/200\n","1000/1000 [==============================] - 0s 118us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 189/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 190/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 191/200\n","1000/1000 [==============================] - 0s 126us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 192/200\n","1000/1000 [==============================] - 0s 117us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 193/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 194/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 195/200\n","1000/1000 [==============================] - 0s 141us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 196/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 197/200\n","1000/1000 [==============================] - 0s 120us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 198/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 199/200\n","1000/1000 [==============================] - 0s 148us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n","Epoch 200/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 7.9785 - acc: 0.5050 - val_loss: 7.6510 - val_acc: 0.5253\n"],"name":"stdout"}]},{"metadata":{"id":"tIutWJb1RLo7","colab_type":"text"},"cell_type":"markdown","source":["####5. Training From Scratch"]},{"metadata":{"id":"7eBGoy57V3zB","colab_type":"code","outputId":"2d7c0340-4a12-4cc8-f436-4a4558174516","colab":{"base_uri":"https://localhost:8080/","height":7089}},"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Embedding, Flatten, Dense\n","\n","model = Sequential()\n","model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n","model.add(Flatten())\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","model.summary()\n","\n","optimizer = optimizers.SGD(lr=0.01, nesterov=True)\n","\n","model.compile(optimizer= optimizer,\n","              loss='binary_crossentropy',\n","              metrics=['acc'])\n","history_9 = model.fit(x_train_mv, y_train_mv,\n","                    epochs=200,\n","                    batch_size=32,\n","                    validation_data=(x_val_mv, y_val_mv))\n","\n","base_model = model"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_7 (Embedding)      (None, 100, 100)          1000000   \n","_________________________________________________________________\n","flatten_7 (Flatten)          (None, 10000)             0         \n","_________________________________________________________________\n","dense_15 (Dense)             (None, 32)                320032    \n","_________________________________________________________________\n","dense_16 (Dense)             (None, 1)                 33        \n","=================================================================\n","Total params: 1,320,065\n","Trainable params: 1,320,065\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 10000 samples, validate on 200 samples\n","Epoch 1/200\n","10000/10000 [==============================] - 3s 263us/step - loss: 0.6933 - acc: 0.5050 - val_loss: 0.6946 - val_acc: 0.4850\n","Epoch 2/200\n","10000/10000 [==============================] - 2s 184us/step - loss: 0.6919 - acc: 0.5264 - val_loss: 0.6944 - val_acc: 0.4700\n","Epoch 3/200\n","10000/10000 [==============================] - 2s 187us/step - loss: 0.6905 - acc: 0.5468 - val_loss: 0.6948 - val_acc: 0.4700\n","Epoch 4/200\n","10000/10000 [==============================] - 2s 183us/step - loss: 0.6893 - acc: 0.5677 - val_loss: 0.6947 - val_acc: 0.4950\n","Epoch 5/200\n","10000/10000 [==============================] - 2s 187us/step - loss: 0.6880 - acc: 0.5759 - val_loss: 0.6945 - val_acc: 0.5100\n","Epoch 6/200\n","10000/10000 [==============================] - 2s 188us/step - loss: 0.6866 - acc: 0.5907 - val_loss: 0.6944 - val_acc: 0.5150\n","Epoch 7/200\n","10000/10000 [==============================] - 2s 184us/step - loss: 0.6850 - acc: 0.6051 - val_loss: 0.6938 - val_acc: 0.5300\n","Epoch 8/200\n","10000/10000 [==============================] - 2s 182us/step - loss: 0.6833 - acc: 0.6242 - val_loss: 0.6932 - val_acc: 0.5200\n","Epoch 9/200\n","10000/10000 [==============================] - 2s 181us/step - loss: 0.6813 - acc: 0.6288 - val_loss: 0.6925 - val_acc: 0.5300\n","Epoch 10/200\n","10000/10000 [==============================] - 2s 186us/step - loss: 0.6789 - acc: 0.6396 - val_loss: 0.6914 - val_acc: 0.5300\n","Epoch 11/200\n","10000/10000 [==============================] - 2s 183us/step - loss: 0.6761 - acc: 0.6630 - val_loss: 0.6895 - val_acc: 0.5450\n","Epoch 12/200\n","10000/10000 [==============================] - 2s 184us/step - loss: 0.6727 - acc: 0.6780 - val_loss: 0.6872 - val_acc: 0.5600\n","Epoch 13/200\n","10000/10000 [==============================] - 2s 180us/step - loss: 0.6684 - acc: 0.6910 - val_loss: 0.6835 - val_acc: 0.5850\n","Epoch 14/200\n","10000/10000 [==============================] - 2s 180us/step - loss: 0.6632 - acc: 0.6943 - val_loss: 0.6806 - val_acc: 0.6050\n","Epoch 15/200\n","10000/10000 [==============================] - 2s 181us/step - loss: 0.6568 - acc: 0.7020 - val_loss: 0.6749 - val_acc: 0.6250\n","Epoch 16/200\n","10000/10000 [==============================] - 2s 180us/step - loss: 0.6490 - acc: 0.7098 - val_loss: 0.6697 - val_acc: 0.6450\n","Epoch 17/200\n","10000/10000 [==============================] - 2s 182us/step - loss: 0.6399 - acc: 0.7103 - val_loss: 0.6614 - val_acc: 0.6650\n","Epoch 18/200\n","10000/10000 [==============================] - 2s 185us/step - loss: 0.6291 - acc: 0.7165 - val_loss: 0.6524 - val_acc: 0.6700\n","Epoch 19/200\n","10000/10000 [==============================] - 2s 188us/step - loss: 0.6169 - acc: 0.7207 - val_loss: 0.6425 - val_acc: 0.6800\n","Epoch 20/200\n","10000/10000 [==============================] - 2s 184us/step - loss: 0.6033 - acc: 0.7287 - val_loss: 0.6334 - val_acc: 0.6700\n","Epoch 21/200\n","10000/10000 [==============================] - 2s 181us/step - loss: 0.5883 - acc: 0.7364 - val_loss: 0.6206 - val_acc: 0.6850\n","Epoch 22/200\n","10000/10000 [==============================] - 2s 184us/step - loss: 0.5724 - acc: 0.7434 - val_loss: 0.6101 - val_acc: 0.7000\n","Epoch 23/200\n","10000/10000 [==============================] - 2s 183us/step - loss: 0.5550 - acc: 0.7578 - val_loss: 0.5996 - val_acc: 0.6950\n","Epoch 24/200\n","10000/10000 [==============================] - 2s 188us/step - loss: 0.5366 - acc: 0.7681 - val_loss: 0.5847 - val_acc: 0.7050\n","Epoch 25/200\n","10000/10000 [==============================] - 2s 182us/step - loss: 0.5167 - acc: 0.7788 - val_loss: 0.5725 - val_acc: 0.7200\n","Epoch 26/200\n","10000/10000 [==============================] - 2s 186us/step - loss: 0.4960 - acc: 0.7956 - val_loss: 0.5594 - val_acc: 0.7150\n","Epoch 27/200\n","10000/10000 [==============================] - 2s 186us/step - loss: 0.4747 - acc: 0.8103 - val_loss: 0.5506 - val_acc: 0.7050\n","Epoch 28/200\n","10000/10000 [==============================] - 2s 182us/step - loss: 0.4528 - acc: 0.8232 - val_loss: 0.5361 - val_acc: 0.7250\n","Epoch 29/200\n","10000/10000 [==============================] - 2s 184us/step - loss: 0.4307 - acc: 0.8394 - val_loss: 0.5234 - val_acc: 0.7450\n","Epoch 30/200\n","10000/10000 [==============================] - 2s 183us/step - loss: 0.4088 - acc: 0.8502 - val_loss: 0.5134 - val_acc: 0.7350\n","Epoch 31/200\n","10000/10000 [==============================] - 2s 186us/step - loss: 0.3877 - acc: 0.8603 - val_loss: 0.5116 - val_acc: 0.7450\n","Epoch 32/200\n","10000/10000 [==============================] - 2s 184us/step - loss: 0.3677 - acc: 0.8685 - val_loss: 0.4971 - val_acc: 0.7400\n","Epoch 33/200\n","10000/10000 [==============================] - 2s 185us/step - loss: 0.3478 - acc: 0.8803 - val_loss: 0.4960 - val_acc: 0.7600\n","Epoch 34/200\n","10000/10000 [==============================] - 2s 186us/step - loss: 0.3294 - acc: 0.8900 - val_loss: 0.4933 - val_acc: 0.7600\n","Epoch 35/200\n","10000/10000 [==============================] - 2s 185us/step - loss: 0.3116 - acc: 0.8972 - val_loss: 0.4845 - val_acc: 0.7600\n","Epoch 36/200\n","10000/10000 [==============================] - 2s 189us/step - loss: 0.2943 - acc: 0.9049 - val_loss: 0.4820 - val_acc: 0.7700\n","Epoch 37/200\n","10000/10000 [==============================] - 2s 184us/step - loss: 0.2774 - acc: 0.9113 - val_loss: 0.4837 - val_acc: 0.7800\n","Epoch 38/200\n","10000/10000 [==============================] - 2s 186us/step - loss: 0.2624 - acc: 0.9185 - val_loss: 0.4821 - val_acc: 0.7850\n","Epoch 39/200\n","10000/10000 [==============================] - 2s 181us/step - loss: 0.2465 - acc: 0.9291 - val_loss: 0.4856 - val_acc: 0.7750\n","Epoch 40/200\n","10000/10000 [==============================] - 2s 183us/step - loss: 0.2328 - acc: 0.9347 - val_loss: 0.4831 - val_acc: 0.7900\n","Epoch 41/200\n","10000/10000 [==============================] - 2s 183us/step - loss: 0.2183 - acc: 0.9410 - val_loss: 0.4845 - val_acc: 0.7850\n","Epoch 42/200\n","10000/10000 [==============================] - 2s 180us/step - loss: 0.2042 - acc: 0.9476 - val_loss: 0.4938 - val_acc: 0.7750\n","Epoch 43/200\n","10000/10000 [==============================] - 2s 177us/step - loss: 0.1914 - acc: 0.9534 - val_loss: 0.4792 - val_acc: 0.7900\n","Epoch 44/200\n","10000/10000 [==============================] - 2s 186us/step - loss: 0.1779 - acc: 0.9605 - val_loss: 0.4935 - val_acc: 0.7700\n","Epoch 45/200\n","10000/10000 [==============================] - 2s 184us/step - loss: 0.1665 - acc: 0.9644 - val_loss: 0.5017 - val_acc: 0.7750\n","Epoch 46/200\n","10000/10000 [==============================] - 2s 184us/step - loss: 0.1549 - acc: 0.9700 - val_loss: 0.4814 - val_acc: 0.8000\n","Epoch 47/200\n","10000/10000 [==============================] - 2s 187us/step - loss: 0.1450 - acc: 0.9743 - val_loss: 0.4863 - val_acc: 0.7700\n","Epoch 48/200\n","10000/10000 [==============================] - 2s 182us/step - loss: 0.1343 - acc: 0.9788 - val_loss: 0.4897 - val_acc: 0.7700\n","Epoch 49/200\n","10000/10000 [==============================] - 2s 181us/step - loss: 0.1257 - acc: 0.9812 - val_loss: 0.4950 - val_acc: 0.7750\n","Epoch 50/200\n","10000/10000 [==============================] - 2s 183us/step - loss: 0.1165 - acc: 0.9851 - val_loss: 0.4997 - val_acc: 0.7850\n","Epoch 51/200\n","10000/10000 [==============================] - 2s 183us/step - loss: 0.1074 - acc: 0.9882 - val_loss: 0.5050 - val_acc: 0.7800\n","Epoch 52/200\n","10000/10000 [==============================] - 2s 181us/step - loss: 0.0996 - acc: 0.9908 - val_loss: 0.5020 - val_acc: 0.7750\n","Epoch 53/200\n","10000/10000 [==============================] - 2s 190us/step - loss: 0.0927 - acc: 0.9933 - val_loss: 0.5050 - val_acc: 0.7800\n","Epoch 54/200\n","10000/10000 [==============================] - 2s 196us/step - loss: 0.0855 - acc: 0.9946 - val_loss: 0.5042 - val_acc: 0.7600\n","Epoch 55/200\n","10000/10000 [==============================] - 2s 195us/step - loss: 0.0790 - acc: 0.9958 - val_loss: 0.5131 - val_acc: 0.7900\n","Epoch 56/200\n","10000/10000 [==============================] - 2s 190us/step - loss: 0.0739 - acc: 0.9961 - val_loss: 0.5140 - val_acc: 0.7950\n","Epoch 57/200\n","10000/10000 [==============================] - 2s 184us/step - loss: 0.0692 - acc: 0.9968 - val_loss: 0.5241 - val_acc: 0.7950\n","Epoch 58/200\n","10000/10000 [==============================] - 2s 183us/step - loss: 0.0637 - acc: 0.9978 - val_loss: 0.5178 - val_acc: 0.7800\n","Epoch 59/200\n","10000/10000 [==============================] - 2s 181us/step - loss: 0.0586 - acc: 0.9983 - val_loss: 0.5280 - val_acc: 0.7900\n","Epoch 60/200\n","10000/10000 [==============================] - 2s 181us/step - loss: 0.0549 - acc: 0.9985 - val_loss: 0.5341 - val_acc: 0.7950\n","Epoch 61/200\n","10000/10000 [==============================] - 2s 183us/step - loss: 0.0515 - acc: 0.9988 - val_loss: 0.5360 - val_acc: 0.7950\n","Epoch 62/200\n","10000/10000 [==============================] - 2s 181us/step - loss: 0.0475 - acc: 0.9994 - val_loss: 0.5379 - val_acc: 0.7850\n","Epoch 63/200\n","10000/10000 [==============================] - 2s 180us/step - loss: 0.0448 - acc: 0.9993 - val_loss: 0.5363 - val_acc: 0.7750\n","Epoch 64/200\n","10000/10000 [==============================] - 2s 186us/step - loss: 0.0419 - acc: 0.9993 - val_loss: 0.5430 - val_acc: 0.7800\n","Epoch 65/200\n","10000/10000 [==============================] - 2s 179us/step - loss: 0.0394 - acc: 0.9996 - val_loss: 0.5562 - val_acc: 0.8000\n","Epoch 66/200\n","10000/10000 [==============================] - 2s 182us/step - loss: 0.0373 - acc: 0.9993 - val_loss: 0.5465 - val_acc: 0.7700\n","Epoch 67/200\n","10000/10000 [==============================] - 2s 179us/step - loss: 0.0346 - acc: 0.9998 - val_loss: 0.5622 - val_acc: 0.8050\n","Epoch 68/200\n","10000/10000 [==============================] - 2s 181us/step - loss: 0.0332 - acc: 0.9996 - val_loss: 0.5542 - val_acc: 0.7800\n","Epoch 69/200\n","10000/10000 [==============================] - 2s 182us/step - loss: 0.0309 - acc: 0.9998 - val_loss: 0.5596 - val_acc: 0.7750\n","Epoch 70/200\n","10000/10000 [==============================] - 2s 182us/step - loss: 0.0296 - acc: 0.9998 - val_loss: 0.5917 - val_acc: 0.7850\n","Epoch 71/200\n","10000/10000 [==============================] - 2s 190us/step - loss: 0.0278 - acc: 0.9999 - val_loss: 0.5744 - val_acc: 0.7950\n","Epoch 72/200\n","10000/10000 [==============================] - 2s 183us/step - loss: 0.0263 - acc: 0.9999 - val_loss: 0.5735 - val_acc: 0.7900\n","Epoch 73/200\n","10000/10000 [==============================] - 2s 179us/step - loss: 0.0252 - acc: 0.9997 - val_loss: 0.5768 - val_acc: 0.7850\n","Epoch 74/200\n","10000/10000 [==============================] - 2s 183us/step - loss: 0.0239 - acc: 0.9998 - val_loss: 0.5775 - val_acc: 0.7800\n","Epoch 75/200\n","10000/10000 [==============================] - 2s 191us/step - loss: 0.0225 - acc: 1.0000 - val_loss: 0.5843 - val_acc: 0.7900\n","Epoch 76/200\n","10000/10000 [==============================] - 2s 196us/step - loss: 0.0217 - acc: 0.9999 - val_loss: 0.5850 - val_acc: 0.7800\n","Epoch 77/200\n","10000/10000 [==============================] - 2s 193us/step - loss: 0.0205 - acc: 1.0000 - val_loss: 0.5850 - val_acc: 0.7850\n","Epoch 78/200\n","10000/10000 [==============================] - 2s 187us/step - loss: 0.0195 - acc: 1.0000 - val_loss: 0.5893 - val_acc: 0.7850\n","Epoch 79/200\n","10000/10000 [==============================] - 2s 193us/step - loss: 0.0191 - acc: 0.9998 - val_loss: 0.5949 - val_acc: 0.7850\n","Epoch 80/200\n","10000/10000 [==============================] - 2s 186us/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.5952 - val_acc: 0.7800\n","Epoch 81/200\n","10000/10000 [==============================] - 2s 179us/step - loss: 0.0172 - acc: 1.0000 - val_loss: 0.5994 - val_acc: 0.7850\n","Epoch 82/200\n","10000/10000 [==============================] - 2s 181us/step - loss: 0.0166 - acc: 1.0000 - val_loss: 0.5992 - val_acc: 0.7850\n","Epoch 83/200\n","10000/10000 [==============================] - 2s 182us/step - loss: 0.0159 - acc: 1.0000 - val_loss: 0.6064 - val_acc: 0.7850\n","Epoch 84/200\n","10000/10000 [==============================] - 2s 179us/step - loss: 0.0155 - acc: 0.9998 - val_loss: 0.6066 - val_acc: 0.7850\n","Epoch 85/200\n","10000/10000 [==============================] - 2s 181us/step - loss: 0.0150 - acc: 0.9999 - val_loss: 0.6084 - val_acc: 0.7850\n","Epoch 86/200\n","10000/10000 [==============================] - 2s 185us/step - loss: 0.0141 - acc: 1.0000 - val_loss: 0.6142 - val_acc: 0.7900\n","Epoch 87/200\n","10000/10000 [==============================] - 2s 184us/step - loss: 0.0136 - acc: 0.9999 - val_loss: 0.6110 - val_acc: 0.7750\n","Epoch 88/200\n","10000/10000 [==============================] - 2s 184us/step - loss: 0.0132 - acc: 1.0000 - val_loss: 0.6176 - val_acc: 0.7850\n","Epoch 89/200\n","10000/10000 [==============================] - 2s 181us/step - loss: 0.0127 - acc: 1.0000 - val_loss: 0.6202 - val_acc: 0.7850\n","Epoch 90/200\n","10000/10000 [==============================] - 2s 183us/step - loss: 0.0123 - acc: 1.0000 - val_loss: 0.6211 - val_acc: 0.7850\n","Epoch 91/200\n","10000/10000 [==============================] - 2s 187us/step - loss: 0.0119 - acc: 1.0000 - val_loss: 0.6237 - val_acc: 0.7850\n","Epoch 92/200\n","10000/10000 [==============================] - 2s 180us/step - loss: 0.0116 - acc: 0.9999 - val_loss: 0.6260 - val_acc: 0.7850\n","Epoch 93/200\n","10000/10000 [==============================] - 2s 180us/step - loss: 0.0112 - acc: 1.0000 - val_loss: 0.6320 - val_acc: 0.7900\n","Epoch 94/200\n","10000/10000 [==============================] - 2s 178us/step - loss: 0.0108 - acc: 1.0000 - val_loss: 0.6296 - val_acc: 0.7850\n","Epoch 95/200\n","10000/10000 [==============================] - 2s 182us/step - loss: 0.0105 - acc: 1.0000 - val_loss: 0.6339 - val_acc: 0.7900\n","Epoch 96/200\n","10000/10000 [==============================] - 2s 179us/step - loss: 0.0102 - acc: 1.0000 - val_loss: 0.6418 - val_acc: 0.7950\n","Epoch 97/200\n","10000/10000 [==============================] - 2s 184us/step - loss: 0.0099 - acc: 1.0000 - val_loss: 0.6373 - val_acc: 0.7850\n","Epoch 98/200\n","10000/10000 [==============================] - 2s 181us/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.6392 - val_acc: 0.7850\n","Epoch 99/200\n","10000/10000 [==============================] - 2s 181us/step - loss: 0.0093 - acc: 1.0000 - val_loss: 0.6449 - val_acc: 0.7900\n","Epoch 100/200\n","10000/10000 [==============================] - 2s 181us/step - loss: 0.0090 - acc: 1.0000 - val_loss: 0.6451 - val_acc: 0.7850\n","Epoch 101/200\n","10000/10000 [==============================] - 2s 182us/step - loss: 0.0088 - acc: 1.0000 - val_loss: 0.6476 - val_acc: 0.7900\n","Epoch 102/200\n","10000/10000 [==============================] - 2s 186us/step - loss: 0.0086 - acc: 1.0000 - val_loss: 0.6483 - val_acc: 0.7850\n","Epoch 103/200\n","10000/10000 [==============================] - 2s 183us/step - loss: 0.0084 - acc: 1.0000 - val_loss: 0.6465 - val_acc: 0.7850\n","Epoch 104/200\n","10000/10000 [==============================] - 2s 182us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.6512 - val_acc: 0.7850\n","Epoch 105/200\n","10000/10000 [==============================] - 2s 182us/step - loss: 0.0079 - acc: 1.0000 - val_loss: 0.6571 - val_acc: 0.7900\n","Epoch 106/200\n","10000/10000 [==============================] - 2s 183us/step - loss: 0.0078 - acc: 1.0000 - val_loss: 0.6580 - val_acc: 0.7900\n","Epoch 107/200\n","10000/10000 [==============================] - 2s 189us/step - loss: 0.0075 - acc: 1.0000 - val_loss: 0.6594 - val_acc: 0.7900\n","Epoch 108/200\n","10000/10000 [==============================] - 2s 184us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.6598 - val_acc: 0.7900\n","Epoch 109/200\n","10000/10000 [==============================] - 2s 182us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.6611 - val_acc: 0.7900\n","Epoch 110/200\n","10000/10000 [==============================] - 2s 183us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.6632 - val_acc: 0.7900\n","Epoch 111/200\n","10000/10000 [==============================] - 2s 187us/step - loss: 0.0069 - acc: 1.0000 - val_loss: 0.6648 - val_acc: 0.7900\n","Epoch 112/200\n","10000/10000 [==============================] - 2s 184us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.6689 - val_acc: 0.7900\n","Epoch 113/200\n","10000/10000 [==============================] - 2s 179us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.6678 - val_acc: 0.7900\n","Epoch 114/200\n","10000/10000 [==============================] - 2s 189us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 0.6684 - val_acc: 0.7850\n","Epoch 115/200\n","10000/10000 [==============================] - 2s 190us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.6713 - val_acc: 0.7900\n","Epoch 116/200\n","10000/10000 [==============================] - 2s 185us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 0.6739 - val_acc: 0.7900\n","Epoch 117/200\n","10000/10000 [==============================] - 2s 184us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 0.6734 - val_acc: 0.7900\n","Epoch 118/200\n","10000/10000 [==============================] - 2s 179us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.6753 - val_acc: 0.7900\n","Epoch 119/200\n","10000/10000 [==============================] - 2s 183us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.6792 - val_acc: 0.7900\n","Epoch 120/200\n","10000/10000 [==============================] - 2s 184us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.6800 - val_acc: 0.7900\n","Epoch 121/200\n","10000/10000 [==============================] - 2s 192us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.6830 - val_acc: 0.7900\n","Epoch 122/200\n","10000/10000 [==============================] - 2s 180us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.6836 - val_acc: 0.7900\n","Epoch 123/200\n","10000/10000 [==============================] - 2s 178us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.6854 - val_acc: 0.7900\n","Epoch 124/200\n","10000/10000 [==============================] - 2s 179us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.6899 - val_acc: 0.7900\n","Epoch 125/200\n","10000/10000 [==============================] - 2s 184us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.6883 - val_acc: 0.7850\n","Epoch 126/200\n","10000/10000 [==============================] - 2s 179us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.6921 - val_acc: 0.7900\n","Epoch 127/200\n","10000/10000 [==============================] - 2s 181us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.6890 - val_acc: 0.7900\n","Epoch 128/200\n","10000/10000 [==============================] - 2s 181us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.6917 - val_acc: 0.7900\n","Epoch 129/200\n","10000/10000 [==============================] - 2s 185us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.6958 - val_acc: 0.7900\n","Epoch 130/200\n","10000/10000 [==============================] - 2s 189us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.6958 - val_acc: 0.7900\n","Epoch 131/200\n","10000/10000 [==============================] - 2s 183us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.6954 - val_acc: 0.7900\n","Epoch 132/200\n","10000/10000 [==============================] - 2s 184us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 0.6991 - val_acc: 0.7900\n","Epoch 133/200\n","10000/10000 [==============================] - 2s 182us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 0.7014 - val_acc: 0.7900\n","Epoch 134/200\n","10000/10000 [==============================] - 2s 180us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.6996 - val_acc: 0.7900\n","Epoch 135/200\n","10000/10000 [==============================] - 2s 183us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.7016 - val_acc: 0.7900\n","Epoch 136/200\n","10000/10000 [==============================] - 2s 189us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.7062 - val_acc: 0.7900\n","Epoch 137/200\n","10000/10000 [==============================] - 2s 189us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.7051 - val_acc: 0.7900\n","Epoch 138/200\n","10000/10000 [==============================] - 2s 189us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.7059 - val_acc: 0.7900\n","Epoch 139/200\n","10000/10000 [==============================] - 2s 191us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.7078 - val_acc: 0.7900\n","Epoch 140/200\n","10000/10000 [==============================] - 2s 187us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.7109 - val_acc: 0.7900\n","Epoch 141/200\n","10000/10000 [==============================] - 2s 187us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.7086 - val_acc: 0.7900\n","Epoch 142/200\n","10000/10000 [==============================] - 2s 196us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.7097 - val_acc: 0.7900\n","Epoch 143/200\n","10000/10000 [==============================] - 2s 193us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.7101 - val_acc: 0.7900\n","Epoch 144/200\n","10000/10000 [==============================] - 2s 196us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.7143 - val_acc: 0.7900\n","Epoch 145/200\n","10000/10000 [==============================] - 2s 195us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.7119 - val_acc: 0.7900\n","Epoch 146/200\n","10000/10000 [==============================] - 2s 186us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 0.7144 - val_acc: 0.7900\n","Epoch 147/200\n","10000/10000 [==============================] - 2s 196us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 0.7189 - val_acc: 0.7900\n","Epoch 148/200\n","10000/10000 [==============================] - 2s 187us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.7178 - val_acc: 0.7850\n","Epoch 149/200\n","10000/10000 [==============================] - 2s 188us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.7196 - val_acc: 0.7900\n","Epoch 150/200\n","10000/10000 [==============================] - 2s 188us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.7203 - val_acc: 0.7900\n","Epoch 151/200\n","10000/10000 [==============================] - 2s 184us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.7218 - val_acc: 0.7900\n","Epoch 152/200\n","10000/10000 [==============================] - 2s 180us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.7217 - val_acc: 0.7900\n","Epoch 153/200\n","10000/10000 [==============================] - 2s 180us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.7246 - val_acc: 0.7900\n","Epoch 154/200\n","10000/10000 [==============================] - 2s 180us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.7250 - val_acc: 0.7900\n","Epoch 155/200\n","10000/10000 [==============================] - 2s 182us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.7264 - val_acc: 0.7900\n","Epoch 156/200\n","10000/10000 [==============================] - 2s 180us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.7263 - val_acc: 0.7900\n","Epoch 157/200\n","10000/10000 [==============================] - 2s 182us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.7290 - val_acc: 0.7900\n","Epoch 158/200\n","10000/10000 [==============================] - 2s 182us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.7275 - val_acc: 0.7900\n","Epoch 159/200\n","10000/10000 [==============================] - 2s 186us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.7283 - val_acc: 0.7900\n","Epoch 160/200\n","10000/10000 [==============================] - 2s 185us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.7318 - val_acc: 0.7900\n","Epoch 161/200\n","10000/10000 [==============================] - 2s 178us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.7319 - val_acc: 0.7900\n","Epoch 162/200\n","10000/10000 [==============================] - 2s 168us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.7331 - val_acc: 0.7900\n","Epoch 163/200\n","10000/10000 [==============================] - 2s 174us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.7360 - val_acc: 0.7900\n","Epoch 164/200\n","10000/10000 [==============================] - 2s 172us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.7358 - val_acc: 0.7900\n","Epoch 165/200\n","10000/10000 [==============================] - 2s 168us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.7382 - val_acc: 0.7900\n","Epoch 166/200\n","10000/10000 [==============================] - 2s 174us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.7368 - val_acc: 0.7900\n","Epoch 167/200\n","10000/10000 [==============================] - 2s 172us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.7351 - val_acc: 0.7900\n","Epoch 168/200\n","10000/10000 [==============================] - 2s 170us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.7390 - val_acc: 0.7900\n","Epoch 169/200\n","10000/10000 [==============================] - 2s 172us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.7387 - val_acc: 0.7900\n","Epoch 170/200\n","10000/10000 [==============================] - 2s 174us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.7423 - val_acc: 0.7900\n","Epoch 171/200\n","10000/10000 [==============================] - 2s 174us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.7408 - val_acc: 0.7900\n","Epoch 172/200\n","10000/10000 [==============================] - 2s 173us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.7466 - val_acc: 0.7950\n","Epoch 173/200\n","10000/10000 [==============================] - 2s 170us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.7435 - val_acc: 0.7900\n","Epoch 174/200\n","10000/10000 [==============================] - 2s 176us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.7446 - val_acc: 0.7900\n","Epoch 175/200\n","10000/10000 [==============================] - 2s 181us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.7450 - val_acc: 0.7900\n","Epoch 176/200\n","10000/10000 [==============================] - 2s 178us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.7490 - val_acc: 0.7900\n","Epoch 177/200\n","10000/10000 [==============================] - 2s 174us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.7467 - val_acc: 0.7900\n","Epoch 178/200\n","10000/10000 [==============================] - 2s 172us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.7474 - val_acc: 0.7900\n","Epoch 179/200\n","10000/10000 [==============================] - 2s 168us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.7498 - val_acc: 0.7900\n","Epoch 180/200\n","10000/10000 [==============================] - 2s 170us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.7524 - val_acc: 0.7900\n","Epoch 181/200\n","10000/10000 [==============================] - 2s 167us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.7510 - val_acc: 0.7900\n","Epoch 182/200\n","10000/10000 [==============================] - 2s 168us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.7521 - val_acc: 0.7900\n","Epoch 183/200\n","10000/10000 [==============================] - 2s 169us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.7537 - val_acc: 0.7900\n","Epoch 184/200\n","10000/10000 [==============================] - 2s 172us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.7528 - val_acc: 0.7900\n","Epoch 185/200\n","10000/10000 [==============================] - 2s 174us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.7551 - val_acc: 0.7900\n","Epoch 186/200\n","10000/10000 [==============================] - 2s 175us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.7542 - val_acc: 0.7900\n","Epoch 187/200\n","10000/10000 [==============================] - 2s 177us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.7552 - val_acc: 0.7850\n","Epoch 188/200\n","10000/10000 [==============================] - 2s 181us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.7556 - val_acc: 0.7900\n","Epoch 189/200\n","10000/10000 [==============================] - 2s 183us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.7565 - val_acc: 0.7900\n","Epoch 190/200\n","10000/10000 [==============================] - 2s 179us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.7592 - val_acc: 0.7900\n","Epoch 191/200\n","10000/10000 [==============================] - 2s 175us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.7594 - val_acc: 0.7900\n","Epoch 192/200\n","10000/10000 [==============================] - 2s 175us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.7617 - val_acc: 0.7900\n","Epoch 193/200\n","10000/10000 [==============================] - 2s 173us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.7599 - val_acc: 0.7900\n","Epoch 194/200\n","10000/10000 [==============================] - 2s 178us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.7623 - val_acc: 0.7900\n","Epoch 195/200\n","10000/10000 [==============================] - 2s 175us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.7632 - val_acc: 0.7900\n","Epoch 196/200\n","10000/10000 [==============================] - 2s 176us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.7642 - val_acc: 0.7900\n","Epoch 197/200\n","10000/10000 [==============================] - 2s 175us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.7632 - val_acc: 0.7900\n","Epoch 198/200\n","10000/10000 [==============================] - 2s 180us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.7652 - val_acc: 0.7900\n","Epoch 199/200\n","10000/10000 [==============================] - 2s 176us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.7663 - val_acc: 0.7900\n","Epoch 200/200\n","10000/10000 [==============================] - 2s 176us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.7653 - val_acc: 0.7900\n"],"name":"stdout"}]},{"metadata":{"id":"XkCWGlMawFG2","colab_type":"code","outputId":"ac84bd96-7ad2-4bbc-de04-f55fde85c414","colab":{"base_uri":"https://localhost:8080/","height":306}},"cell_type":"code","source":["model = Sequential()\n","for layer in base_model.layers[:-1]:\n","    model.add(layer)\n","    \n","for layer in model.layers:\n","  layer.trainable = False\n","  \n","\n","model.add(Dense(32, activation='relu'))\n","model.add(Dense(3, activation='softmax'))\n","\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_7 (Embedding)      (None, 100, 100)          1000000   \n","_________________________________________________________________\n","flatten_7 (Flatten)          (None, 10000)             0         \n","_________________________________________________________________\n","dense_15 (Dense)             (None, 32)                320032    \n","_________________________________________________________________\n","dense_17 (Dense)             (None, 32)                1056      \n","_________________________________________________________________\n","dense_18 (Dense)             (None, 3)                 99        \n","=================================================================\n","Total params: 1,321,187\n","Trainable params: 1,155\n","Non-trainable params: 1,320,032\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"X139UdSjwuj3","colab_type":"code","outputId":"ad94a171-ebff-407f-c32b-f66b50d26998","colab":{"base_uri":"https://localhost:8080/","height":6834}},"cell_type":"code","source":["optimizer = optimizers.Adam(lr=0.1)\n","loss = 'categorical_crossentropy'\n","epochs = 200\n","batch_size = 54\n","\n","model.compile(optimizer= optimizer,\n","              loss = loss,\n","              metrics=['acc'])\n","\n","history_9 = model.fit(x_train, y_train,\n","                    epochs=epochs,\n","                    batch_size=batch_size,\n","                    validation_data=(x_val, y_val))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 1000 samples, validate on 316 samples\n","Epoch 1/200\n","1000/1000 [==============================] - 1s 1ms/step - loss: 0.9698 - acc: 0.5020 - val_loss: 0.8941 - val_acc: 0.5190\n","Epoch 2/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.8765 - acc: 0.5460 - val_loss: 0.8345 - val_acc: 0.6139\n","Epoch 3/200\n","1000/1000 [==============================] - 0s 92us/step - loss: 0.8342 - acc: 0.5900 - val_loss: 0.8110 - val_acc: 0.5949\n","Epoch 4/200\n","1000/1000 [==============================] - 0s 93us/step - loss: 0.8235 - acc: 0.6130 - val_loss: 0.8038 - val_acc: 0.6203\n","Epoch 5/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.8271 - acc: 0.5990 - val_loss: 0.8235 - val_acc: 0.6108\n","Epoch 6/200\n","1000/1000 [==============================] - 0s 94us/step - loss: 0.8177 - acc: 0.6090 - val_loss: 0.8062 - val_acc: 0.5918\n","Epoch 7/200\n","1000/1000 [==============================] - 0s 91us/step - loss: 0.8250 - acc: 0.6030 - val_loss: 0.7980 - val_acc: 0.6076\n","Epoch 8/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.8100 - acc: 0.6170 - val_loss: 0.8236 - val_acc: 0.6013\n","Epoch 9/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.8037 - acc: 0.6270 - val_loss: 0.8070 - val_acc: 0.6044\n","Epoch 10/200\n","1000/1000 [==============================] - 0s 121us/step - loss: 0.8072 - acc: 0.6100 - val_loss: 0.8135 - val_acc: 0.5981\n","Epoch 11/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 0.8008 - acc: 0.6100 - val_loss: 0.8261 - val_acc: 0.6076\n","Epoch 12/200\n","1000/1000 [==============================] - 0s 122us/step - loss: 0.8097 - acc: 0.6170 - val_loss: 0.8209 - val_acc: 0.6013\n","Epoch 13/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.8130 - acc: 0.6150 - val_loss: 0.8099 - val_acc: 0.5981\n","Epoch 14/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.8087 - acc: 0.6110 - val_loss: 0.8036 - val_acc: 0.6013\n","Epoch 15/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.8187 - acc: 0.6110 - val_loss: 0.8040 - val_acc: 0.6076\n","Epoch 16/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.8023 - acc: 0.6300 - val_loss: 0.8015 - val_acc: 0.5949\n","Epoch 17/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.8029 - acc: 0.6310 - val_loss: 0.7958 - val_acc: 0.6013\n","Epoch 18/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 0.8150 - acc: 0.6140 - val_loss: 0.8503 - val_acc: 0.6044\n","Epoch 19/200\n","1000/1000 [==============================] - 0s 150us/step - loss: 0.8305 - acc: 0.6080 - val_loss: 0.8244 - val_acc: 0.5918\n","Epoch 20/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.8341 - acc: 0.6000 - val_loss: 0.8264 - val_acc: 0.6044\n","Epoch 21/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.8173 - acc: 0.6230 - val_loss: 0.8134 - val_acc: 0.5981\n","Epoch 22/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.8144 - acc: 0.6160 - val_loss: 0.8185 - val_acc: 0.6139\n","Epoch 23/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.8017 - acc: 0.6320 - val_loss: 0.7999 - val_acc: 0.6044\n","Epoch 24/200\n","1000/1000 [==============================] - 0s 117us/step - loss: 0.8035 - acc: 0.6290 - val_loss: 0.8066 - val_acc: 0.5918\n","Epoch 25/200\n","1000/1000 [==============================] - 0s 125us/step - loss: 0.7992 - acc: 0.6310 - val_loss: 0.8018 - val_acc: 0.6139\n","Epoch 26/200\n","1000/1000 [==============================] - 0s 117us/step - loss: 0.8076 - acc: 0.6300 - val_loss: 0.8012 - val_acc: 0.6013\n","Epoch 27/200\n","1000/1000 [==============================] - 0s 121us/step - loss: 0.8024 - acc: 0.6340 - val_loss: 0.8291 - val_acc: 0.5981\n","Epoch 28/200\n","1000/1000 [==============================] - 0s 127us/step - loss: 0.7960 - acc: 0.6370 - val_loss: 0.8587 - val_acc: 0.5918\n","Epoch 29/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7969 - acc: 0.6250 - val_loss: 0.8029 - val_acc: 0.5981\n","Epoch 30/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.7972 - acc: 0.6320 - val_loss: 0.8309 - val_acc: 0.5918\n","Epoch 31/200\n","1000/1000 [==============================] - 0s 124us/step - loss: 0.7965 - acc: 0.6410 - val_loss: 0.8044 - val_acc: 0.6139\n","Epoch 32/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 0.7932 - acc: 0.6440 - val_loss: 0.8081 - val_acc: 0.6076\n","Epoch 33/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 0.7946 - acc: 0.6440 - val_loss: 0.7987 - val_acc: 0.6076\n","Epoch 34/200\n","1000/1000 [==============================] - 0s 123us/step - loss: 0.8231 - acc: 0.6190 - val_loss: 0.7970 - val_acc: 0.6076\n","Epoch 35/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.8265 - acc: 0.6160 - val_loss: 0.7987 - val_acc: 0.6044\n","Epoch 36/200\n","1000/1000 [==============================] - 0s 127us/step - loss: 0.8214 - acc: 0.6100 - val_loss: 0.7924 - val_acc: 0.6108\n","Epoch 37/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.8051 - acc: 0.6320 - val_loss: 0.8122 - val_acc: 0.5981\n","Epoch 38/200\n","1000/1000 [==============================] - 0s 126us/step - loss: 0.7956 - acc: 0.6440 - val_loss: 0.8115 - val_acc: 0.5981\n","Epoch 39/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.8395 - acc: 0.6090 - val_loss: 0.8056 - val_acc: 0.6013\n","Epoch 40/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.8248 - acc: 0.6180 - val_loss: 0.8151 - val_acc: 0.5949\n","Epoch 41/200\n","1000/1000 [==============================] - 0s 117us/step - loss: 0.8015 - acc: 0.6290 - val_loss: 0.8316 - val_acc: 0.5918\n","Epoch 42/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 0.7991 - acc: 0.6390 - val_loss: 0.8016 - val_acc: 0.6044\n","Epoch 43/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 0.7967 - acc: 0.6410 - val_loss: 0.8029 - val_acc: 0.6108\n","Epoch 44/200\n","1000/1000 [==============================] - 0s 121us/step - loss: 0.7957 - acc: 0.6430 - val_loss: 0.8362 - val_acc: 0.5981\n","Epoch 45/200\n","1000/1000 [==============================] - 0s 136us/step - loss: 0.8005 - acc: 0.6340 - val_loss: 0.8008 - val_acc: 0.6108\n","Epoch 46/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7985 - acc: 0.6440 - val_loss: 0.8096 - val_acc: 0.6013\n","Epoch 47/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 0.8074 - acc: 0.6230 - val_loss: 0.8082 - val_acc: 0.6076\n","Epoch 48/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.8019 - acc: 0.6360 - val_loss: 0.8058 - val_acc: 0.6013\n","Epoch 49/200\n","1000/1000 [==============================] - 0s 122us/step - loss: 0.7994 - acc: 0.6450 - val_loss: 0.8429 - val_acc: 0.5918\n","Epoch 50/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.7976 - acc: 0.6400 - val_loss: 0.8172 - val_acc: 0.5981\n","Epoch 51/200\n","1000/1000 [==============================] - 0s 118us/step - loss: 0.7942 - acc: 0.6400 - val_loss: 0.8211 - val_acc: 0.5949\n","Epoch 52/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 0.7943 - acc: 0.6430 - val_loss: 0.8065 - val_acc: 0.6044\n","Epoch 53/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.7950 - acc: 0.6390 - val_loss: 0.8113 - val_acc: 0.5949\n","Epoch 54/200\n","1000/1000 [==============================] - 0s 125us/step - loss: 0.8080 - acc: 0.6320 - val_loss: 0.8211 - val_acc: 0.6013\n","Epoch 55/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.8144 - acc: 0.6290 - val_loss: 0.8098 - val_acc: 0.6013\n","Epoch 56/200\n","1000/1000 [==============================] - 0s 117us/step - loss: 0.7977 - acc: 0.6420 - val_loss: 0.8195 - val_acc: 0.5918\n","Epoch 57/200\n","1000/1000 [==============================] - 0s 124us/step - loss: 0.8015 - acc: 0.6350 - val_loss: 0.8097 - val_acc: 0.5981\n","Epoch 58/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 0.7993 - acc: 0.6330 - val_loss: 0.8057 - val_acc: 0.6044\n","Epoch 59/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.8088 - acc: 0.6270 - val_loss: 0.8045 - val_acc: 0.6108\n","Epoch 60/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7882 - acc: 0.6410 - val_loss: 0.8039 - val_acc: 0.6108\n","Epoch 61/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.7996 - acc: 0.6230 - val_loss: 0.8063 - val_acc: 0.6108\n","Epoch 62/200\n","1000/1000 [==============================] - 0s 118us/step - loss: 0.7994 - acc: 0.6300 - val_loss: 0.8079 - val_acc: 0.5981\n","Epoch 63/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 0.7976 - acc: 0.6320 - val_loss: 0.8127 - val_acc: 0.6044\n","Epoch 64/200\n","1000/1000 [==============================] - 0s 130us/step - loss: 0.7929 - acc: 0.6440 - val_loss: 0.8134 - val_acc: 0.5981\n","Epoch 65/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7893 - acc: 0.6430 - val_loss: 0.8163 - val_acc: 0.5981\n","Epoch 66/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7935 - acc: 0.6490 - val_loss: 0.8090 - val_acc: 0.6013\n","Epoch 67/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 0.7922 - acc: 0.6470 - val_loss: 0.8055 - val_acc: 0.6108\n","Epoch 68/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 0.8014 - acc: 0.6250 - val_loss: 0.8357 - val_acc: 0.5886\n","Epoch 69/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 0.7898 - acc: 0.6490 - val_loss: 0.8134 - val_acc: 0.6076\n","Epoch 70/200\n","1000/1000 [==============================] - 0s 121us/step - loss: 0.7907 - acc: 0.6420 - val_loss: 0.8045 - val_acc: 0.5981\n","Epoch 71/200\n","1000/1000 [==============================] - 0s 123us/step - loss: 0.7999 - acc: 0.6320 - val_loss: 0.8278 - val_acc: 0.6076\n","Epoch 72/200\n","1000/1000 [==============================] - 0s 122us/step - loss: 0.7912 - acc: 0.6470 - val_loss: 0.8335 - val_acc: 0.5886\n","Epoch 73/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.7908 - acc: 0.6410 - val_loss: 0.8042 - val_acc: 0.6044\n","Epoch 74/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7894 - acc: 0.6470 - val_loss: 0.8147 - val_acc: 0.6108\n","Epoch 75/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.7892 - acc: 0.6440 - val_loss: 0.8161 - val_acc: 0.6013\n","Epoch 76/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.7874 - acc: 0.6500 - val_loss: 0.8188 - val_acc: 0.6013\n","Epoch 77/200\n","1000/1000 [==============================] - 0s 125us/step - loss: 0.7890 - acc: 0.6440 - val_loss: 0.8096 - val_acc: 0.6076\n","Epoch 78/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 0.7915 - acc: 0.6440 - val_loss: 0.8107 - val_acc: 0.5981\n","Epoch 79/200\n","1000/1000 [==============================] - 0s 122us/step - loss: 0.7937 - acc: 0.6430 - val_loss: 0.8215 - val_acc: 0.6013\n","Epoch 80/200\n","1000/1000 [==============================] - 0s 136us/step - loss: 0.7901 - acc: 0.6420 - val_loss: 0.8346 - val_acc: 0.5949\n","Epoch 81/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.8085 - acc: 0.6370 - val_loss: 0.8313 - val_acc: 0.6044\n","Epoch 82/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 0.7932 - acc: 0.6420 - val_loss: 0.8450 - val_acc: 0.5854\n","Epoch 83/200\n","1000/1000 [==============================] - 0s 133us/step - loss: 0.8062 - acc: 0.6340 - val_loss: 0.8078 - val_acc: 0.5918\n","Epoch 84/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.8223 - acc: 0.6300 - val_loss: 0.8522 - val_acc: 0.6044\n","Epoch 85/200\n","1000/1000 [==============================] - 0s 120us/step - loss: 0.8138 - acc: 0.6260 - val_loss: 0.7997 - val_acc: 0.6044\n","Epoch 86/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.7890 - acc: 0.6440 - val_loss: 0.8661 - val_acc: 0.5949\n","Epoch 87/200\n","1000/1000 [==============================] - 0s 136us/step - loss: 0.8068 - acc: 0.6300 - val_loss: 0.8047 - val_acc: 0.6044\n","Epoch 88/200\n","1000/1000 [==============================] - 0s 141us/step - loss: 0.7965 - acc: 0.6290 - val_loss: 0.8194 - val_acc: 0.6044\n","Epoch 89/200\n","1000/1000 [==============================] - 0s 128us/step - loss: 0.7956 - acc: 0.6440 - val_loss: 0.8006 - val_acc: 0.6108\n","Epoch 90/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7935 - acc: 0.6410 - val_loss: 0.8076 - val_acc: 0.6044\n","Epoch 91/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7866 - acc: 0.6460 - val_loss: 0.8127 - val_acc: 0.6076\n","Epoch 92/200\n","1000/1000 [==============================] - 0s 118us/step - loss: 0.7887 - acc: 0.6370 - val_loss: 0.8067 - val_acc: 0.5949\n","Epoch 93/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7922 - acc: 0.6380 - val_loss: 0.8313 - val_acc: 0.5886\n","Epoch 94/200\n","1000/1000 [==============================] - 0s 119us/step - loss: 0.8148 - acc: 0.6240 - val_loss: 0.8158 - val_acc: 0.5918\n","Epoch 95/200\n","1000/1000 [==============================] - 0s 126us/step - loss: 0.8043 - acc: 0.6180 - val_loss: 0.8360 - val_acc: 0.5981\n","Epoch 96/200\n","1000/1000 [==============================] - 0s 121us/step - loss: 0.8001 - acc: 0.6320 - val_loss: 0.8148 - val_acc: 0.5949\n","Epoch 97/200\n","1000/1000 [==============================] - 0s 131us/step - loss: 0.7968 - acc: 0.6400 - val_loss: 0.8074 - val_acc: 0.6171\n","Epoch 98/200\n","1000/1000 [==============================] - 0s 121us/step - loss: 0.7990 - acc: 0.6380 - val_loss: 0.8131 - val_acc: 0.5981\n","Epoch 99/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7940 - acc: 0.6490 - val_loss: 0.8416 - val_acc: 0.5854\n","Epoch 100/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.8006 - acc: 0.6390 - val_loss: 0.7998 - val_acc: 0.6108\n","Epoch 101/200\n","1000/1000 [==============================] - 0s 121us/step - loss: 0.7865 - acc: 0.6460 - val_loss: 0.8174 - val_acc: 0.6139\n","Epoch 102/200\n","1000/1000 [==============================] - 0s 121us/step - loss: 0.7894 - acc: 0.6470 - val_loss: 0.8238 - val_acc: 0.5981\n","Epoch 103/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.8036 - acc: 0.6340 - val_loss: 0.8231 - val_acc: 0.5981\n","Epoch 104/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 0.7977 - acc: 0.6360 - val_loss: 0.7993 - val_acc: 0.6076\n","Epoch 105/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7914 - acc: 0.6420 - val_loss: 0.8212 - val_acc: 0.5949\n","Epoch 106/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7855 - acc: 0.6500 - val_loss: 0.8186 - val_acc: 0.6076\n","Epoch 107/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 0.7869 - acc: 0.6430 - val_loss: 0.8064 - val_acc: 0.6203\n","Epoch 108/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.8003 - acc: 0.6410 - val_loss: 0.8093 - val_acc: 0.5981\n","Epoch 109/200\n","1000/1000 [==============================] - 0s 128us/step - loss: 0.8117 - acc: 0.6270 - val_loss: 0.8497 - val_acc: 0.5791\n","Epoch 110/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.7962 - acc: 0.6390 - val_loss: 0.8185 - val_acc: 0.5981\n","Epoch 111/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.7977 - acc: 0.6450 - val_loss: 0.8037 - val_acc: 0.5949\n","Epoch 112/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.7887 - acc: 0.6390 - val_loss: 0.8078 - val_acc: 0.5949\n","Epoch 113/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 0.7965 - acc: 0.6340 - val_loss: 0.8284 - val_acc: 0.5949\n","Epoch 114/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 0.8115 - acc: 0.6280 - val_loss: 0.8121 - val_acc: 0.6044\n","Epoch 115/200\n","1000/1000 [==============================] - 0s 134us/step - loss: 0.8169 - acc: 0.6320 - val_loss: 0.8097 - val_acc: 0.5981\n","Epoch 116/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.8009 - acc: 0.6430 - val_loss: 0.8194 - val_acc: 0.6076\n","Epoch 117/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.7907 - acc: 0.6430 - val_loss: 0.8147 - val_acc: 0.6044\n","Epoch 118/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 0.7837 - acc: 0.6450 - val_loss: 0.8084 - val_acc: 0.5949\n","Epoch 119/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7943 - acc: 0.6490 - val_loss: 0.8154 - val_acc: 0.6013\n","Epoch 120/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7889 - acc: 0.6450 - val_loss: 0.8084 - val_acc: 0.6076\n","Epoch 121/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7896 - acc: 0.6460 - val_loss: 0.8033 - val_acc: 0.6203\n","Epoch 122/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 0.7911 - acc: 0.6440 - val_loss: 0.8068 - val_acc: 0.6013\n","Epoch 123/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.7994 - acc: 0.6380 - val_loss: 0.8201 - val_acc: 0.6013\n","Epoch 124/200\n","1000/1000 [==============================] - 0s 122us/step - loss: 0.7999 - acc: 0.6310 - val_loss: 0.8058 - val_acc: 0.6139\n","Epoch 125/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 0.8176 - acc: 0.6300 - val_loss: 0.8620 - val_acc: 0.5886\n","Epoch 126/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 0.8135 - acc: 0.6370 - val_loss: 0.8026 - val_acc: 0.6171\n","Epoch 127/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 0.7924 - acc: 0.6470 - val_loss: 0.8156 - val_acc: 0.6076\n","Epoch 128/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.7864 - acc: 0.6480 - val_loss: 0.8226 - val_acc: 0.6013\n","Epoch 129/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.8088 - acc: 0.6240 - val_loss: 0.8064 - val_acc: 0.6076\n","Epoch 130/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 0.8081 - acc: 0.6320 - val_loss: 0.8068 - val_acc: 0.6044\n","Epoch 131/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.7849 - acc: 0.6490 - val_loss: 0.8064 - val_acc: 0.6044\n","Epoch 132/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7881 - acc: 0.6430 - val_loss: 0.8062 - val_acc: 0.6139\n","Epoch 133/200\n","1000/1000 [==============================] - 0s 134us/step - loss: 0.7910 - acc: 0.6420 - val_loss: 0.8084 - val_acc: 0.5981\n","Epoch 134/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.8013 - acc: 0.6460 - val_loss: 0.8052 - val_acc: 0.6076\n","Epoch 135/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.8123 - acc: 0.6230 - val_loss: 0.8166 - val_acc: 0.6013\n","Epoch 136/200\n","1000/1000 [==============================] - 0s 122us/step - loss: 0.7941 - acc: 0.6460 - val_loss: 0.8268 - val_acc: 0.6013\n","Epoch 137/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7874 - acc: 0.6460 - val_loss: 0.8135 - val_acc: 0.6076\n","Epoch 138/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7885 - acc: 0.6440 - val_loss: 0.8016 - val_acc: 0.6139\n","Epoch 139/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.7897 - acc: 0.6460 - val_loss: 0.8331 - val_acc: 0.5918\n","Epoch 140/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.8044 - acc: 0.6290 - val_loss: 0.7985 - val_acc: 0.6076\n","Epoch 141/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 0.7918 - acc: 0.6410 - val_loss: 0.8415 - val_acc: 0.5854\n","Epoch 142/200\n","1000/1000 [==============================] - 0s 126us/step - loss: 0.7968 - acc: 0.6440 - val_loss: 0.8193 - val_acc: 0.5886\n","Epoch 143/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 0.7958 - acc: 0.6440 - val_loss: 0.8050 - val_acc: 0.6108\n","Epoch 144/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.8084 - acc: 0.6360 - val_loss: 0.8047 - val_acc: 0.6108\n","Epoch 145/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7898 - acc: 0.6530 - val_loss: 0.8517 - val_acc: 0.5949\n","Epoch 146/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.8204 - acc: 0.6170 - val_loss: 0.7994 - val_acc: 0.6108\n","Epoch 147/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7899 - acc: 0.6380 - val_loss: 0.8124 - val_acc: 0.6139\n","Epoch 148/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7904 - acc: 0.6450 - val_loss: 0.8056 - val_acc: 0.6203\n","Epoch 149/200\n","1000/1000 [==============================] - 0s 140us/step - loss: 0.7851 - acc: 0.6450 - val_loss: 0.8195 - val_acc: 0.6044\n","Epoch 150/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.7910 - acc: 0.6500 - val_loss: 0.8059 - val_acc: 0.6076\n","Epoch 151/200\n","1000/1000 [==============================] - 0s 119us/step - loss: 0.7895 - acc: 0.6480 - val_loss: 0.8157 - val_acc: 0.6108\n","Epoch 152/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7973 - acc: 0.6370 - val_loss: 0.8066 - val_acc: 0.5949\n","Epoch 153/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 0.7971 - acc: 0.6440 - val_loss: 0.8175 - val_acc: 0.6013\n","Epoch 154/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7942 - acc: 0.6410 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 155/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.7900 - acc: 0.6430 - val_loss: 0.8066 - val_acc: 0.6203\n","Epoch 156/200\n","1000/1000 [==============================] - 0s 129us/step - loss: 0.7971 - acc: 0.6440 - val_loss: 0.8083 - val_acc: 0.6108\n","Epoch 157/200\n","1000/1000 [==============================] - 0s 120us/step - loss: 0.7964 - acc: 0.6360 - val_loss: 0.8324 - val_acc: 0.5854\n","Epoch 158/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.8053 - acc: 0.6290 - val_loss: 0.8066 - val_acc: 0.6203\n","Epoch 159/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.8110 - acc: 0.6390 - val_loss: 0.8062 - val_acc: 0.6171\n","Epoch 160/200\n","1000/1000 [==============================] - 0s 121us/step - loss: 0.7837 - acc: 0.6490 - val_loss: 0.8055 - val_acc: 0.6108\n","Epoch 161/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7904 - acc: 0.6500 - val_loss: 0.8172 - val_acc: 0.5949\n","Epoch 162/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7989 - acc: 0.6410 - val_loss: 0.8072 - val_acc: 0.5949\n","Epoch 163/200\n","1000/1000 [==============================] - 0s 123us/step - loss: 0.7892 - acc: 0.6540 - val_loss: 0.8107 - val_acc: 0.5886\n","Epoch 164/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7922 - acc: 0.6570 - val_loss: 0.8176 - val_acc: 0.5981\n","Epoch 165/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7868 - acc: 0.6450 - val_loss: 0.8056 - val_acc: 0.6076\n","Epoch 166/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7890 - acc: 0.6490 - val_loss: 0.8146 - val_acc: 0.6108\n","Epoch 167/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.7949 - acc: 0.6490 - val_loss: 0.8216 - val_acc: 0.5981\n","Epoch 168/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.8053 - acc: 0.6380 - val_loss: 0.8112 - val_acc: 0.5949\n","Epoch 169/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7993 - acc: 0.6430 - val_loss: 0.8154 - val_acc: 0.6044\n","Epoch 170/200\n","1000/1000 [==============================] - 0s 132us/step - loss: 0.7868 - acc: 0.6520 - val_loss: 0.8182 - val_acc: 0.6171\n","Epoch 171/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.7959 - acc: 0.6490 - val_loss: 0.8528 - val_acc: 0.5981\n","Epoch 172/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.8036 - acc: 0.6430 - val_loss: 0.8164 - val_acc: 0.6076\n","Epoch 173/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.7932 - acc: 0.6520 - val_loss: 0.8314 - val_acc: 0.5918\n","Epoch 174/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.8048 - acc: 0.6370 - val_loss: 0.8015 - val_acc: 0.6044\n","Epoch 175/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7912 - acc: 0.6390 - val_loss: 0.8100 - val_acc: 0.6203\n","Epoch 176/200\n","1000/1000 [==============================] - 0s 118us/step - loss: 0.7910 - acc: 0.6480 - val_loss: 0.8023 - val_acc: 0.6108\n","Epoch 177/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.7849 - acc: 0.6510 - val_loss: 0.8444 - val_acc: 0.5918\n","Epoch 178/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7904 - acc: 0.6450 - val_loss: 0.8074 - val_acc: 0.6044\n","Epoch 179/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 0.7930 - acc: 0.6430 - val_loss: 0.8075 - val_acc: 0.6108\n","Epoch 180/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.7923 - acc: 0.6450 - val_loss: 0.8123 - val_acc: 0.6044\n","Epoch 181/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.7879 - acc: 0.6470 - val_loss: 0.8634 - val_acc: 0.5886\n","Epoch 182/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.8102 - acc: 0.6260 - val_loss: 0.8023 - val_acc: 0.6013\n","Epoch 183/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7921 - acc: 0.6390 - val_loss: 0.8242 - val_acc: 0.6044\n","Epoch 184/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7896 - acc: 0.6430 - val_loss: 0.8161 - val_acc: 0.5949\n","Epoch 185/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.8218 - acc: 0.6270 - val_loss: 0.8134 - val_acc: 0.5949\n","Epoch 186/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 0.7959 - acc: 0.6420 - val_loss: 0.8297 - val_acc: 0.6013\n","Epoch 187/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.7834 - acc: 0.6480 - val_loss: 0.8104 - val_acc: 0.6108\n","Epoch 188/200\n","1000/1000 [==============================] - 0s 143us/step - loss: 0.7932 - acc: 0.6490 - val_loss: 0.8140 - val_acc: 0.5981\n","Epoch 189/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7907 - acc: 0.6460 - val_loss: 0.8313 - val_acc: 0.5981\n","Epoch 190/200\n","1000/1000 [==============================] - 0s 121us/step - loss: 0.7846 - acc: 0.6450 - val_loss: 0.8182 - val_acc: 0.6013\n","Epoch 191/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7889 - acc: 0.6520 - val_loss: 0.8056 - val_acc: 0.5981\n","Epoch 192/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7911 - acc: 0.6480 - val_loss: 0.8175 - val_acc: 0.5918\n","Epoch 193/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7925 - acc: 0.6490 - val_loss: 0.8428 - val_acc: 0.5981\n","Epoch 194/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.8000 - acc: 0.6360 - val_loss: 0.8074 - val_acc: 0.6108\n","Epoch 195/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7969 - acc: 0.6380 - val_loss: 0.8076 - val_acc: 0.6044\n","Epoch 196/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.7887 - acc: 0.6450 - val_loss: 0.8048 - val_acc: 0.6139\n","Epoch 197/200\n","1000/1000 [==============================] - 0s 142us/step - loss: 0.7838 - acc: 0.6520 - val_loss: 0.8053 - val_acc: 0.6013\n","Epoch 198/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.8089 - acc: 0.6370 - val_loss: 0.8085 - val_acc: 0.6108\n","Epoch 199/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.7844 - acc: 0.6430 - val_loss: 0.8085 - val_acc: 0.6108\n","Epoch 200/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 0.7841 - acc: 0.6450 - val_loss: 0.8289 - val_acc: 0.6013\n"],"name":"stdout"}]},{"metadata":{"id":"4kQ-e7cuzj3d","colab_type":"code","outputId":"d66a0110-df6f-4420-ed6c-ba4ad4cdee3e","colab":{"base_uri":"https://localhost:8080/","height":6834}},"cell_type":"code","source":["optimizer = optimizers.Adam(lr=0.01)\n","loss = 'categorical_crossentropy'\n","epochs = 200\n","batch_size = 54\n","\n","model.compile(optimizer= optimizer,\n","              loss = loss,\n","              metrics=['acc'])\n","\n","history_9 = model.fit(x_train, y_train,\n","                    epochs=epochs,\n","                    batch_size=batch_size,\n","                    validation_data=(x_val, y_val))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 1000 samples, validate on 316 samples\n","Epoch 1/200\n","1000/1000 [==============================] - 1s 1ms/step - loss: 0.7808 - acc: 0.6460 - val_loss: 0.8105 - val_acc: 0.6044\n","Epoch 2/200\n","1000/1000 [==============================] - 0s 143us/step - loss: 0.7788 - acc: 0.6540 - val_loss: 0.8131 - val_acc: 0.6076\n","Epoch 3/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7791 - acc: 0.6520 - val_loss: 0.8155 - val_acc: 0.6013\n","Epoch 4/200\n","1000/1000 [==============================] - 0s 132us/step - loss: 0.7785 - acc: 0.6550 - val_loss: 0.8104 - val_acc: 0.6108\n","Epoch 5/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.7785 - acc: 0.6510 - val_loss: 0.8146 - val_acc: 0.6044\n","Epoch 6/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.7793 - acc: 0.6530 - val_loss: 0.8099 - val_acc: 0.6139\n","Epoch 7/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7805 - acc: 0.6500 - val_loss: 0.8134 - val_acc: 0.6044\n","Epoch 8/200\n","1000/1000 [==============================] - 0s 120us/step - loss: 0.7782 - acc: 0.6510 - val_loss: 0.8134 - val_acc: 0.6044\n","Epoch 9/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 0.7796 - acc: 0.6550 - val_loss: 0.8119 - val_acc: 0.6076\n","Epoch 10/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.7789 - acc: 0.6500 - val_loss: 0.8112 - val_acc: 0.6108\n","Epoch 11/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7788 - acc: 0.6510 - val_loss: 0.8142 - val_acc: 0.6044\n","Epoch 12/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7785 - acc: 0.6530 - val_loss: 0.8138 - val_acc: 0.6044\n","Epoch 13/200\n","1000/1000 [==============================] - 0s 119us/step - loss: 0.7781 - acc: 0.6570 - val_loss: 0.8131 - val_acc: 0.6044\n","Epoch 14/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7788 - acc: 0.6530 - val_loss: 0.8141 - val_acc: 0.6076\n","Epoch 15/200\n","1000/1000 [==============================] - 0s 124us/step - loss: 0.7796 - acc: 0.6540 - val_loss: 0.8115 - val_acc: 0.6108\n","Epoch 16/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 0.7787 - acc: 0.6510 - val_loss: 0.8175 - val_acc: 0.5918\n","Epoch 17/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.7790 - acc: 0.6480 - val_loss: 0.8137 - val_acc: 0.6044\n","Epoch 18/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.7810 - acc: 0.6580 - val_loss: 0.8097 - val_acc: 0.6076\n","Epoch 19/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.7810 - acc: 0.6440 - val_loss: 0.8193 - val_acc: 0.6013\n","Epoch 20/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.7786 - acc: 0.6520 - val_loss: 0.8105 - val_acc: 0.6108\n","Epoch 21/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 0.7795 - acc: 0.6510 - val_loss: 0.8123 - val_acc: 0.6108\n","Epoch 22/200\n","1000/1000 [==============================] - 0s 140us/step - loss: 0.7801 - acc: 0.6490 - val_loss: 0.8181 - val_acc: 0.5918\n","Epoch 23/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.7781 - acc: 0.6530 - val_loss: 0.8112 - val_acc: 0.6108\n","Epoch 24/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.7783 - acc: 0.6490 - val_loss: 0.8212 - val_acc: 0.6044\n","Epoch 25/200\n","1000/1000 [==============================] - 0s 122us/step - loss: 0.7804 - acc: 0.6530 - val_loss: 0.8128 - val_acc: 0.6044\n","Epoch 26/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7788 - acc: 0.6570 - val_loss: 0.8119 - val_acc: 0.6108\n","Epoch 27/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7788 - acc: 0.6490 - val_loss: 0.8188 - val_acc: 0.5981\n","Epoch 28/200\n","1000/1000 [==============================] - 0s 117us/step - loss: 0.7791 - acc: 0.6510 - val_loss: 0.8135 - val_acc: 0.6044\n","Epoch 29/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.7791 - acc: 0.6540 - val_loss: 0.8124 - val_acc: 0.6108\n","Epoch 30/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 0.7805 - acc: 0.6470 - val_loss: 0.8140 - val_acc: 0.6076\n","Epoch 31/200\n","1000/1000 [==============================] - 0s 119us/step - loss: 0.7789 - acc: 0.6480 - val_loss: 0.8166 - val_acc: 0.5981\n","Epoch 32/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7803 - acc: 0.6500 - val_loss: 0.8106 - val_acc: 0.6108\n","Epoch 33/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7791 - acc: 0.6480 - val_loss: 0.8159 - val_acc: 0.5981\n","Epoch 34/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 0.7800 - acc: 0.6550 - val_loss: 0.8143 - val_acc: 0.6013\n","Epoch 35/200\n","1000/1000 [==============================] - 0s 122us/step - loss: 0.7788 - acc: 0.6550 - val_loss: 0.8145 - val_acc: 0.6013\n","Epoch 36/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7791 - acc: 0.6520 - val_loss: 0.8130 - val_acc: 0.6108\n","Epoch 37/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7790 - acc: 0.6500 - val_loss: 0.8163 - val_acc: 0.6013\n","Epoch 38/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7799 - acc: 0.6500 - val_loss: 0.8124 - val_acc: 0.6108\n","Epoch 39/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 0.7790 - acc: 0.6560 - val_loss: 0.8116 - val_acc: 0.6108\n","Epoch 40/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 0.7789 - acc: 0.6550 - val_loss: 0.8154 - val_acc: 0.6013\n","Epoch 41/200\n","1000/1000 [==============================] - 0s 124us/step - loss: 0.7792 - acc: 0.6500 - val_loss: 0.8156 - val_acc: 0.6013\n","Epoch 42/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7785 - acc: 0.6530 - val_loss: 0.8160 - val_acc: 0.6013\n","Epoch 43/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 0.7783 - acc: 0.6550 - val_loss: 0.8138 - val_acc: 0.6108\n","Epoch 44/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 0.7787 - acc: 0.6560 - val_loss: 0.8137 - val_acc: 0.6108\n","Epoch 45/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.7782 - acc: 0.6480 - val_loss: 0.8142 - val_acc: 0.6076\n","Epoch 46/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.7792 - acc: 0.6550 - val_loss: 0.8141 - val_acc: 0.6044\n","Epoch 47/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 0.7809 - acc: 0.6500 - val_loss: 0.8154 - val_acc: 0.5981\n","Epoch 48/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7782 - acc: 0.6540 - val_loss: 0.8156 - val_acc: 0.6013\n","Epoch 49/200\n","1000/1000 [==============================] - 0s 146us/step - loss: 0.7790 - acc: 0.6530 - val_loss: 0.8127 - val_acc: 0.6108\n","Epoch 50/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7794 - acc: 0.6560 - val_loss: 0.8118 - val_acc: 0.6108\n","Epoch 51/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7798 - acc: 0.6490 - val_loss: 0.8191 - val_acc: 0.5981\n","Epoch 52/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 0.7797 - acc: 0.6520 - val_loss: 0.8158 - val_acc: 0.5981\n","Epoch 53/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7785 - acc: 0.6540 - val_loss: 0.8111 - val_acc: 0.6076\n","Epoch 54/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7793 - acc: 0.6500 - val_loss: 0.8176 - val_acc: 0.5981\n","Epoch 55/200\n","1000/1000 [==============================] - 0s 122us/step - loss: 0.7787 - acc: 0.6540 - val_loss: 0.8138 - val_acc: 0.6108\n","Epoch 56/200\n","1000/1000 [==============================] - 0s 121us/step - loss: 0.7796 - acc: 0.6510 - val_loss: 0.8174 - val_acc: 0.5949\n","Epoch 57/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 0.7798 - acc: 0.6460 - val_loss: 0.8155 - val_acc: 0.6013\n","Epoch 58/200\n","1000/1000 [==============================] - 0s 124us/step - loss: 0.7793 - acc: 0.6540 - val_loss: 0.8121 - val_acc: 0.6108\n","Epoch 59/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7791 - acc: 0.6480 - val_loss: 0.8148 - val_acc: 0.6076\n","Epoch 60/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.7781 - acc: 0.6570 - val_loss: 0.8198 - val_acc: 0.5981\n","Epoch 61/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.7800 - acc: 0.6450 - val_loss: 0.8164 - val_acc: 0.5981\n","Epoch 62/200\n","1000/1000 [==============================] - 0s 121us/step - loss: 0.7784 - acc: 0.6520 - val_loss: 0.8164 - val_acc: 0.6044\n","Epoch 63/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7805 - acc: 0.6490 - val_loss: 0.8111 - val_acc: 0.6076\n","Epoch 64/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.7784 - acc: 0.6500 - val_loss: 0.8138 - val_acc: 0.6076\n","Epoch 65/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7778 - acc: 0.6520 - val_loss: 0.8156 - val_acc: 0.6044\n","Epoch 66/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 0.7780 - acc: 0.6520 - val_loss: 0.8150 - val_acc: 0.6108\n","Epoch 67/200\n","1000/1000 [==============================] - 0s 125us/step - loss: 0.7784 - acc: 0.6520 - val_loss: 0.8132 - val_acc: 0.6076\n","Epoch 68/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.7781 - acc: 0.6500 - val_loss: 0.8171 - val_acc: 0.5981\n","Epoch 69/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 0.7781 - acc: 0.6530 - val_loss: 0.8140 - val_acc: 0.6076\n","Epoch 70/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7802 - acc: 0.6410 - val_loss: 0.8154 - val_acc: 0.6013\n","Epoch 71/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.7792 - acc: 0.6550 - val_loss: 0.8139 - val_acc: 0.6108\n","Epoch 72/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7786 - acc: 0.6480 - val_loss: 0.8177 - val_acc: 0.6013\n","Epoch 73/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.7786 - acc: 0.6520 - val_loss: 0.8162 - val_acc: 0.6013\n","Epoch 74/200\n","1000/1000 [==============================] - 0s 117us/step - loss: 0.7784 - acc: 0.6550 - val_loss: 0.8116 - val_acc: 0.6108\n","Epoch 75/200\n","1000/1000 [==============================] - 0s 121us/step - loss: 0.7779 - acc: 0.6500 - val_loss: 0.8157 - val_acc: 0.6076\n","Epoch 76/200\n","1000/1000 [==============================] - 0s 134us/step - loss: 0.7781 - acc: 0.6500 - val_loss: 0.8154 - val_acc: 0.6044\n","Epoch 77/200\n","1000/1000 [==============================] - 0s 118us/step - loss: 0.7789 - acc: 0.6520 - val_loss: 0.8190 - val_acc: 0.5981\n","Epoch 78/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 0.7786 - acc: 0.6530 - val_loss: 0.8149 - val_acc: 0.6076\n","Epoch 79/200\n","1000/1000 [==============================] - 0s 120us/step - loss: 0.7780 - acc: 0.6550 - val_loss: 0.8156 - val_acc: 0.6076\n","Epoch 80/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7782 - acc: 0.6520 - val_loss: 0.8137 - val_acc: 0.6108\n","Epoch 81/200\n","1000/1000 [==============================] - 0s 122us/step - loss: 0.7778 - acc: 0.6530 - val_loss: 0.8144 - val_acc: 0.6108\n","Epoch 82/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7779 - acc: 0.6550 - val_loss: 0.8149 - val_acc: 0.6108\n","Epoch 83/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7788 - acc: 0.6490 - val_loss: 0.8156 - val_acc: 0.6013\n","Epoch 84/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.7797 - acc: 0.6470 - val_loss: 0.8166 - val_acc: 0.6013\n","Epoch 85/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.7824 - acc: 0.6540 - val_loss: 0.8122 - val_acc: 0.6044\n","Epoch 86/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7796 - acc: 0.6480 - val_loss: 0.8128 - val_acc: 0.6013\n","Epoch 87/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.7793 - acc: 0.6500 - val_loss: 0.8144 - val_acc: 0.6108\n","Epoch 88/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.7806 - acc: 0.6490 - val_loss: 0.8177 - val_acc: 0.5981\n","Epoch 89/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.7782 - acc: 0.6530 - val_loss: 0.8155 - val_acc: 0.6076\n","Epoch 90/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.7782 - acc: 0.6560 - val_loss: 0.8148 - val_acc: 0.6108\n","Epoch 91/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7776 - acc: 0.6490 - val_loss: 0.8150 - val_acc: 0.6044\n","Epoch 92/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7773 - acc: 0.6530 - val_loss: 0.8138 - val_acc: 0.6076\n","Epoch 93/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7790 - acc: 0.6530 - val_loss: 0.8126 - val_acc: 0.6044\n","Epoch 94/200\n","1000/1000 [==============================] - 0s 128us/step - loss: 0.7789 - acc: 0.6520 - val_loss: 0.8167 - val_acc: 0.6013\n","Epoch 95/200\n","1000/1000 [==============================] - 0s 129us/step - loss: 0.7777 - acc: 0.6560 - val_loss: 0.8149 - val_acc: 0.6076\n","Epoch 96/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 0.7776 - acc: 0.6540 - val_loss: 0.8145 - val_acc: 0.6076\n","Epoch 97/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7789 - acc: 0.6510 - val_loss: 0.8140 - val_acc: 0.6013\n","Epoch 98/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7774 - acc: 0.6520 - val_loss: 0.8162 - val_acc: 0.6076\n","Epoch 99/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7777 - acc: 0.6540 - val_loss: 0.8154 - val_acc: 0.6076\n","Epoch 100/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.7776 - acc: 0.6490 - val_loss: 0.8173 - val_acc: 0.6013\n","Epoch 101/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7779 - acc: 0.6540 - val_loss: 0.8146 - val_acc: 0.6076\n","Epoch 102/200\n","1000/1000 [==============================] - 0s 126us/step - loss: 0.7787 - acc: 0.6510 - val_loss: 0.8144 - val_acc: 0.6044\n","Epoch 103/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.7774 - acc: 0.6520 - val_loss: 0.8196 - val_acc: 0.5981\n","Epoch 104/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7781 - acc: 0.6530 - val_loss: 0.8145 - val_acc: 0.6044\n","Epoch 105/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.7782 - acc: 0.6540 - val_loss: 0.8199 - val_acc: 0.5981\n","Epoch 106/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7792 - acc: 0.6520 - val_loss: 0.8192 - val_acc: 0.6013\n","Epoch 107/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7780 - acc: 0.6550 - val_loss: 0.8147 - val_acc: 0.6044\n","Epoch 108/200\n","1000/1000 [==============================] - 0s 123us/step - loss: 0.7796 - acc: 0.6470 - val_loss: 0.8206 - val_acc: 0.6013\n","Epoch 109/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.7797 - acc: 0.6520 - val_loss: 0.8142 - val_acc: 0.6044\n","Epoch 110/200\n","1000/1000 [==============================] - 0s 120us/step - loss: 0.7805 - acc: 0.6530 - val_loss: 0.8141 - val_acc: 0.6076\n","Epoch 111/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.7793 - acc: 0.6510 - val_loss: 0.8149 - val_acc: 0.6076\n","Epoch 112/200\n","1000/1000 [==============================] - 0s 122us/step - loss: 0.7799 - acc: 0.6570 - val_loss: 0.8156 - val_acc: 0.6076\n","Epoch 113/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.7777 - acc: 0.6510 - val_loss: 0.8151 - val_acc: 0.6044\n","Epoch 114/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.7785 - acc: 0.6530 - val_loss: 0.8166 - val_acc: 0.6076\n","Epoch 115/200\n","1000/1000 [==============================] - 0s 124us/step - loss: 0.7795 - acc: 0.6510 - val_loss: 0.8112 - val_acc: 0.6044\n","Epoch 116/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.7785 - acc: 0.6480 - val_loss: 0.8178 - val_acc: 0.6076\n","Epoch 117/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7786 - acc: 0.6510 - val_loss: 0.8152 - val_acc: 0.6044\n","Epoch 118/200\n","1000/1000 [==============================] - 0s 117us/step - loss: 0.7784 - acc: 0.6500 - val_loss: 0.8184 - val_acc: 0.6044\n","Epoch 119/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 0.7778 - acc: 0.6560 - val_loss: 0.8151 - val_acc: 0.6044\n","Epoch 120/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7775 - acc: 0.6480 - val_loss: 0.8195 - val_acc: 0.5981\n","Epoch 121/200\n","1000/1000 [==============================] - 0s 142us/step - loss: 0.7787 - acc: 0.6540 - val_loss: 0.8139 - val_acc: 0.6044\n","Epoch 122/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 0.7806 - acc: 0.6480 - val_loss: 0.8162 - val_acc: 0.6044\n","Epoch 123/200\n","1000/1000 [==============================] - 0s 126us/step - loss: 0.7783 - acc: 0.6540 - val_loss: 0.8160 - val_acc: 0.6076\n","Epoch 124/200\n","1000/1000 [==============================] - 0s 142us/step - loss: 0.7775 - acc: 0.6540 - val_loss: 0.8182 - val_acc: 0.6076\n","Epoch 125/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.7788 - acc: 0.6480 - val_loss: 0.8140 - val_acc: 0.6013\n","Epoch 126/200\n","1000/1000 [==============================] - 0s 92us/step - loss: 0.7782 - acc: 0.6510 - val_loss: 0.8169 - val_acc: 0.6044\n","Epoch 127/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 0.7782 - acc: 0.6490 - val_loss: 0.8175 - val_acc: 0.6076\n","Epoch 128/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7785 - acc: 0.6500 - val_loss: 0.8150 - val_acc: 0.6044\n","Epoch 129/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.7774 - acc: 0.6540 - val_loss: 0.8196 - val_acc: 0.6013\n","Epoch 130/200\n","1000/1000 [==============================] - 0s 178us/step - loss: 0.7794 - acc: 0.6540 - val_loss: 0.8140 - val_acc: 0.6013\n","Epoch 131/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7792 - acc: 0.6520 - val_loss: 0.8210 - val_acc: 0.6013\n","Epoch 132/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7823 - acc: 0.6550 - val_loss: 0.8118 - val_acc: 0.6044\n","Epoch 133/200\n","1000/1000 [==============================] - 0s 119us/step - loss: 0.7786 - acc: 0.6500 - val_loss: 0.8152 - val_acc: 0.6044\n","Epoch 134/200\n","1000/1000 [==============================] - 0s 124us/step - loss: 0.7779 - acc: 0.6530 - val_loss: 0.8159 - val_acc: 0.6044\n","Epoch 135/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 0.7773 - acc: 0.6540 - val_loss: 0.8178 - val_acc: 0.6044\n","Epoch 136/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 0.7779 - acc: 0.6520 - val_loss: 0.8165 - val_acc: 0.6044\n","Epoch 137/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.7783 - acc: 0.6500 - val_loss: 0.8154 - val_acc: 0.6013\n","Epoch 138/200\n","1000/1000 [==============================] - 0s 117us/step - loss: 0.7803 - acc: 0.6510 - val_loss: 0.8141 - val_acc: 0.6013\n","Epoch 139/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7778 - acc: 0.6520 - val_loss: 0.8185 - val_acc: 0.6044\n","Epoch 140/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 0.7768 - acc: 0.6550 - val_loss: 0.8153 - val_acc: 0.5981\n","Epoch 141/200\n","1000/1000 [==============================] - 0s 139us/step - loss: 0.7772 - acc: 0.6520 - val_loss: 0.8202 - val_acc: 0.5981\n","Epoch 142/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.7780 - acc: 0.6580 - val_loss: 0.8153 - val_acc: 0.6013\n","Epoch 143/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.7789 - acc: 0.6580 - val_loss: 0.8183 - val_acc: 0.6076\n","Epoch 144/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.7777 - acc: 0.6540 - val_loss: 0.8172 - val_acc: 0.6044\n","Epoch 145/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.7779 - acc: 0.6550 - val_loss: 0.8155 - val_acc: 0.6013\n","Epoch 146/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7816 - acc: 0.6460 - val_loss: 0.8213 - val_acc: 0.5981\n","Epoch 147/200\n","1000/1000 [==============================] - 0s 140us/step - loss: 0.7782 - acc: 0.6530 - val_loss: 0.8148 - val_acc: 0.5981\n","Epoch 148/200\n","1000/1000 [==============================] - 0s 123us/step - loss: 0.7769 - acc: 0.6550 - val_loss: 0.8182 - val_acc: 0.6076\n","Epoch 149/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7788 - acc: 0.6500 - val_loss: 0.8171 - val_acc: 0.6044\n","Epoch 150/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7779 - acc: 0.6500 - val_loss: 0.8144 - val_acc: 0.6013\n","Epoch 151/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7776 - acc: 0.6500 - val_loss: 0.8202 - val_acc: 0.5981\n","Epoch 152/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 0.7768 - acc: 0.6520 - val_loss: 0.8164 - val_acc: 0.6044\n","Epoch 153/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 0.7779 - acc: 0.6530 - val_loss: 0.8174 - val_acc: 0.6044\n","Epoch 154/200\n","1000/1000 [==============================] - 0s 126us/step - loss: 0.7780 - acc: 0.6510 - val_loss: 0.8198 - val_acc: 0.6076\n","Epoch 155/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 0.7788 - acc: 0.6500 - val_loss: 0.8189 - val_acc: 0.6076\n","Epoch 156/200\n","1000/1000 [==============================] - 0s 139us/step - loss: 0.7805 - acc: 0.6550 - val_loss: 0.8148 - val_acc: 0.6044\n","Epoch 157/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.7773 - acc: 0.6530 - val_loss: 0.8175 - val_acc: 0.6044\n","Epoch 158/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.7770 - acc: 0.6530 - val_loss: 0.8159 - val_acc: 0.6044\n","Epoch 159/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7777 - acc: 0.6500 - val_loss: 0.8183 - val_acc: 0.6076\n","Epoch 160/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 0.7781 - acc: 0.6540 - val_loss: 0.8195 - val_acc: 0.6076\n","Epoch 161/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.7785 - acc: 0.6550 - val_loss: 0.8154 - val_acc: 0.6013\n","Epoch 162/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7778 - acc: 0.6500 - val_loss: 0.8155 - val_acc: 0.6013\n","Epoch 163/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7789 - acc: 0.6540 - val_loss: 0.8158 - val_acc: 0.6013\n","Epoch 164/200\n","1000/1000 [==============================] - 0s 119us/step - loss: 0.7781 - acc: 0.6510 - val_loss: 0.8150 - val_acc: 0.6044\n","Epoch 165/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 0.7824 - acc: 0.6470 - val_loss: 0.8148 - val_acc: 0.6044\n","Epoch 166/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.7831 - acc: 0.6540 - val_loss: 0.8141 - val_acc: 0.6044\n","Epoch 167/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7843 - acc: 0.6460 - val_loss: 0.8177 - val_acc: 0.6076\n","Epoch 168/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7788 - acc: 0.6490 - val_loss: 0.8175 - val_acc: 0.6044\n","Epoch 169/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.7781 - acc: 0.6540 - val_loss: 0.8169 - val_acc: 0.6044\n","Epoch 170/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.7781 - acc: 0.6530 - val_loss: 0.8183 - val_acc: 0.6076\n","Epoch 171/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7780 - acc: 0.6480 - val_loss: 0.8173 - val_acc: 0.6044\n","Epoch 172/200\n","1000/1000 [==============================] - 0s 117us/step - loss: 0.7774 - acc: 0.6510 - val_loss: 0.8187 - val_acc: 0.6044\n","Epoch 173/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 0.7769 - acc: 0.6510 - val_loss: 0.8171 - val_acc: 0.6044\n","Epoch 174/200\n","1000/1000 [==============================] - 0s 129us/step - loss: 0.7788 - acc: 0.6520 - val_loss: 0.8145 - val_acc: 0.6044\n","Epoch 175/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.7787 - acc: 0.6460 - val_loss: 0.8230 - val_acc: 0.5981\n","Epoch 176/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.7774 - acc: 0.6530 - val_loss: 0.8169 - val_acc: 0.6013\n","Epoch 177/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.7772 - acc: 0.6520 - val_loss: 0.8209 - val_acc: 0.6013\n","Epoch 178/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.7785 - acc: 0.6530 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 179/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.7789 - acc: 0.6540 - val_loss: 0.8167 - val_acc: 0.6013\n","Epoch 180/200\n","1000/1000 [==============================] - 0s 122us/step - loss: 0.7770 - acc: 0.6530 - val_loss: 0.8186 - val_acc: 0.6044\n","Epoch 181/200\n","1000/1000 [==============================] - 0s 120us/step - loss: 0.7770 - acc: 0.6520 - val_loss: 0.8189 - val_acc: 0.6044\n","Epoch 182/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.7766 - acc: 0.6520 - val_loss: 0.8168 - val_acc: 0.6013\n","Epoch 183/200\n","1000/1000 [==============================] - 0s 121us/step - loss: 0.7785 - acc: 0.6510 - val_loss: 0.8168 - val_acc: 0.5981\n","Epoch 184/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.7781 - acc: 0.6560 - val_loss: 0.8170 - val_acc: 0.5981\n","Epoch 185/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 0.7768 - acc: 0.6500 - val_loss: 0.8187 - val_acc: 0.6044\n","Epoch 186/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7796 - acc: 0.6470 - val_loss: 0.8211 - val_acc: 0.6013\n","Epoch 187/200\n","1000/1000 [==============================] - 0s 126us/step - loss: 0.7817 - acc: 0.6490 - val_loss: 0.8145 - val_acc: 0.5981\n","Epoch 188/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.7829 - acc: 0.6470 - val_loss: 0.8213 - val_acc: 0.6013\n","Epoch 189/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7779 - acc: 0.6540 - val_loss: 0.8158 - val_acc: 0.6013\n","Epoch 190/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.7796 - acc: 0.6520 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 191/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7786 - acc: 0.6540 - val_loss: 0.8184 - val_acc: 0.6044\n","Epoch 192/200\n","1000/1000 [==============================] - 0s 125us/step - loss: 0.7768 - acc: 0.6530 - val_loss: 0.8174 - val_acc: 0.6013\n","Epoch 193/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7791 - acc: 0.6510 - val_loss: 0.8201 - val_acc: 0.6044\n","Epoch 194/200\n","1000/1000 [==============================] - 0s 145us/step - loss: 0.7783 - acc: 0.6530 - val_loss: 0.8191 - val_acc: 0.6044\n","Epoch 195/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.7769 - acc: 0.6520 - val_loss: 0.8178 - val_acc: 0.6044\n","Epoch 196/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.7769 - acc: 0.6550 - val_loss: 0.8200 - val_acc: 0.6044\n","Epoch 197/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7773 - acc: 0.6510 - val_loss: 0.8207 - val_acc: 0.6044\n","Epoch 198/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7774 - acc: 0.6520 - val_loss: 0.8153 - val_acc: 0.6013\n","Epoch 199/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.7796 - acc: 0.6510 - val_loss: 0.8180 - val_acc: 0.6013\n","Epoch 200/200\n","1000/1000 [==============================] - 0s 117us/step - loss: 0.7781 - acc: 0.6500 - val_loss: 0.8222 - val_acc: 0.5981\n"],"name":"stdout"}]},{"metadata":{"id":"zF-q8BuSzq5Z","colab_type":"code","outputId":"cb5c9f2b-3af8-413b-af35-6e3f12614ccd","colab":{"base_uri":"https://localhost:8080/","height":6834}},"cell_type":"code","source":["optimizer = optimizers.SGD(lr=0.001)\n","loss = 'categorical_crossentropy'\n","epochs = 200\n","batch_size = 54\n","\n","model.compile(optimizer= optimizer,\n","              loss = loss,\n","              metrics=['acc'])\n","\n","history_9 = model.fit(x_train, y_train,\n","                    epochs=epochs,\n","                    batch_size=batch_size,\n","                    validation_data=(x_val, y_val))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 1000 samples, validate on 316 samples\n","Epoch 1/200\n","1000/1000 [==============================] - 1s 1ms/step - loss: 0.7772 - acc: 0.6510 - val_loss: 0.8221 - val_acc: 0.5981\n","Epoch 2/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 0.7772 - acc: 0.6510 - val_loss: 0.8220 - val_acc: 0.5981\n","Epoch 3/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7772 - acc: 0.6510 - val_loss: 0.8219 - val_acc: 0.5981\n","Epoch 4/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 0.7771 - acc: 0.6510 - val_loss: 0.8218 - val_acc: 0.5981\n","Epoch 5/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 0.7771 - acc: 0.6510 - val_loss: 0.8218 - val_acc: 0.5981\n","Epoch 6/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7771 - acc: 0.6520 - val_loss: 0.8217 - val_acc: 0.5981\n","Epoch 7/200\n","1000/1000 [==============================] - 0s 94us/step - loss: 0.7771 - acc: 0.6510 - val_loss: 0.8216 - val_acc: 0.5981\n","Epoch 8/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7770 - acc: 0.6510 - val_loss: 0.8216 - val_acc: 0.5981\n","Epoch 9/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7770 - acc: 0.6500 - val_loss: 0.8215 - val_acc: 0.6013\n","Epoch 10/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 0.7770 - acc: 0.6500 - val_loss: 0.8215 - val_acc: 0.6013\n","Epoch 11/200\n","1000/1000 [==============================] - 0s 123us/step - loss: 0.7770 - acc: 0.6500 - val_loss: 0.8214 - val_acc: 0.6013\n","Epoch 12/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 0.7770 - acc: 0.6500 - val_loss: 0.8213 - val_acc: 0.6013\n","Epoch 13/200\n","1000/1000 [==============================] - 0s 120us/step - loss: 0.7769 - acc: 0.6500 - val_loss: 0.8213 - val_acc: 0.6013\n","Epoch 14/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7769 - acc: 0.6500 - val_loss: 0.8212 - val_acc: 0.6013\n","Epoch 15/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 0.7769 - acc: 0.6500 - val_loss: 0.8212 - val_acc: 0.6044\n","Epoch 16/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7769 - acc: 0.6490 - val_loss: 0.8211 - val_acc: 0.6076\n","Epoch 17/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7768 - acc: 0.6490 - val_loss: 0.8211 - val_acc: 0.6076\n","Epoch 18/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 0.7768 - acc: 0.6490 - val_loss: 0.8210 - val_acc: 0.6076\n","Epoch 19/200\n","1000/1000 [==============================] - 0s 119us/step - loss: 0.7768 - acc: 0.6500 - val_loss: 0.8210 - val_acc: 0.6076\n","Epoch 20/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7768 - acc: 0.6500 - val_loss: 0.8209 - val_acc: 0.6076\n","Epoch 21/200\n","1000/1000 [==============================] - 0s 116us/step - loss: 0.7768 - acc: 0.6500 - val_loss: 0.8209 - val_acc: 0.6076\n","Epoch 22/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.7768 - acc: 0.6500 - val_loss: 0.8208 - val_acc: 0.6076\n","Epoch 23/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7767 - acc: 0.6500 - val_loss: 0.8207 - val_acc: 0.6076\n","Epoch 24/200\n","1000/1000 [==============================] - 0s 94us/step - loss: 0.7767 - acc: 0.6500 - val_loss: 0.8207 - val_acc: 0.6076\n","Epoch 25/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7767 - acc: 0.6510 - val_loss: 0.8206 - val_acc: 0.6076\n","Epoch 26/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 0.7767 - acc: 0.6510 - val_loss: 0.8206 - val_acc: 0.6076\n","Epoch 27/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7767 - acc: 0.6510 - val_loss: 0.8205 - val_acc: 0.6076\n","Epoch 28/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7766 - acc: 0.6510 - val_loss: 0.8205 - val_acc: 0.6076\n","Epoch 29/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 0.7766 - acc: 0.6510 - val_loss: 0.8204 - val_acc: 0.6076\n","Epoch 30/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.7766 - acc: 0.6510 - val_loss: 0.8204 - val_acc: 0.6076\n","Epoch 31/200\n","1000/1000 [==============================] - 0s 119us/step - loss: 0.7766 - acc: 0.6510 - val_loss: 0.8203 - val_acc: 0.6108\n","Epoch 32/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7766 - acc: 0.6510 - val_loss: 0.8203 - val_acc: 0.6108\n","Epoch 33/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 0.7766 - acc: 0.6510 - val_loss: 0.8202 - val_acc: 0.6108\n","Epoch 34/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7766 - acc: 0.6510 - val_loss: 0.8202 - val_acc: 0.6108\n","Epoch 35/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.7765 - acc: 0.6520 - val_loss: 0.8202 - val_acc: 0.6108\n","Epoch 36/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 0.7765 - acc: 0.6520 - val_loss: 0.8201 - val_acc: 0.6108\n","Epoch 37/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 0.7765 - acc: 0.6520 - val_loss: 0.8201 - val_acc: 0.6108\n","Epoch 38/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 0.7765 - acc: 0.6520 - val_loss: 0.8201 - val_acc: 0.6108\n","Epoch 39/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.7765 - acc: 0.6520 - val_loss: 0.8200 - val_acc: 0.6108\n","Epoch 40/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 0.7765 - acc: 0.6520 - val_loss: 0.8200 - val_acc: 0.6108\n","Epoch 41/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 0.7765 - acc: 0.6520 - val_loss: 0.8200 - val_acc: 0.6108\n","Epoch 42/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.7765 - acc: 0.6510 - val_loss: 0.8199 - val_acc: 0.6108\n","Epoch 43/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7765 - acc: 0.6510 - val_loss: 0.8199 - val_acc: 0.6076\n","Epoch 44/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7764 - acc: 0.6510 - val_loss: 0.8199 - val_acc: 0.6044\n","Epoch 45/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7764 - acc: 0.6510 - val_loss: 0.8198 - val_acc: 0.6044\n","Epoch 46/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7764 - acc: 0.6510 - val_loss: 0.8198 - val_acc: 0.6044\n","Epoch 47/200\n","1000/1000 [==============================] - 0s 120us/step - loss: 0.7764 - acc: 0.6510 - val_loss: 0.8198 - val_acc: 0.6044\n","Epoch 48/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7764 - acc: 0.6510 - val_loss: 0.8197 - val_acc: 0.6044\n","Epoch 49/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7764 - acc: 0.6500 - val_loss: 0.8197 - val_acc: 0.6044\n","Epoch 50/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7764 - acc: 0.6500 - val_loss: 0.8197 - val_acc: 0.6044\n","Epoch 51/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7764 - acc: 0.6500 - val_loss: 0.8197 - val_acc: 0.6044\n","Epoch 52/200\n","1000/1000 [==============================] - 0s 94us/step - loss: 0.7764 - acc: 0.6500 - val_loss: 0.8196 - val_acc: 0.6044\n","Epoch 53/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7764 - acc: 0.6500 - val_loss: 0.8196 - val_acc: 0.6044\n","Epoch 54/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7764 - acc: 0.6500 - val_loss: 0.8196 - val_acc: 0.6044\n","Epoch 55/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 0.7764 - acc: 0.6500 - val_loss: 0.8196 - val_acc: 0.6044\n","Epoch 56/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7764 - acc: 0.6510 - val_loss: 0.8196 - val_acc: 0.6044\n","Epoch 57/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7764 - acc: 0.6510 - val_loss: 0.8195 - val_acc: 0.6044\n","Epoch 58/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 0.7764 - acc: 0.6510 - val_loss: 0.8195 - val_acc: 0.6044\n","Epoch 59/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.7763 - acc: 0.6510 - val_loss: 0.8195 - val_acc: 0.6044\n","Epoch 60/200\n","1000/1000 [==============================] - 0s 118us/step - loss: 0.7763 - acc: 0.6510 - val_loss: 0.8194 - val_acc: 0.6044\n","Epoch 61/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7763 - acc: 0.6510 - val_loss: 0.8194 - val_acc: 0.6044\n","Epoch 62/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.7763 - acc: 0.6510 - val_loss: 0.8194 - val_acc: 0.6044\n","Epoch 63/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7763 - acc: 0.6510 - val_loss: 0.8194 - val_acc: 0.6044\n","Epoch 64/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7763 - acc: 0.6510 - val_loss: 0.8194 - val_acc: 0.6044\n","Epoch 65/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7763 - acc: 0.6510 - val_loss: 0.8193 - val_acc: 0.6044\n","Epoch 66/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7763 - acc: 0.6510 - val_loss: 0.8193 - val_acc: 0.6044\n","Epoch 67/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 0.7763 - acc: 0.6510 - val_loss: 0.8193 - val_acc: 0.6044\n","Epoch 68/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7763 - acc: 0.6510 - val_loss: 0.8193 - val_acc: 0.6044\n","Epoch 69/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7763 - acc: 0.6510 - val_loss: 0.8193 - val_acc: 0.6044\n","Epoch 70/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.7763 - acc: 0.6520 - val_loss: 0.8192 - val_acc: 0.6044\n","Epoch 71/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7763 - acc: 0.6520 - val_loss: 0.8192 - val_acc: 0.6044\n","Epoch 72/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 0.7763 - acc: 0.6520 - val_loss: 0.8192 - val_acc: 0.6044\n","Epoch 73/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 0.7763 - acc: 0.6520 - val_loss: 0.8192 - val_acc: 0.6044\n","Epoch 74/200\n","1000/1000 [==============================] - 0s 94us/step - loss: 0.7763 - acc: 0.6520 - val_loss: 0.8192 - val_acc: 0.6044\n","Epoch 75/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7763 - acc: 0.6520 - val_loss: 0.8192 - val_acc: 0.6044\n","Epoch 76/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 0.7763 - acc: 0.6520 - val_loss: 0.8191 - val_acc: 0.6044\n","Epoch 77/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7763 - acc: 0.6520 - val_loss: 0.8191 - val_acc: 0.6044\n","Epoch 78/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7763 - acc: 0.6520 - val_loss: 0.8191 - val_acc: 0.6044\n","Epoch 79/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7763 - acc: 0.6520 - val_loss: 0.8191 - val_acc: 0.6044\n","Epoch 80/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.7763 - acc: 0.6520 - val_loss: 0.8191 - val_acc: 0.6044\n","Epoch 81/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8190 - val_acc: 0.6044\n","Epoch 82/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7762 - acc: 0.6530 - val_loss: 0.8190 - val_acc: 0.6044\n","Epoch 83/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7762 - acc: 0.6530 - val_loss: 0.8190 - val_acc: 0.6044\n","Epoch 84/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 0.7762 - acc: 0.6530 - val_loss: 0.8190 - val_acc: 0.6044\n","Epoch 85/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 0.7762 - acc: 0.6530 - val_loss: 0.8190 - val_acc: 0.6044\n","Epoch 86/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7762 - acc: 0.6530 - val_loss: 0.8189 - val_acc: 0.6044\n","Epoch 87/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7762 - acc: 0.6530 - val_loss: 0.8189 - val_acc: 0.6044\n","Epoch 88/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 0.7762 - acc: 0.6530 - val_loss: 0.8189 - val_acc: 0.6044\n","Epoch 89/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.7762 - acc: 0.6530 - val_loss: 0.8189 - val_acc: 0.6044\n","Epoch 90/200\n","1000/1000 [==============================] - 0s 122us/step - loss: 0.7762 - acc: 0.6530 - val_loss: 0.8189 - val_acc: 0.6044\n","Epoch 91/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 0.7762 - acc: 0.6530 - val_loss: 0.8189 - val_acc: 0.6044\n","Epoch 92/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7762 - acc: 0.6530 - val_loss: 0.8188 - val_acc: 0.6044\n","Epoch 93/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7762 - acc: 0.6530 - val_loss: 0.8188 - val_acc: 0.6044\n","Epoch 94/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8188 - val_acc: 0.6044\n","Epoch 95/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8188 - val_acc: 0.6044\n","Epoch 96/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8188 - val_acc: 0.6044\n","Epoch 97/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8188 - val_acc: 0.6044\n","Epoch 98/200\n","1000/1000 [==============================] - 0s 125us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8187 - val_acc: 0.6044\n","Epoch 99/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8187 - val_acc: 0.6044\n","Epoch 100/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8187 - val_acc: 0.6044\n","Epoch 101/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8187 - val_acc: 0.6044\n","Epoch 102/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8187 - val_acc: 0.6044\n","Epoch 103/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8187 - val_acc: 0.6044\n","Epoch 104/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8187 - val_acc: 0.6044\n","Epoch 105/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8186 - val_acc: 0.6044\n","Epoch 106/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8186 - val_acc: 0.6044\n","Epoch 107/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8186 - val_acc: 0.6044\n","Epoch 108/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8186 - val_acc: 0.6044\n","Epoch 109/200\n","1000/1000 [==============================] - 0s 117us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8186 - val_acc: 0.6044\n","Epoch 110/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8186 - val_acc: 0.6044\n","Epoch 111/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8185 - val_acc: 0.6044\n","Epoch 112/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8185 - val_acc: 0.6044\n","Epoch 113/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8185 - val_acc: 0.6044\n","Epoch 114/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8185 - val_acc: 0.6044\n","Epoch 115/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8185 - val_acc: 0.6044\n","Epoch 116/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8185 - val_acc: 0.6044\n","Epoch 117/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7762 - acc: 0.6510 - val_loss: 0.8185 - val_acc: 0.6044\n","Epoch 118/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8185 - val_acc: 0.6044\n","Epoch 119/200\n","1000/1000 [==============================] - 0s 117us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8185 - val_acc: 0.6044\n","Epoch 120/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8185 - val_acc: 0.6044\n","Epoch 121/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7762 - acc: 0.6510 - val_loss: 0.8184 - val_acc: 0.6044\n","Epoch 122/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8184 - val_acc: 0.6044\n","Epoch 123/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8184 - val_acc: 0.6044\n","Epoch 124/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8184 - val_acc: 0.6044\n","Epoch 125/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8184 - val_acc: 0.6044\n","Epoch 126/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8184 - val_acc: 0.6044\n","Epoch 127/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8184 - val_acc: 0.6044\n","Epoch 128/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8184 - val_acc: 0.6044\n","Epoch 129/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8184 - val_acc: 0.6044\n","Epoch 130/200\n","1000/1000 [==============================] - 0s 93us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8184 - val_acc: 0.6044\n","Epoch 131/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8184 - val_acc: 0.6044\n","Epoch 132/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8183 - val_acc: 0.6044\n","Epoch 133/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8183 - val_acc: 0.6044\n","Epoch 134/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8183 - val_acc: 0.6044\n","Epoch 135/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8183 - val_acc: 0.6044\n","Epoch 136/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8183 - val_acc: 0.6044\n","Epoch 137/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8183 - val_acc: 0.6044\n","Epoch 138/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8183 - val_acc: 0.6044\n","Epoch 139/200\n","1000/1000 [==============================] - 0s 118us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8183 - val_acc: 0.6044\n","Epoch 140/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8183 - val_acc: 0.6044\n","Epoch 141/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8183 - val_acc: 0.6044\n","Epoch 142/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8183 - val_acc: 0.6044\n","Epoch 143/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8183 - val_acc: 0.6044\n","Epoch 144/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8183 - val_acc: 0.6044\n","Epoch 145/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8183 - val_acc: 0.6044\n","Epoch 146/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8183 - val_acc: 0.6044\n","Epoch 147/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 148/200\n","1000/1000 [==============================] - 0s 127us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 149/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 150/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 151/200\n","1000/1000 [==============================] - 0s 93us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 152/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 153/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 154/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 155/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 156/200\n","1000/1000 [==============================] - 0s 120us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 157/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 158/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 159/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 160/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 161/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 162/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 163/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 164/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 165/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 166/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 167/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 168/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 169/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 170/200\n","1000/1000 [==============================] - 0s 92us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 171/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 172/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 173/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 174/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 175/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 0.7761 - acc: 0.6490 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 176/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 177/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 178/200\n","1000/1000 [==============================] - 0s 124us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 179/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 180/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 181/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 182/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 183/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 184/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 185/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 186/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 187/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 188/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 189/200\n","1000/1000 [==============================] - 0s 94us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 190/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 191/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 192/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 193/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8179 - val_acc: 0.6044\n","Epoch 194/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 195/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 196/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8179 - val_acc: 0.6044\n","Epoch 197/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8179 - val_acc: 0.6044\n","Epoch 198/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8179 - val_acc: 0.6044\n","Epoch 199/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8179 - val_acc: 0.6044\n","Epoch 200/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8179 - val_acc: 0.6044\n"],"name":"stdout"}]},{"metadata":{"id":"TAoRjwxvzwHB","colab_type":"code","outputId":"7e93dc52-d4be-473a-827d-849ae02bc884","colab":{"base_uri":"https://localhost:8080/","height":6834}},"cell_type":"code","source":["optimizer = optimizers.SGD(lr=0.01)\n","loss = 'categorical_crossentropy'\n","epochs = 200\n","batch_size = 54\n","\n","model.compile(optimizer= optimizer,\n","              loss = loss,\n","              metrics=['acc'])\n","\n","history_9 = model.fit(x_train, y_train,\n","                    epochs=epochs,\n","                    batch_size=batch_size,\n","                    validation_data=(x_val, y_val))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 1000 samples, validate on 316 samples\n","Epoch 1/200\n","1000/1000 [==============================] - 1s 1ms/step - loss: 0.7762 - acc: 0.6500 - val_loss: 0.8178 - val_acc: 0.6044\n","Epoch 2/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7762 - acc: 0.6510 - val_loss: 0.8177 - val_acc: 0.6044\n","Epoch 3/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 0.7762 - acc: 0.6510 - val_loss: 0.8177 - val_acc: 0.6044\n","Epoch 4/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8176 - val_acc: 0.6044\n","Epoch 5/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8175 - val_acc: 0.6044\n","Epoch 6/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 0.7762 - acc: 0.6530 - val_loss: 0.8176 - val_acc: 0.6044\n","Epoch 7/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8175 - val_acc: 0.6044\n","Epoch 8/200\n","1000/1000 [==============================] - 0s 93us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8175 - val_acc: 0.6044\n","Epoch 9/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 0.7762 - acc: 0.6530 - val_loss: 0.8175 - val_acc: 0.6044\n","Epoch 10/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 0.7762 - acc: 0.6510 - val_loss: 0.8176 - val_acc: 0.6044\n","Epoch 11/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7762 - acc: 0.6530 - val_loss: 0.8176 - val_acc: 0.6044\n","Epoch 12/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.7762 - acc: 0.6510 - val_loss: 0.8176 - val_acc: 0.6044\n","Epoch 13/200\n","1000/1000 [==============================] - 0s 120us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8176 - val_acc: 0.6044\n","Epoch 14/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7762 - acc: 0.6510 - val_loss: 0.8176 - val_acc: 0.6044\n","Epoch 15/200\n","1000/1000 [==============================] - 0s 94us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8176 - val_acc: 0.6044\n","Epoch 16/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8176 - val_acc: 0.6044\n","Epoch 17/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 0.7762 - acc: 0.6530 - val_loss: 0.8176 - val_acc: 0.6044\n","Epoch 18/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8176 - val_acc: 0.6044\n","Epoch 19/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7762 - acc: 0.6530 - val_loss: 0.8176 - val_acc: 0.6044\n","Epoch 20/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8176 - val_acc: 0.6044\n","Epoch 21/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8176 - val_acc: 0.6044\n","Epoch 22/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8176 - val_acc: 0.6044\n","Epoch 23/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8176 - val_acc: 0.6044\n","Epoch 24/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8176 - val_acc: 0.6044\n","Epoch 25/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8175 - val_acc: 0.6044\n","Epoch 26/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8175 - val_acc: 0.6044\n","Epoch 27/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8174 - val_acc: 0.6044\n","Epoch 28/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8174 - val_acc: 0.6044\n","Epoch 29/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8175 - val_acc: 0.6044\n","Epoch 30/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8174 - val_acc: 0.6044\n","Epoch 31/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8175 - val_acc: 0.6044\n","Epoch 32/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8174 - val_acc: 0.6044\n","Epoch 33/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8174 - val_acc: 0.6044\n","Epoch 34/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8173 - val_acc: 0.6044\n","Epoch 35/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8174 - val_acc: 0.6044\n","Epoch 36/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8174 - val_acc: 0.6044\n","Epoch 37/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8174 - val_acc: 0.6044\n","Epoch 38/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8175 - val_acc: 0.6044\n","Epoch 39/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8174 - val_acc: 0.6044\n","Epoch 40/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8174 - val_acc: 0.6044\n","Epoch 41/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8175 - val_acc: 0.6044\n","Epoch 42/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8175 - val_acc: 0.6044\n","Epoch 43/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8175 - val_acc: 0.6044\n","Epoch 44/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8175 - val_acc: 0.6044\n","Epoch 45/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8176 - val_acc: 0.6044\n","Epoch 46/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8176 - val_acc: 0.6044\n","Epoch 47/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8176 - val_acc: 0.6044\n","Epoch 48/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8177 - val_acc: 0.6044\n","Epoch 49/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8177 - val_acc: 0.6044\n","Epoch 50/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8177 - val_acc: 0.6044\n","Epoch 51/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8176 - val_acc: 0.6044\n","Epoch 52/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7762 - acc: 0.6530 - val_loss: 0.8177 - val_acc: 0.6044\n","Epoch 53/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8177 - val_acc: 0.6044\n","Epoch 54/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8177 - val_acc: 0.6044\n","Epoch 55/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8178 - val_acc: 0.6044\n","Epoch 56/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8177 - val_acc: 0.6044\n","Epoch 57/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8178 - val_acc: 0.6044\n","Epoch 58/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8177 - val_acc: 0.6044\n","Epoch 59/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8177 - val_acc: 0.6044\n","Epoch 60/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7762 - acc: 0.6510 - val_loss: 0.8177 - val_acc: 0.6044\n","Epoch 61/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8178 - val_acc: 0.6044\n","Epoch 62/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8178 - val_acc: 0.6044\n","Epoch 63/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8177 - val_acc: 0.6044\n","Epoch 64/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8178 - val_acc: 0.6044\n","Epoch 65/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8178 - val_acc: 0.6044\n","Epoch 66/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8178 - val_acc: 0.6044\n","Epoch 67/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8178 - val_acc: 0.6044\n","Epoch 68/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7762 - acc: 0.6530 - val_loss: 0.8179 - val_acc: 0.6044\n","Epoch 69/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8179 - val_acc: 0.6044\n","Epoch 70/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8178 - val_acc: 0.6044\n","Epoch 71/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8179 - val_acc: 0.6044\n","Epoch 72/200\n","1000/1000 [==============================] - 0s 129us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8178 - val_acc: 0.6044\n","Epoch 73/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8179 - val_acc: 0.6044\n","Epoch 74/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 75/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 76/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 77/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 78/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 79/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 80/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 81/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 82/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 83/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 84/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 85/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 86/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 87/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8179 - val_acc: 0.6044\n","Epoch 88/200\n","1000/1000 [==============================] - 0s 94us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8179 - val_acc: 0.6044\n","Epoch 89/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8179 - val_acc: 0.6044\n","Epoch 90/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8179 - val_acc: 0.6044\n","Epoch 91/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 92/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 93/200\n","1000/1000 [==============================] - 0s 94us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 94/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8179 - val_acc: 0.6044\n","Epoch 95/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 96/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 97/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 98/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 99/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 100/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 101/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 102/200\n","1000/1000 [==============================] - 0s 122us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8179 - val_acc: 0.6013\n","Epoch 103/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 104/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 105/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 106/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 107/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 108/200\n","1000/1000 [==============================] - 0s 94us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 109/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 110/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 111/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 112/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 113/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 114/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 115/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 116/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 117/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 118/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 119/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 120/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 121/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 122/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 123/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 124/200\n","1000/1000 [==============================] - 0s 121us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 125/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 126/200\n","1000/1000 [==============================] - 0s 93us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 127/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 128/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 129/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8180 - val_acc: 0.6013\n","Epoch 130/200\n","1000/1000 [==============================] - 0s 104us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8180 - val_acc: 0.6013\n","Epoch 131/200\n","1000/1000 [==============================] - 0s 109us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 132/200\n","1000/1000 [==============================] - 0s 118us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 133/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8180 - val_acc: 0.6013\n","Epoch 134/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8180 - val_acc: 0.6013\n","Epoch 135/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8179 - val_acc: 0.6013\n","Epoch 136/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8179 - val_acc: 0.6013\n","Epoch 137/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7762 - acc: 0.6520 - val_loss: 0.8179 - val_acc: 0.6013\n","Epoch 138/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 139/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 140/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 141/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 142/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 143/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 144/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 145/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 146/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 147/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 148/200\n","1000/1000 [==============================] - 0s 92us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8179 - val_acc: 0.6013\n","Epoch 149/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 150/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 151/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 152/200\n","1000/1000 [==============================] - 0s 114us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 153/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8179 - val_acc: 0.6013\n","Epoch 154/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8179 - val_acc: 0.6044\n","Epoch 155/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8179 - val_acc: 0.6013\n","Epoch 156/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 157/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 158/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 159/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 160/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8179 - val_acc: 0.6013\n","Epoch 161/200\n","1000/1000 [==============================] - 0s 110us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8180 - val_acc: 0.6013\n","Epoch 162/200\n","1000/1000 [==============================] - 0s 113us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8180 - val_acc: 0.6013\n","Epoch 163/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8180 - val_acc: 0.6013\n","Epoch 164/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 165/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8180 - val_acc: 0.6013\n","Epoch 166/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8179 - val_acc: 0.6013\n","Epoch 167/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 168/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 169/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 170/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 171/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 172/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 173/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 174/200\n","1000/1000 [==============================] - 0s 99us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 175/200\n","1000/1000 [==============================] - 0s 122us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 176/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6044\n","Epoch 177/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 178/200\n","1000/1000 [==============================] - 0s 98us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8181 - val_acc: 0.6013\n","Epoch 179/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 180/200\n","1000/1000 [==============================] - 0s 106us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 181/200\n","1000/1000 [==============================] - 0s 108us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8181 - val_acc: 0.6013\n","Epoch 182/200\n","1000/1000 [==============================] - 0s 102us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 183/200\n","1000/1000 [==============================] - 0s 112us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8183 - val_acc: 0.6044\n","Epoch 184/200\n","1000/1000 [==============================] - 0s 105us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8183 - val_acc: 0.6044\n","Epoch 185/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 186/200\n","1000/1000 [==============================] - 0s 97us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 187/200\n","1000/1000 [==============================] - 0s 103us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 188/200\n","1000/1000 [==============================] - 0s 101us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 189/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 190/200\n","1000/1000 [==============================] - 0s 111us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 191/200\n","1000/1000 [==============================] - 0s 115us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8181 - val_acc: 0.6013\n","Epoch 192/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 193/200\n","1000/1000 [==============================] - 0s 96us/step - loss: 0.7761 - acc: 0.6510 - val_loss: 0.8182 - val_acc: 0.6044\n","Epoch 194/200\n","1000/1000 [==============================] - 0s 100us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 195/200\n","1000/1000 [==============================] - 0s 107us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 196/200\n","1000/1000 [==============================] - 0s 121us/step - loss: 0.7761 - acc: 0.6530 - val_loss: 0.8181 - val_acc: 0.6044\n","Epoch 197/200\n","1000/1000 [==============================] - 0s 95us/step - loss: 0.7761 - acc: 0.6500 - val_loss: 0.8180 - val_acc: 0.6013\n","Epoch 198/200\n","1000/1000 [==============================] - 0s 84us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8180 - val_acc: 0.6013\n","Epoch 199/200\n","1000/1000 [==============================] - 0s 84us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8180 - val_acc: 0.6013\n","Epoch 200/200\n","1000/1000 [==============================] - 0s 86us/step - loss: 0.7761 - acc: 0.6520 - val_loss: 0.8180 - val_acc: 0.6013\n"],"name":"stdout"}]},{"metadata":{"id":"PWyJ9QKCyBCK","colab_type":"code","outputId":"5bfc696f-5f3c-42d6-cdde-203e2de94d8b","colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["model.evaluate(x_test, y_test)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["328/328 [==============================] - 0s 90us/step\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.8900761676997673, 0.5670731707317073]"]},"metadata":{"tags":[]},"execution_count":87}]},{"metadata":{"id":"1LeTTLUXwUSY","colab_type":"code","outputId":"84df3099-5ddf-44ee-faf5-abe9dfefaa87","colab":{"base_uri":"https://localhost:8080/","height":365}},"cell_type":"code","source":["import seaborn as sns\n","from sklearn import metrics\n","\n","\n","y_pred = model.predict(x_test)\n","matrix = metrics.confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n","sns.heatmap(matrix,annot=True,fmt='.5g')"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f6e835a0518>"]},"metadata":{"tags":[]},"execution_count":88},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAcEAAAFLCAYAAACukDdWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEYIAAMciBAEAjkUIAgAc6/8DCydmV75KYe8AAAAASUVORK5CYII=\n","text/plain":["<Figure size 576x396 with 2 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"7PgSEy8YvIKu","colab_type":"text"},"cell_type":"markdown","source":["### 2.  IMBD - Keras Embedding"]},{"metadata":{"id":"UKdFBSddGYSj","colab_type":"text"},"cell_type":"markdown","source":["####1. Build Keras Embedding Layer"]},{"metadata":{"id":"TdS9_aWEfvOQ","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.layers import Embedding\n","\n","# The Embedding layer takes at least two arguments:\n","# the number of possible tokens, here 1000 (1 + maximum word index),\n","# and the dimensionality of the embeddings, here 64.\n","embedding_layer = Embedding(1000, 64)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6olvc3S1GOJ3","colab_type":"text"},"cell_type":"markdown","source":["####2. Load, Pad Data"]},{"metadata":{"id":"fCRBi4d0f1bC","colab_type":"code","outputId":"f64eb1be-528e-467c-9dc8-65e394856691","colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["from keras.datasets import imdb\n","from keras import preprocessing\n","\n","# Number of words to consider as features\n","max_features = 10000\n","# Cut texts after this number of words \n","# (among top max_features most common words)\n","maxlen = 20\n","\n","# Load the data as lists of integers.\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n","\n","# This turns our lists of integers\n","# into a 2D integer tensor of shape `(samples, maxlen)`\n","x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n","17465344/17464789 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"metadata":{"id":"I-KCLq3ZGT2B","colab_type":"text"},"cell_type":"markdown","source":["#### 3. Build and Train Model"]},{"metadata":{"id":"lAUNHh-qqJVT","colab_type":"code","outputId":"c4deb021-927f-41ff-dd07-a49fea4f40cb","colab":{"base_uri":"https://localhost:8080/","height":615}},"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Flatten, Dense\n","\n","model = Sequential()\n","# We specify the maximum input length to our Embedding layer\n","# so we can later flatten the embedded inputs\n","model.add(Embedding(10000, 8, input_length=maxlen))\n","# After the Embedding layer, \n","# our activations have shape `(samples, maxlen, 8)`.\n","\n","# We flatten the 3D tensor of embeddings \n","# into a 2D tensor of shape `(samples, maxlen * 8)`\n","model.add(Flatten())\n","\n","# We add the classifier on top\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n","model.summary()\n","\n","history = model.fit(x_train, y_train,\n","                    epochs=10,\n","                    batch_size=32,\n","                    validation_split=0.2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_9 (Embedding)      (None, 20, 8)             80000     \n","_________________________________________________________________\n","flatten_8 (Flatten)          (None, 160)               0         \n","_________________________________________________________________\n","dense_19 (Dense)             (None, 1)                 161       \n","=================================================================\n","Total params: 80,161\n","Trainable params: 80,161\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 20000 samples, validate on 5000 samples\n","Epoch 1/10\n","20000/20000 [==============================] - 4s 215us/step - loss: 0.6759 - acc: 0.6050 - val_loss: 0.6398 - val_acc: 0.6814\n","Epoch 2/10\n","20000/20000 [==============================] - 3s 152us/step - loss: 0.5657 - acc: 0.7427 - val_loss: 0.5467 - val_acc: 0.7206\n","Epoch 3/10\n","20000/20000 [==============================] - 3s 152us/step - loss: 0.4752 - acc: 0.7808 - val_loss: 0.5113 - val_acc: 0.7384\n","Epoch 4/10\n","20000/20000 [==============================] - 3s 152us/step - loss: 0.4263 - acc: 0.8077 - val_loss: 0.5008 - val_acc: 0.7452\n","Epoch 5/10\n","20000/20000 [==============================] - 3s 151us/step - loss: 0.3930 - acc: 0.8258 - val_loss: 0.4981 - val_acc: 0.7538\n","Epoch 6/10\n","20000/20000 [==============================] - 3s 159us/step - loss: 0.3668 - acc: 0.8395 - val_loss: 0.5014 - val_acc: 0.7530\n","Epoch 7/10\n","20000/20000 [==============================] - 3s 156us/step - loss: 0.3435 - acc: 0.8533 - val_loss: 0.5052 - val_acc: 0.7520\n","Epoch 8/10\n","20000/20000 [==============================] - 3s 154us/step - loss: 0.3223 - acc: 0.8657 - val_loss: 0.5132 - val_acc: 0.7486\n","Epoch 9/10\n","20000/20000 [==============================] - 3s 152us/step - loss: 0.3022 - acc: 0.8766 - val_loss: 0.5213 - val_acc: 0.7490\n","Epoch 10/10\n","20000/20000 [==============================] - 3s 152us/step - loss: 0.2839 - acc: 0.8860 - val_loss: 0.5303 - val_acc: 0.7466\n"],"name":"stdout"}]}]}