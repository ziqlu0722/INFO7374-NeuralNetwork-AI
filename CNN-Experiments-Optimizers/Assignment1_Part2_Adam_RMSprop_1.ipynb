{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. IMPORT LIBRARIES AND CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import numpy as np\n",
    "import keras\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "# dataset\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# modeling tools\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check and Prepare Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape: (50000, 32, 32, 3)\n",
      "test data shape: (10000, 32, 32, 3)\n",
      "shape of one instance: (32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "# check data dimension\n",
    "print('training data shape: {}'.format(x_train.shape))\n",
    "print('test data shape: {}'.format(x_test.shape))\n",
    "print('shape of one instance: {}'.format(x_train.shape[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training labels are: [6, 9, 4, 1, 2, 7, 8, 3, 5, 0]\n",
      "# labels: 10\n"
     ]
    }
   ],
   "source": [
    "# check labels\n",
    "labels = []\n",
    "for y in y_train.flatten():\n",
    "    if y not in labels:\n",
    "        labels.append(y)\n",
    "print('training labels are: {}'.format(labels))\n",
    "print('# labels: {}'.format(len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to one-hot encoded vectors.\n",
    "y_train = to_categorical(y_train, num_classes = 10)\n",
    "y_test = to_categorical(y_test, num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transform images from (32,32,3) to 3072-dimensional vectors (32*32*3)\n",
    "\n",
    "# x_train = np.reshape(x_train,(50000,3072))\n",
    "# x_test = np.reshape(x_test,(10000,3072))\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalization of pixel values (to [0-1] range)\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. HELP FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotAcc(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model_accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc = 'upper right')\n",
    "    \n",
    "def plotLoss(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model_loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc = 'upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. BUILD CNN CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes= 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RMSprop - LR = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 253s 162ms/step - loss: 1.8749 - acc: 0.3083 - val_loss: 1.5650 - val_acc: 0.4298\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 245s 157ms/step - loss: 1.5941 - acc: 0.4188 - val_loss: 1.4158 - val_acc: 0.4894\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 245s 156ms/step - loss: 1.4657 - acc: 0.4690 - val_loss: 1.4036 - val_acc: 0.4968\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 230s 147ms/step - loss: 1.3819 - acc: 0.5057 - val_loss: 1.2733 - val_acc: 0.5404\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 248s 158ms/step - loss: 1.3113 - acc: 0.5358 - val_loss: 1.1206 - val_acc: 0.6031\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 318s 203ms/step - loss: 1.2460 - acc: 0.5551 - val_loss: 1.1075 - val_acc: 0.6033\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 442s 283ms/step - loss: 1.1946 - acc: 0.5773 - val_loss: 1.0258 - val_acc: 0.6363\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 407s 261ms/step - loss: 1.1551 - acc: 0.5926 - val_loss: 0.9976 - val_acc: 0.6450\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 418s 268ms/step - loss: 1.1173 - acc: 0.6055 - val_loss: 0.9652 - val_acc: 0.6630\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 251s 161ms/step - loss: 1.0870 - acc: 0.6169 - val_loss: 0.9746 - val_acc: 0.6542\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 255s 163ms/step - loss: 1.0597 - acc: 0.6298 - val_loss: 0.9068 - val_acc: 0.6800\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 244s 156ms/step - loss: 1.0346 - acc: 0.6373 - val_loss: 0.9481 - val_acc: 0.6671\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 428s 274ms/step - loss: 1.0179 - acc: 0.6416 - val_loss: 0.8614 - val_acc: 0.6987\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 427s 273ms/step - loss: 0.9958 - acc: 0.6504 - val_loss: 0.8416 - val_acc: 0.7069\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 392s 251ms/step - loss: 0.9854 - acc: 0.6534 - val_loss: 0.8540 - val_acc: 0.6978\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 359s 230ms/step - loss: 0.9678 - acc: 0.6626 - val_loss: 0.9324 - val_acc: 0.6763\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 213s 136ms/step - loss: 0.9526 - acc: 0.6673 - val_loss: 0.8121 - val_acc: 0.7176\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 296s 190ms/step - loss: 0.9412 - acc: 0.6724 - val_loss: 0.7935 - val_acc: 0.7244\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 471s 301ms/step - loss: 0.9362 - acc: 0.6726 - val_loss: 0.8179 - val_acc: 0.7172\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 421s 269ms/step - loss: 0.9180 - acc: 0.6800 - val_loss: 0.7978 - val_acc: 0.7319\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 397s 254ms/step - loss: 0.9114 - acc: 0.6863 - val_loss: 0.7821 - val_acc: 0.7349\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 267s 171ms/step - loss: 0.9034 - acc: 0.6877 - val_loss: 0.8066 - val_acc: 0.7252\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 433s 277ms/step - loss: 0.8951 - acc: 0.6910 - val_loss: 0.7814 - val_acc: 0.7369\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 407s 260ms/step - loss: 0.8939 - acc: 0.6902 - val_loss: 0.8131 - val_acc: 0.7308\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 409s 261ms/step - loss: 0.8792 - acc: 0.6985 - val_loss: 0.8061 - val_acc: 0.7240\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 433s 277ms/step - loss: 0.8784 - acc: 0.6974 - val_loss: 0.7734 - val_acc: 0.7364\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 415s 265ms/step - loss: 0.8676 - acc: 0.7018 - val_loss: 0.7984 - val_acc: 0.7300\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 432s 277ms/step - loss: 0.8682 - acc: 0.7026 - val_loss: 0.7921 - val_acc: 0.7275\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 421s 270ms/step - loss: 0.8584 - acc: 0.7050 - val_loss: 0.7554 - val_acc: 0.7431\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - 373s 238ms/step - loss: 0.8597 - acc: 0.7066 - val_loss: 0.7414 - val_acc: 0.7569\n",
      "Epoch 31/100\n",
      "1563/1563 [==============================] - 282s 181ms/step - loss: 0.8538 - acc: 0.7084 - val_loss: 0.7354 - val_acc: 0.7490\n",
      "Epoch 32/100\n",
      "1563/1563 [==============================] - 218s 140ms/step - loss: 0.8469 - acc: 0.7099 - val_loss: 0.7156 - val_acc: 0.7588\n",
      "Epoch 33/100\n",
      "1563/1563 [==============================] - 222s 142ms/step - loss: 0.8446 - acc: 0.7134 - val_loss: 0.7397 - val_acc: 0.7499\n",
      "Epoch 34/100\n",
      "1563/1563 [==============================] - 226s 144ms/step - loss: 0.8367 - acc: 0.7134 - val_loss: 0.7262 - val_acc: 0.7569\n",
      "Epoch 35/100\n",
      "1563/1563 [==============================] - 235s 150ms/step - loss: 0.8325 - acc: 0.7168 - val_loss: 0.7430 - val_acc: 0.7542\n",
      "Epoch 36/100\n",
      "1563/1563 [==============================] - 214s 137ms/step - loss: 0.8376 - acc: 0.7151 - val_loss: 0.7415 - val_acc: 0.7518\n",
      "Epoch 37/100\n",
      "1563/1563 [==============================] - 231s 148ms/step - loss: 0.8308 - acc: 0.7161 - val_loss: 0.7132 - val_acc: 0.7581\n",
      "Epoch 38/100\n",
      "1563/1563 [==============================] - 216s 138ms/step - loss: 0.8286 - acc: 0.7175 - val_loss: 0.7301 - val_acc: 0.7551\n",
      "Epoch 39/100\n",
      "1563/1563 [==============================] - 223s 142ms/step - loss: 0.8227 - acc: 0.7214 - val_loss: 0.7652 - val_acc: 0.7476\n",
      "Epoch 40/100\n",
      "1563/1563 [==============================] - 211s 135ms/step - loss: 0.8206 - acc: 0.7215 - val_loss: 0.7565 - val_acc: 0.7480\n",
      "Epoch 41/100\n",
      "1563/1563 [==============================] - 232s 148ms/step - loss: 0.8193 - acc: 0.7208 - val_loss: 0.7220 - val_acc: 0.7605\n",
      "Epoch 42/100\n",
      "1563/1563 [==============================] - 268s 171ms/step - loss: 0.8158 - acc: 0.7253 - val_loss: 0.6863 - val_acc: 0.7733\n",
      "Epoch 43/100\n",
      "1563/1563 [==============================] - 260s 167ms/step - loss: 0.8113 - acc: 0.7267 - val_loss: 0.7839 - val_acc: 0.7416\n",
      "Epoch 44/100\n",
      "1563/1563 [==============================] - 230s 147ms/step - loss: 0.8165 - acc: 0.7242 - val_loss: 0.7091 - val_acc: 0.7749\n",
      "Epoch 45/100\n",
      "1563/1563 [==============================] - 243s 156ms/step - loss: 0.8145 - acc: 0.7234 - val_loss: 0.7398 - val_acc: 0.7605\n",
      "Epoch 46/100\n",
      "1563/1563 [==============================] - 238s 152ms/step - loss: 0.8058 - acc: 0.7267 - val_loss: 0.7363 - val_acc: 0.7586\n",
      "Epoch 47/100\n",
      "1563/1563 [==============================] - 250s 160ms/step - loss: 0.8068 - acc: 0.7275 - val_loss: 0.7774 - val_acc: 0.7619\n",
      "Epoch 48/100\n",
      "1563/1563 [==============================] - 261s 167ms/step - loss: 0.8079 - acc: 0.7272 - val_loss: 0.7151 - val_acc: 0.7592\n",
      "Epoch 49/100\n",
      "1563/1563 [==============================] - 745s 477ms/step - loss: 0.8072 - acc: 0.7269 - val_loss: 0.6795 - val_acc: 0.7797\n",
      "Epoch 50/100\n",
      "1563/1563 [==============================] - 976s 625ms/step - loss: 0.8051 - acc: 0.7274 - val_loss: 0.6860 - val_acc: 0.7715\n",
      "Epoch 51/100\n",
      "1563/1563 [==============================] - 3608s 2s/step - loss: 0.8039 - acc: 0.7280 - val_loss: 0.7088 - val_acc: 0.7695\n",
      "Epoch 52/100\n",
      "1563/1563 [==============================] - 257s 164ms/step - loss: 0.7993 - acc: 0.7308 - val_loss: 0.7673 - val_acc: 0.7502\n",
      "Epoch 53/100\n",
      "1563/1563 [==============================] - 279s 179ms/step - loss: 0.8002 - acc: 0.7314 - val_loss: 0.7303 - val_acc: 0.7687\n",
      "Epoch 54/100\n",
      "1563/1563 [==============================] - 293s 188ms/step - loss: 0.7983 - acc: 0.7304 - val_loss: 0.7097 - val_acc: 0.7675\n",
      "Epoch 55/100\n",
      "1563/1563 [==============================] - 307s 197ms/step - loss: 0.7912 - acc: 0.7346 - val_loss: 0.7598 - val_acc: 0.7494\n",
      "Epoch 56/100\n",
      "1563/1563 [==============================] - 234s 150ms/step - loss: 0.8012 - acc: 0.7298 - val_loss: 0.7300 - val_acc: 0.7599\n",
      "Epoch 57/100\n",
      "1563/1563 [==============================] - 235s 151ms/step - loss: 0.8019 - acc: 0.7302 - val_loss: 0.7278 - val_acc: 0.7653\n",
      "Epoch 58/100\n",
      "1563/1563 [==============================] - 232s 148ms/step - loss: 0.8010 - acc: 0.7301 - val_loss: 0.7310 - val_acc: 0.7610\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 246s 158ms/step - loss: 0.7968 - acc: 0.7329 - val_loss: 0.7888 - val_acc: 0.7361\n",
      "Epoch 60/100\n",
      "1563/1563 [==============================] - 255s 163ms/step - loss: 0.7984 - acc: 0.7317 - val_loss: 0.7397 - val_acc: 0.7614\n",
      "Epoch 61/100\n",
      "1563/1563 [==============================] - 253s 162ms/step - loss: 0.7898 - acc: 0.7334 - val_loss: 0.6777 - val_acc: 0.7791\n",
      "Epoch 62/100\n",
      "1563/1563 [==============================] - 261s 167ms/step - loss: 0.7980 - acc: 0.7318 - val_loss: 0.6800 - val_acc: 0.7769\n",
      "Epoch 63/100\n",
      "1563/1563 [==============================] - 251s 161ms/step - loss: 0.7923 - acc: 0.7358 - val_loss: 0.6914 - val_acc: 0.7689\n",
      "Epoch 64/100\n",
      "1563/1563 [==============================] - 266s 170ms/step - loss: 0.7947 - acc: 0.7318 - val_loss: 0.7679 - val_acc: 0.7570\n",
      "Epoch 65/100\n",
      "1563/1563 [==============================] - 275s 176ms/step - loss: 0.7958 - acc: 0.7337 - val_loss: 0.7599 - val_acc: 0.7526\n",
      "Epoch 66/100\n",
      "1563/1563 [==============================] - 275s 176ms/step - loss: 0.7948 - acc: 0.7335 - val_loss: 0.7365 - val_acc: 0.7661\n",
      "Epoch 67/100\n",
      "1563/1563 [==============================] - 268s 171ms/step - loss: 0.7985 - acc: 0.7322 - val_loss: 0.7705 - val_acc: 0.7650\n",
      "Epoch 68/100\n",
      "1563/1563 [==============================] - 264s 169ms/step - loss: 0.7975 - acc: 0.7347 - val_loss: 0.7964 - val_acc: 0.7325\n",
      "Epoch 69/100\n",
      "1563/1563 [==============================] - 267s 171ms/step - loss: 0.7997 - acc: 0.7344 - val_loss: 0.8128 - val_acc: 0.7383\n",
      "Epoch 70/100\n",
      "1563/1563 [==============================] - 273s 175ms/step - loss: 0.7946 - acc: 0.7340 - val_loss: 0.7191 - val_acc: 0.7649\n",
      "Epoch 71/100\n",
      "1563/1563 [==============================] - 287s 183ms/step - loss: 0.8020 - acc: 0.7332 - val_loss: 0.7568 - val_acc: 0.7560\n",
      "Epoch 72/100\n",
      "1563/1563 [==============================] - 279s 178ms/step - loss: 0.7991 - acc: 0.7341 - val_loss: 0.7609 - val_acc: 0.7575\n",
      "Epoch 73/100\n",
      "1563/1563 [==============================] - 215s 137ms/step - loss: 0.7964 - acc: 0.7326 - val_loss: 0.7763 - val_acc: 0.7477\n",
      "Epoch 74/100\n",
      "1563/1563 [==============================] - 242s 155ms/step - loss: 0.8008 - acc: 0.7337 - val_loss: 0.6761 - val_acc: 0.7836\n",
      "Epoch 75/100\n",
      "1563/1563 [==============================] - 276s 176ms/step - loss: 0.8017 - acc: 0.7325 - val_loss: 0.7829 - val_acc: 0.7378\n",
      "Epoch 76/100\n",
      "1563/1563 [==============================] - 216s 138ms/step - loss: 0.8047 - acc: 0.7342 - val_loss: 0.7494 - val_acc: 0.7643\n",
      "Epoch 77/100\n",
      "1563/1563 [==============================] - 246s 157ms/step - loss: 0.8019 - acc: 0.7327 - val_loss: 0.7195 - val_acc: 0.7626\n",
      "Epoch 78/100\n",
      "1563/1563 [==============================] - 280s 179ms/step - loss: 0.8073 - acc: 0.7332 - val_loss: 0.7357 - val_acc: 0.7594\n",
      "Epoch 79/100\n",
      "1563/1563 [==============================] - 323s 207ms/step - loss: 0.8066 - acc: 0.7341 - val_loss: 0.7702 - val_acc: 0.7447\n",
      "Epoch 80/100\n",
      "1563/1563 [==============================] - 290s 185ms/step - loss: 0.8015 - acc: 0.7316 - val_loss: 0.8046 - val_acc: 0.7319\n",
      "Epoch 81/100\n",
      "1563/1563 [==============================] - 271s 174ms/step - loss: 0.8121 - acc: 0.7314 - val_loss: 0.7098 - val_acc: 0.7676\n",
      "Epoch 82/100\n",
      "1563/1563 [==============================] - 311s 199ms/step - loss: 0.8101 - acc: 0.7309 - val_loss: 0.7053 - val_acc: 0.7697\n",
      "Epoch 83/100\n",
      "1563/1563 [==============================] - 257s 165ms/step - loss: 0.8098 - acc: 0.7313 - val_loss: 0.7089 - val_acc: 0.7707\n",
      "Epoch 84/100\n",
      "1563/1563 [==============================] - 281s 180ms/step - loss: 0.8145 - acc: 0.7305 - val_loss: 0.7027 - val_acc: 0.7699\n",
      "Epoch 85/100\n",
      "1563/1563 [==============================] - 282s 180ms/step - loss: 0.8188 - acc: 0.7297 - val_loss: 0.7936 - val_acc: 0.7336\n",
      "Epoch 86/100\n",
      "1563/1563 [==============================] - 248s 158ms/step - loss: 0.8108 - acc: 0.7324 - val_loss: 0.7580 - val_acc: 0.7604\n",
      "Epoch 87/100\n",
      "1563/1563 [==============================] - 208s 133ms/step - loss: 0.8151 - acc: 0.7302 - val_loss: 0.7254 - val_acc: 0.7663\n",
      "Epoch 88/100\n",
      "1563/1563 [==============================] - 215s 137ms/step - loss: 0.8146 - acc: 0.7293 - val_loss: 0.7517 - val_acc: 0.7581\n",
      "Epoch 89/100\n",
      "1563/1563 [==============================] - 246s 158ms/step - loss: 0.8177 - acc: 0.7313 - val_loss: 0.6880 - val_acc: 0.7797\n",
      "Epoch 90/100\n",
      "1563/1563 [==============================] - 214s 137ms/step - loss: 0.8187 - acc: 0.7289 - val_loss: 0.6887 - val_acc: 0.7757\n",
      "Epoch 91/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.8213 - acc: 0.7287 - val_loss: 0.7380 - val_acc: 0.7552\n",
      "Epoch 92/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.8226 - acc: 0.7265 - val_loss: 0.7468 - val_acc: 0.7660\n",
      "Epoch 93/100\n",
      "1563/1563 [==============================] - 214s 137ms/step - loss: 0.8217 - acc: 0.7291 - val_loss: 0.7168 - val_acc: 0.7676\n",
      "Epoch 94/100\n",
      "1563/1563 [==============================] - 213s 136ms/step - loss: 0.8301 - acc: 0.7271 - val_loss: 0.7314 - val_acc: 0.7606\n",
      "Epoch 95/100\n",
      "1563/1563 [==============================] - 215s 138ms/step - loss: 0.8251 - acc: 0.7301 - val_loss: 0.7041 - val_acc: 0.7668\n",
      "Epoch 96/100\n",
      "1563/1563 [==============================] - 206s 132ms/step - loss: 0.8351 - acc: 0.7260 - val_loss: 0.7559 - val_acc: 0.7597\n",
      "Epoch 97/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.8295 - acc: 0.7281 - val_loss: 0.6909 - val_acc: 0.7779\n",
      "Epoch 98/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.8389 - acc: 0.7243 - val_loss: 0.8510 - val_acc: 0.7237\n",
      "Epoch 99/100\n",
      "1563/1563 [==============================] - 204s 131ms/step - loss: 0.8346 - acc: 0.7261 - val_loss: 0.7190 - val_acc: 0.7649\n",
      "Epoch 100/100\n",
      "1563/1563 [==============================] - 208s 133ms/step - loss: 0.8413 - acc: 0.7246 - val_loss: 0.8067 - val_acc: 0.7382\n",
      "Saved trained model at /Users/lvziqing/Desktop/INFO7374_NeuralNets/saved_models/keras_cifar10_trained_model.h5 \n",
      "10000/10000 [==============================] - 11s 1ms/step\n",
      "Test loss: 0.8066593133926392\n",
      "Test accuracy: 0.7382\n"
     ]
    }
   ],
   "source": [
    "# hyper parameters to be tuned\n",
    "n_neuron = 256\n",
    "batch_size = 32\n",
    "dropout_rate = 0.2\n",
    "data_augmentation = True\n",
    "epochs = 100\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# set up model structure\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "    \n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    " \n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adam - LR = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 214s 137ms/step - loss: 1.8261 - acc: 0.3256 - val_loss: 1.5104 - val_acc: 0.4460\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 225s 144ms/step - loss: 1.5427 - acc: 0.4360 - val_loss: 1.3955 - val_acc: 0.4963\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 218s 139ms/step - loss: 1.4372 - acc: 0.4768 - val_loss: 1.2579 - val_acc: 0.5469\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 224s 143ms/step - loss: 1.3584 - acc: 0.5105 - val_loss: 1.1869 - val_acc: 0.5759\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 218s 139ms/step - loss: 1.2845 - acc: 0.5399 - val_loss: 1.1670 - val_acc: 0.5815\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 218s 140ms/step - loss: 1.2347 - acc: 0.5615 - val_loss: 1.0591 - val_acc: 0.6219\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 216s 138ms/step - loss: 1.1781 - acc: 0.5806 - val_loss: 1.0431 - val_acc: 0.6308\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 217s 139ms/step - loss: 1.1374 - acc: 0.5977 - val_loss: 0.9660 - val_acc: 0.6597\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 217s 139ms/step - loss: 1.1052 - acc: 0.6102 - val_loss: 0.9482 - val_acc: 0.6671\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 217s 139ms/step - loss: 1.0662 - acc: 0.6218 - val_loss: 0.9294 - val_acc: 0.6663\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 215s 138ms/step - loss: 1.0427 - acc: 0.6318 - val_loss: 0.8872 - val_acc: 0.6855\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 214s 137ms/step - loss: 1.0165 - acc: 0.6418 - val_loss: 0.8585 - val_acc: 0.7008\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 219s 140ms/step - loss: 0.9909 - acc: 0.6533 - val_loss: 0.9517 - val_acc: 0.6631\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 216s 138ms/step - loss: 0.9684 - acc: 0.6577 - val_loss: 0.8269 - val_acc: 0.7101\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 216s 138ms/step - loss: 0.9470 - acc: 0.6686 - val_loss: 0.8387 - val_acc: 0.7003\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 217s 139ms/step - loss: 0.9257 - acc: 0.6756 - val_loss: 0.8079 - val_acc: 0.7130\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 219s 140ms/step - loss: 0.9106 - acc: 0.6768 - val_loss: 0.8071 - val_acc: 0.7184\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 217s 139ms/step - loss: 0.8898 - acc: 0.6897 - val_loss: 0.7727 - val_acc: 0.7276\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 214s 137ms/step - loss: 0.8728 - acc: 0.6944 - val_loss: 0.7899 - val_acc: 0.7229\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 214s 137ms/step - loss: 0.8637 - acc: 0.7005 - val_loss: 0.8217 - val_acc: 0.7120\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 215s 138ms/step - loss: 0.8480 - acc: 0.7040 - val_loss: 0.7547 - val_acc: 0.7356\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 221s 141ms/step - loss: 0.8326 - acc: 0.7093 - val_loss: 0.7377 - val_acc: 0.7389\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 221s 141ms/step - loss: 0.8206 - acc: 0.7135 - val_loss: 0.7178 - val_acc: 0.7438\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 221s 141ms/step - loss: 0.8167 - acc: 0.7162 - val_loss: 0.6892 - val_acc: 0.7574\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 219s 140ms/step - loss: 0.8000 - acc: 0.7208 - val_loss: 0.6867 - val_acc: 0.7553\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 223s 143ms/step - loss: 0.7882 - acc: 0.7231 - val_loss: 0.6908 - val_acc: 0.7555\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 218s 140ms/step - loss: 0.7776 - acc: 0.7276 - val_loss: 0.7150 - val_acc: 0.7480\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 219s 140ms/step - loss: 0.7631 - acc: 0.7336 - val_loss: 0.6774 - val_acc: 0.7587\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 216s 138ms/step - loss: 0.7563 - acc: 0.7355 - val_loss: 0.6751 - val_acc: 0.7611\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - 216s 138ms/step - loss: 0.7502 - acc: 0.7376 - val_loss: 0.6512 - val_acc: 0.7677\n",
      "Epoch 31/100\n",
      "1563/1563 [==============================] - 219s 140ms/step - loss: 0.7405 - acc: 0.7418 - val_loss: 0.6455 - val_acc: 0.7738\n",
      "Epoch 32/100\n",
      "1563/1563 [==============================] - 213s 136ms/step - loss: 0.7341 - acc: 0.7422 - val_loss: 0.6444 - val_acc: 0.7738\n",
      "Epoch 33/100\n",
      "1563/1563 [==============================] - 218s 140ms/step - loss: 0.7254 - acc: 0.7456 - val_loss: 0.6382 - val_acc: 0.7757\n",
      "Epoch 34/100\n",
      "1563/1563 [==============================] - 218s 139ms/step - loss: 0.7197 - acc: 0.7502 - val_loss: 0.6294 - val_acc: 0.7806\n",
      "Epoch 35/100\n",
      "1563/1563 [==============================] - 222s 142ms/step - loss: 0.7126 - acc: 0.7507 - val_loss: 0.6445 - val_acc: 0.7709\n",
      "Epoch 36/100\n",
      "1563/1563 [==============================] - 220s 141ms/step - loss: 0.7088 - acc: 0.7526 - val_loss: 0.6490 - val_acc: 0.7695\n",
      "Epoch 37/100\n",
      "1563/1563 [==============================] - 218s 139ms/step - loss: 0.6991 - acc: 0.7542 - val_loss: 0.6226 - val_acc: 0.7806\n",
      "Epoch 38/100\n",
      "1563/1563 [==============================] - 217s 139ms/step - loss: 0.6907 - acc: 0.7563 - val_loss: 0.6158 - val_acc: 0.7837\n",
      "Epoch 39/100\n",
      "1563/1563 [==============================] - 217s 139ms/step - loss: 0.6892 - acc: 0.7599 - val_loss: 0.6001 - val_acc: 0.7862\n",
      "Epoch 40/100\n",
      "1563/1563 [==============================] - 216s 138ms/step - loss: 0.6803 - acc: 0.7638 - val_loss: 0.6025 - val_acc: 0.7917\n",
      "Epoch 41/100\n",
      "1563/1563 [==============================] - 215s 137ms/step - loss: 0.6680 - acc: 0.7663 - val_loss: 0.5994 - val_acc: 0.7901\n",
      "Epoch 42/100\n",
      "1563/1563 [==============================] - 213s 136ms/step - loss: 0.6707 - acc: 0.7653 - val_loss: 0.5871 - val_acc: 0.7923\n",
      "Epoch 43/100\n",
      "1563/1563 [==============================] - 214s 137ms/step - loss: 0.6660 - acc: 0.7670 - val_loss: 0.6240 - val_acc: 0.7805\n",
      "Epoch 44/100\n",
      "1563/1563 [==============================] - 221s 142ms/step - loss: 0.6616 - acc: 0.7670 - val_loss: 0.5796 - val_acc: 0.7975\n",
      "Epoch 45/100\n",
      "1563/1563 [==============================] - 215s 138ms/step - loss: 0.6522 - acc: 0.7732 - val_loss: 0.5979 - val_acc: 0.7905\n",
      "Epoch 46/100\n",
      "1563/1563 [==============================] - 211s 135ms/step - loss: 0.6491 - acc: 0.7738 - val_loss: 0.5703 - val_acc: 0.7996\n",
      "Epoch 47/100\n",
      "1563/1563 [==============================] - 215s 137ms/step - loss: 0.6429 - acc: 0.7757 - val_loss: 0.5521 - val_acc: 0.8077\n",
      "Epoch 48/100\n",
      "1563/1563 [==============================] - 216s 138ms/step - loss: 0.6376 - acc: 0.7776 - val_loss: 0.5492 - val_acc: 0.8085\n",
      "Epoch 49/100\n",
      "1563/1563 [==============================] - 221s 141ms/step - loss: 0.6342 - acc: 0.7779 - val_loss: 0.5730 - val_acc: 0.7992\n",
      "Epoch 50/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.6319 - acc: 0.7799 - val_loss: 0.5898 - val_acc: 0.7936\n",
      "Epoch 51/100\n",
      "1563/1563 [==============================] - 216s 138ms/step - loss: 0.6276 - acc: 0.7798 - val_loss: 0.5848 - val_acc: 0.7973\n",
      "Epoch 52/100\n",
      "1563/1563 [==============================] - 227s 146ms/step - loss: 0.6239 - acc: 0.7800 - val_loss: 0.5627 - val_acc: 0.8029\n",
      "Epoch 53/100\n",
      "1563/1563 [==============================] - 217s 139ms/step - loss: 0.6164 - acc: 0.7840 - val_loss: 0.5493 - val_acc: 0.8082\n",
      "Epoch 54/100\n",
      "1563/1563 [==============================] - 217s 139ms/step - loss: 0.6128 - acc: 0.7868 - val_loss: 0.5833 - val_acc: 0.7965\n",
      "Epoch 55/100\n",
      "1563/1563 [==============================] - 224s 143ms/step - loss: 0.6138 - acc: 0.7867 - val_loss: 0.5566 - val_acc: 0.8064\n",
      "Epoch 56/100\n",
      "1563/1563 [==============================] - 231s 148ms/step - loss: 0.6014 - acc: 0.7895 - val_loss: 0.5299 - val_acc: 0.8193\n",
      "Epoch 57/100\n",
      "1563/1563 [==============================] - 231s 148ms/step - loss: 0.6006 - acc: 0.7900 - val_loss: 0.5434 - val_acc: 0.8157\n",
      "Epoch 58/100\n",
      "1563/1563 [==============================] - 229s 147ms/step - loss: 0.5959 - acc: 0.7914 - val_loss: 0.5454 - val_acc: 0.8104\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 211s 135ms/step - loss: 0.5916 - acc: 0.7904 - val_loss: 0.5454 - val_acc: 0.8128\n",
      "Epoch 60/100\n",
      "1563/1563 [==============================] - 230s 147ms/step - loss: 0.5885 - acc: 0.7943 - val_loss: 0.5365 - val_acc: 0.8163\n",
      "Epoch 61/100\n",
      "1563/1563 [==============================] - 230s 147ms/step - loss: 0.5864 - acc: 0.7951 - val_loss: 0.5394 - val_acc: 0.8144\n",
      "Epoch 62/100\n",
      "1563/1563 [==============================] - 219s 140ms/step - loss: 0.5851 - acc: 0.7961 - val_loss: 0.5460 - val_acc: 0.8116\n",
      "Epoch 63/100\n",
      "1563/1563 [==============================] - 220s 141ms/step - loss: 0.5780 - acc: 0.7991 - val_loss: 0.5310 - val_acc: 0.8177\n",
      "Epoch 64/100\n",
      "1563/1563 [==============================] - 225s 144ms/step - loss: 0.5762 - acc: 0.7978 - val_loss: 0.5199 - val_acc: 0.8206\n",
      "Epoch 65/100\n",
      "1563/1563 [==============================] - 214s 137ms/step - loss: 0.5693 - acc: 0.8011 - val_loss: 0.5313 - val_acc: 0.8173\n",
      "Epoch 66/100\n",
      "1563/1563 [==============================] - 220s 141ms/step - loss: 0.5730 - acc: 0.7995 - val_loss: 0.5281 - val_acc: 0.8203\n",
      "Epoch 67/100\n",
      "1563/1563 [==============================] - 214s 137ms/step - loss: 0.5631 - acc: 0.8036 - val_loss: 0.5347 - val_acc: 0.8182\n",
      "Epoch 68/100\n",
      "1563/1563 [==============================] - 217s 139ms/step - loss: 0.5574 - acc: 0.8055 - val_loss: 0.5301 - val_acc: 0.8173\n",
      "Epoch 69/100\n",
      "1563/1563 [==============================] - 215s 138ms/step - loss: 0.5595 - acc: 0.8040 - val_loss: 0.5019 - val_acc: 0.8275\n",
      "Epoch 70/100\n",
      "1563/1563 [==============================] - 226s 145ms/step - loss: 0.5644 - acc: 0.8054 - val_loss: 0.5193 - val_acc: 0.8222\n",
      "Epoch 71/100\n",
      "1563/1563 [==============================] - 217s 139ms/step - loss: 0.5568 - acc: 0.8058 - val_loss: 0.5215 - val_acc: 0.8213\n",
      "Epoch 72/100\n",
      "1563/1563 [==============================] - 219s 140ms/step - loss: 0.5551 - acc: 0.8054 - val_loss: 0.5362 - val_acc: 0.8146\n",
      "Epoch 73/100\n",
      "1563/1563 [==============================] - 215s 137ms/step - loss: 0.5492 - acc: 0.8088 - val_loss: 0.5200 - val_acc: 0.8264\n",
      "Epoch 74/100\n",
      "1563/1563 [==============================] - 226s 144ms/step - loss: 0.5488 - acc: 0.8081 - val_loss: 0.4951 - val_acc: 0.8316\n",
      "Epoch 75/100\n",
      "1563/1563 [==============================] - 216s 138ms/step - loss: 0.5438 - acc: 0.8099 - val_loss: 0.4862 - val_acc: 0.8366\n",
      "Epoch 76/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.5432 - acc: 0.8099 - val_loss: 0.4935 - val_acc: 0.8303\n",
      "Epoch 77/100\n",
      "1563/1563 [==============================] - 220s 141ms/step - loss: 0.5445 - acc: 0.8103 - val_loss: 0.5071 - val_acc: 0.8272\n",
      "Epoch 78/100\n",
      "1563/1563 [==============================] - 216s 138ms/step - loss: 0.5396 - acc: 0.8095 - val_loss: 0.4931 - val_acc: 0.8316\n",
      "Epoch 79/100\n",
      "1563/1563 [==============================] - 214s 137ms/step - loss: 0.5399 - acc: 0.8115 - val_loss: 0.5134 - val_acc: 0.8281\n",
      "Epoch 80/100\n",
      "1563/1563 [==============================] - 216s 138ms/step - loss: 0.5348 - acc: 0.8140 - val_loss: 0.5135 - val_acc: 0.8248\n",
      "Epoch 81/100\n",
      "1563/1563 [==============================] - 3346s 2s/step - loss: 0.5335 - acc: 0.8146 - val_loss: 0.4799 - val_acc: 0.8385\n",
      "Epoch 82/100\n",
      "1563/1563 [==============================] - 1665s 1s/step - loss: 0.5280 - acc: 0.8142 - val_loss: 0.5183 - val_acc: 0.8229\n",
      "Epoch 83/100\n",
      "1563/1563 [==============================] - 250s 160ms/step - loss: 0.5241 - acc: 0.8171 - val_loss: 0.4946 - val_acc: 0.8344\n",
      "Epoch 84/100\n",
      "1563/1563 [==============================] - 248s 159ms/step - loss: 0.5269 - acc: 0.8161 - val_loss: 0.5068 - val_acc: 0.8268\n",
      "Epoch 85/100\n",
      "1563/1563 [==============================] - 240s 154ms/step - loss: 0.5160 - acc: 0.8193 - val_loss: 0.4915 - val_acc: 0.8312\n",
      "Epoch 86/100\n",
      "1563/1563 [==============================] - 245s 156ms/step - loss: 0.5177 - acc: 0.8189 - val_loss: 0.5012 - val_acc: 0.8303\n",
      "Epoch 87/100\n",
      "1563/1563 [==============================] - 244s 156ms/step - loss: 0.5148 - acc: 0.8192 - val_loss: 0.4750 - val_acc: 0.8385\n",
      "Epoch 88/100\n",
      "1563/1563 [==============================] - 247s 158ms/step - loss: 0.5199 - acc: 0.8180 - val_loss: 0.4872 - val_acc: 0.8361\n",
      "Epoch 89/100\n",
      "1563/1563 [==============================] - 243s 156ms/step - loss: 0.5154 - acc: 0.8215 - val_loss: 0.5101 - val_acc: 0.8268\n",
      "Epoch 90/100\n",
      "1563/1563 [==============================] - 237s 151ms/step - loss: 0.5111 - acc: 0.8221 - val_loss: 0.5131 - val_acc: 0.8241\n",
      "Epoch 91/100\n",
      "1563/1563 [==============================] - 239s 153ms/step - loss: 0.5088 - acc: 0.8189 - val_loss: 0.5046 - val_acc: 0.8262\n",
      "Epoch 92/100\n",
      "1563/1563 [==============================] - 236s 151ms/step - loss: 0.5068 - acc: 0.8224 - val_loss: 0.4658 - val_acc: 0.8446\n",
      "Epoch 93/100\n",
      "1563/1563 [==============================] - 234s 150ms/step - loss: 0.5101 - acc: 0.8219 - val_loss: 0.4847 - val_acc: 0.8336\n",
      "Epoch 94/100\n",
      "1563/1563 [==============================] - 232s 149ms/step - loss: 0.5030 - acc: 0.8247 - val_loss: 0.5079 - val_acc: 0.8276\n",
      "Epoch 95/100\n",
      "1563/1563 [==============================] - 235s 150ms/step - loss: 0.4992 - acc: 0.8248 - val_loss: 0.4920 - val_acc: 0.8335\n",
      "Epoch 96/100\n",
      "1563/1563 [==============================] - 230s 147ms/step - loss: 0.4970 - acc: 0.8269 - val_loss: 0.4768 - val_acc: 0.8360\n",
      "Epoch 97/100\n",
      "1563/1563 [==============================] - 235s 150ms/step - loss: 0.4986 - acc: 0.8250 - val_loss: 0.4931 - val_acc: 0.8304\n",
      "Epoch 98/100\n",
      "1563/1563 [==============================] - 3371s 2s/step - loss: 0.4965 - acc: 0.8260 - val_loss: 0.4807 - val_acc: 0.8361\n",
      "Epoch 99/100\n",
      "1563/1563 [==============================] - 238s 152ms/step - loss: 0.4957 - acc: 0.8263 - val_loss: 0.4761 - val_acc: 0.8381\n",
      "Epoch 100/100\n",
      "1563/1563 [==============================] - 236s 151ms/step - loss: 0.4889 - acc: 0.8283 - val_loss: 0.4856 - val_acc: 0.8356\n",
      "Saved trained model at /Users/lvziqing/Desktop/INFO7374_NeuralNets/saved_models_2/keras_cifar10_trained_model_2.h5 \n",
      "10000/10000 [==============================] - 15s 1ms/step\n",
      "Test loss: 0.48560277504920957\n",
      "Test accuracy: 0.8356\n"
     ]
    }
   ],
   "source": [
    "# hyper parameters to be tuned\n",
    "n_neuron = 256\n",
    "batch_size = 32\n",
    "dropout_rate = 0.2\n",
    "data_augmentation = True\n",
    "epochs = 100\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models_2')\n",
    "model_name = 'keras_cifar10_trained_model_2.h5'\n",
    "\n",
    "# set up model structure\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "  \n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    " \n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
