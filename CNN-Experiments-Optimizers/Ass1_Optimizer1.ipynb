{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ass1_Optimizer1",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ziqlu0722/CNN-Classifier/blob/master/Ass1_Optimizer1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "vZ_ARCAL8DlR",
        "colab_type": "code",
        "outputId": "5c4ee51c-be5a-46b3-d31f-e4a02d34ae40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "uRo79n9M8KCN",
        "colab_type": "code",
        "outputId": "1f331ce7-da21-42e4-e27d-272bbbb0b9d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 100\n",
        "data_augmentation = True\n",
        "num_predictions = 20\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'keras_cifar10_trained_model.h5'\n",
        "\n",
        "# The data, split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 103s 1us/step\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bOZEkloP8Twm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rNlMzm2T8fK0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "opt = keras.optimizers.Adadelta(lr=0.0001, decay=1e-6)\n",
        "\n",
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8PgLq0vB8jMs",
        "colab_type": "code",
        "outputId": "fd6095b4-988b-41e0-8d16-b897ffedcab1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        # randomly shift images horizontally (fraction of total width)\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically (fraction of total height)\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.,  # set range for random shear\n",
        "        zoom_range=0.,  # set range for random zoom\n",
        "        channel_shift_range=0.,  # set range for random channel shifts\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        cval=0.,  # value used for fill_mode = \"constant\"\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False,  # randomly flip images\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pTYx31gV8nVz",
        "colab_type": "code",
        "outputId": "19e9f536-f422-4105-f9da-a793e00eafed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4811
        }
      },
      "cell_type": "code",
      "source": [
        "datagen.fit(x_train)\n",
        "\n",
        "# fits the model on batches with real-time data augmentation:\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    steps_per_epoch=len(x_train) / 32, epochs=100,validation_data=(x_test, y_test))\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1563/1562 [==============================] - 77s 49ms/step - loss: 2.2632 - acc: 0.1481 - val_loss: 2.2584 - val_acc: 0.1678\n",
            "Epoch 2/100\n",
            "1563/1562 [==============================] - 78s 50ms/step - loss: 2.2614 - acc: 0.1495 - val_loss: 2.2569 - val_acc: 0.1697\n",
            "Epoch 3/100\n",
            "1563/1562 [==============================] - 78s 50ms/step - loss: 2.2603 - acc: 0.1516 - val_loss: 2.2554 - val_acc: 0.1723\n",
            "Epoch 4/100\n",
            "1563/1562 [==============================] - 79s 50ms/step - loss: 2.2589 - acc: 0.1501 - val_loss: 2.2538 - val_acc: 0.1738\n",
            "Epoch 5/100\n",
            "1563/1562 [==============================] - 78s 50ms/step - loss: 2.2573 - acc: 0.1525 - val_loss: 2.2522 - val_acc: 0.1766\n",
            "Epoch 6/100\n",
            "1563/1562 [==============================] - 78s 50ms/step - loss: 2.2562 - acc: 0.1547 - val_loss: 2.2505 - val_acc: 0.1802\n",
            "Epoch 7/100\n",
            "1563/1562 [==============================] - 78s 50ms/step - loss: 2.2539 - acc: 0.1550 - val_loss: 2.2487 - val_acc: 0.1819\n",
            "Epoch 8/100\n",
            "1563/1562 [==============================] - 78s 50ms/step - loss: 2.2533 - acc: 0.1557 - val_loss: 2.2470 - val_acc: 0.1858\n",
            "Epoch 9/100\n",
            "1563/1562 [==============================] - 78s 50ms/step - loss: 2.2509 - acc: 0.1577 - val_loss: 2.2451 - val_acc: 0.1890\n",
            "Epoch 10/100\n",
            "1563/1562 [==============================] - 78s 50ms/step - loss: 2.2502 - acc: 0.1561 - val_loss: 2.2432 - val_acc: 0.1925\n",
            "Epoch 11/100\n",
            "1563/1562 [==============================] - 79s 50ms/step - loss: 2.2474 - acc: 0.1623 - val_loss: 2.2413 - val_acc: 0.1959\n",
            "Epoch 12/100\n",
            "1563/1562 [==============================] - 79s 51ms/step - loss: 2.2451 - acc: 0.1618 - val_loss: 2.2393 - val_acc: 0.1989\n",
            "Epoch 13/100\n",
            "1563/1562 [==============================] - 78s 50ms/step - loss: 2.2445 - acc: 0.1650 - val_loss: 2.2373 - val_acc: 0.2029\n",
            "Epoch 14/100\n",
            "1563/1562 [==============================] - 83s 53ms/step - loss: 2.2424 - acc: 0.1647 - val_loss: 2.2351 - val_acc: 0.2058\n",
            "Epoch 15/100\n",
            "1563/1562 [==============================] - 82s 52ms/step - loss: 2.2402 - acc: 0.1644 - val_loss: 2.2330 - val_acc: 0.2081\n",
            "Epoch 16/100\n",
            "1563/1562 [==============================] - 80s 51ms/step - loss: 2.2386 - acc: 0.1658 - val_loss: 2.2308 - val_acc: 0.2096\n",
            "Epoch 17/100\n",
            "1563/1562 [==============================] - 81s 52ms/step - loss: 2.2363 - acc: 0.1687 - val_loss: 2.2285 - val_acc: 0.2131\n",
            "Epoch 18/100\n",
            "1563/1562 [==============================] - 80s 51ms/step - loss: 2.2340 - acc: 0.1697 - val_loss: 2.2262 - val_acc: 0.2153\n",
            "Epoch 19/100\n",
            "1563/1562 [==============================] - 81s 52ms/step - loss: 2.2315 - acc: 0.1710 - val_loss: 2.2237 - val_acc: 0.2191\n",
            "Epoch 20/100\n",
            "1563/1562 [==============================] - 81s 52ms/step - loss: 2.2295 - acc: 0.1728 - val_loss: 2.2212 - val_acc: 0.2205\n",
            "Epoch 21/100\n",
            "1563/1562 [==============================] - 81s 52ms/step - loss: 2.2264 - acc: 0.1717 - val_loss: 2.2187 - val_acc: 0.2246\n",
            "Epoch 22/100\n",
            "1563/1562 [==============================] - 79s 51ms/step - loss: 2.2242 - acc: 0.1758 - val_loss: 2.2161 - val_acc: 0.2258\n",
            "Epoch 23/100\n",
            "1563/1562 [==============================] - 80s 51ms/step - loss: 2.2222 - acc: 0.1784 - val_loss: 2.2134 - val_acc: 0.2275\n",
            "Epoch 24/100\n",
            "1563/1562 [==============================] - 80s 51ms/step - loss: 2.2208 - acc: 0.1773 - val_loss: 2.2107 - val_acc: 0.2304\n",
            "Epoch 25/100\n",
            "1563/1562 [==============================] - 80s 51ms/step - loss: 2.2185 - acc: 0.1777 - val_loss: 2.2079 - val_acc: 0.2324\n",
            "Epoch 26/100\n",
            "1563/1562 [==============================] - 80s 51ms/step - loss: 2.2159 - acc: 0.1796 - val_loss: 2.2050 - val_acc: 0.2352\n",
            "Epoch 27/100\n",
            "1563/1562 [==============================] - 81s 52ms/step - loss: 2.2134 - acc: 0.1809 - val_loss: 2.2022 - val_acc: 0.2380\n",
            "Epoch 28/100\n",
            "1563/1562 [==============================] - 104s 67ms/step - loss: 2.2098 - acc: 0.1809 - val_loss: 2.1991 - val_acc: 0.2387\n",
            "Epoch 29/100\n",
            "1563/1562 [==============================] - 126s 81ms/step - loss: 2.2074 - acc: 0.1848 - val_loss: 2.1962 - val_acc: 0.2412\n",
            "Epoch 30/100\n",
            "1563/1562 [==============================] - 126s 81ms/step - loss: 2.2055 - acc: 0.1846 - val_loss: 2.1931 - val_acc: 0.2440\n",
            "Epoch 31/100\n",
            "1563/1562 [==============================] - 126s 80ms/step - loss: 2.2019 - acc: 0.1876 - val_loss: 2.1899 - val_acc: 0.2458\n",
            "Epoch 32/100\n",
            "1563/1562 [==============================] - 126s 80ms/step - loss: 2.1983 - acc: 0.1874 - val_loss: 2.1867 - val_acc: 0.2484\n",
            "Epoch 33/100\n",
            "1563/1562 [==============================] - 126s 80ms/step - loss: 2.1954 - acc: 0.1900 - val_loss: 2.1834 - val_acc: 0.2497\n",
            "Epoch 34/100\n",
            "1563/1562 [==============================] - 127s 81ms/step - loss: 2.1933 - acc: 0.1894 - val_loss: 2.1801 - val_acc: 0.2501\n",
            "Epoch 35/100\n",
            "1563/1562 [==============================] - 126s 80ms/step - loss: 2.1900 - acc: 0.1908 - val_loss: 2.1769 - val_acc: 0.2513\n",
            "Epoch 36/100\n",
            "1563/1562 [==============================] - 127s 81ms/step - loss: 2.1874 - acc: 0.1915 - val_loss: 2.1736 - val_acc: 0.2523\n",
            "Epoch 37/100\n",
            "1563/1562 [==============================] - 126s 81ms/step - loss: 2.1858 - acc: 0.1885 - val_loss: 2.1704 - val_acc: 0.2541\n",
            "Epoch 38/100\n",
            "1563/1562 [==============================] - 126s 81ms/step - loss: 2.1816 - acc: 0.1934 - val_loss: 2.1671 - val_acc: 0.2547\n",
            "Epoch 39/100\n",
            "1563/1562 [==============================] - 126s 80ms/step - loss: 2.1786 - acc: 0.1948 - val_loss: 2.1637 - val_acc: 0.2551\n",
            "Epoch 40/100\n",
            "1563/1562 [==============================] - 126s 81ms/step - loss: 2.1740 - acc: 0.1994 - val_loss: 2.1603 - val_acc: 0.2567\n",
            "Epoch 41/100\n",
            "1563/1562 [==============================] - 125s 80ms/step - loss: 2.1716 - acc: 0.1938 - val_loss: 2.1568 - val_acc: 0.2574\n",
            "Epoch 42/100\n",
            "1563/1562 [==============================] - 124s 79ms/step - loss: 2.1702 - acc: 0.1967 - val_loss: 2.1535 - val_acc: 0.2590\n",
            "Epoch 43/100\n",
            "1563/1562 [==============================] - 127s 81ms/step - loss: 2.1680 - acc: 0.1991 - val_loss: 2.1502 - val_acc: 0.2592\n",
            "Epoch 44/100\n",
            "1563/1562 [==============================] - 126s 80ms/step - loss: 2.1643 - acc: 0.1989 - val_loss: 2.1469 - val_acc: 0.2605\n",
            "Epoch 45/100\n",
            "1563/1562 [==============================] - 126s 80ms/step - loss: 2.1612 - acc: 0.1992 - val_loss: 2.1435 - val_acc: 0.2620\n",
            "Epoch 46/100\n",
            "1563/1562 [==============================] - 125s 80ms/step - loss: 2.1590 - acc: 0.2002 - val_loss: 2.1402 - val_acc: 0.2632\n",
            "Epoch 47/100\n",
            "1563/1562 [==============================] - 126s 81ms/step - loss: 2.1556 - acc: 0.2047 - val_loss: 2.1369 - val_acc: 0.2640\n",
            "Epoch 48/100\n",
            "1563/1562 [==============================] - 123s 79ms/step - loss: 2.1529 - acc: 0.2042 - val_loss: 2.1338 - val_acc: 0.2653\n",
            "Epoch 49/100\n",
            "1563/1562 [==============================] - 124s 79ms/step - loss: 2.1518 - acc: 0.2011 - val_loss: 2.1306 - val_acc: 0.2657\n",
            "Epoch 50/100\n",
            "1563/1562 [==============================] - 137s 88ms/step - loss: 2.1464 - acc: 0.2046 - val_loss: 2.1273 - val_acc: 0.2660\n",
            "Epoch 51/100\n",
            "1563/1562 [==============================] - 119s 76ms/step - loss: 2.1447 - acc: 0.2071 - val_loss: 2.1242 - val_acc: 0.2670\n",
            "Epoch 52/100\n",
            "1563/1562 [==============================] - 81s 52ms/step - loss: 2.1423 - acc: 0.2030 - val_loss: 2.1210 - val_acc: 0.2662\n",
            "Epoch 53/100\n",
            "1563/1562 [==============================] - 121s 77ms/step - loss: 2.1384 - acc: 0.2046 - val_loss: 2.1178 - val_acc: 0.2667\n",
            "Epoch 54/100\n",
            "1563/1562 [==============================] - 125s 80ms/step - loss: 2.1354 - acc: 0.2098 - val_loss: 2.1148 - val_acc: 0.2676\n",
            "Epoch 55/100\n",
            "1563/1562 [==============================] - 124s 79ms/step - loss: 2.1335 - acc: 0.2087 - val_loss: 2.1117 - val_acc: 0.2687\n",
            "Epoch 56/100\n",
            "1563/1562 [==============================] - 125s 80ms/step - loss: 2.1330 - acc: 0.2076 - val_loss: 2.1088 - val_acc: 0.2692\n",
            "Epoch 57/100\n",
            "1563/1562 [==============================] - 125s 80ms/step - loss: 2.1284 - acc: 0.2120 - val_loss: 2.1060 - val_acc: 0.2686\n",
            "Epoch 58/100\n",
            "1563/1562 [==============================] - 123s 78ms/step - loss: 2.1251 - acc: 0.2117 - val_loss: 2.1031 - val_acc: 0.2689\n",
            "Epoch 59/100\n",
            "1563/1562 [==============================] - 125s 80ms/step - loss: 2.1244 - acc: 0.2100 - val_loss: 2.1003 - val_acc: 0.2681\n",
            "Epoch 60/100\n",
            "1563/1562 [==============================] - 123s 78ms/step - loss: 2.1214 - acc: 0.2121 - val_loss: 2.0975 - val_acc: 0.2682\n",
            "Epoch 61/100\n",
            "1563/1562 [==============================] - 125s 80ms/step - loss: 2.1203 - acc: 0.2096 - val_loss: 2.0949 - val_acc: 0.2686\n",
            "Epoch 62/100\n",
            "1563/1562 [==============================] - 125s 80ms/step - loss: 2.1176 - acc: 0.2124 - val_loss: 2.0923 - val_acc: 0.2691\n",
            "Epoch 63/100\n",
            "1563/1562 [==============================] - 124s 80ms/step - loss: 2.1145 - acc: 0.2142 - val_loss: 2.0897 - val_acc: 0.2702\n",
            "Epoch 64/100\n",
            "1563/1562 [==============================] - 125s 80ms/step - loss: 2.1140 - acc: 0.2091 - val_loss: 2.0873 - val_acc: 0.2715\n",
            "Epoch 65/100\n",
            "1563/1562 [==============================] - 124s 79ms/step - loss: 2.1099 - acc: 0.2119 - val_loss: 2.0848 - val_acc: 0.2720\n",
            "Epoch 66/100\n",
            "1563/1562 [==============================] - 124s 80ms/step - loss: 2.1102 - acc: 0.2121 - val_loss: 2.0824 - val_acc: 0.2729\n",
            "Epoch 67/100\n",
            "1563/1562 [==============================] - 124s 79ms/step - loss: 2.1060 - acc: 0.2154 - val_loss: 2.0800 - val_acc: 0.2735\n",
            "Epoch 68/100\n",
            "1563/1562 [==============================] - 125s 80ms/step - loss: 2.1070 - acc: 0.2138 - val_loss: 2.0778 - val_acc: 0.2731\n",
            "Epoch 69/100\n",
            "1563/1562 [==============================] - 124s 79ms/step - loss: 2.1038 - acc: 0.2133 - val_loss: 2.0756 - val_acc: 0.2737\n",
            "Epoch 70/100\n",
            "1563/1562 [==============================] - 124s 79ms/step - loss: 2.1018 - acc: 0.2169 - val_loss: 2.0735 - val_acc: 0.2743\n",
            "Epoch 71/100\n",
            "1563/1562 [==============================] - 124s 79ms/step - loss: 2.0996 - acc: 0.2166 - val_loss: 2.0715 - val_acc: 0.2748\n",
            "Epoch 72/100\n",
            "1563/1562 [==============================] - 126s 80ms/step - loss: 2.0998 - acc: 0.2152 - val_loss: 2.0693 - val_acc: 0.2751\n",
            "Epoch 73/100\n",
            "1563/1562 [==============================] - 124s 79ms/step - loss: 2.0979 - acc: 0.2159 - val_loss: 2.0673 - val_acc: 0.2751\n",
            "Epoch 74/100\n",
            "1563/1562 [==============================] - 126s 81ms/step - loss: 2.0959 - acc: 0.2176 - val_loss: 2.0654 - val_acc: 0.2744\n",
            "Epoch 75/100\n",
            "1563/1562 [==============================] - 125s 80ms/step - loss: 2.0937 - acc: 0.2198 - val_loss: 2.0634 - val_acc: 0.2745\n",
            "Epoch 76/100\n",
            "1563/1562 [==============================] - 126s 81ms/step - loss: 2.0912 - acc: 0.2160 - val_loss: 2.0615 - val_acc: 0.2748\n",
            "Epoch 77/100\n",
            "1563/1562 [==============================] - 124s 79ms/step - loss: 2.0892 - acc: 0.2182 - val_loss: 2.0596 - val_acc: 0.2747\n",
            "Epoch 78/100\n",
            "1563/1562 [==============================] - 123s 79ms/step - loss: 2.0902 - acc: 0.2183 - val_loss: 2.0579 - val_acc: 0.2749\n",
            "Epoch 79/100\n",
            "1563/1562 [==============================] - 124s 79ms/step - loss: 2.0870 - acc: 0.2216 - val_loss: 2.0563 - val_acc: 0.2759\n",
            "Epoch 80/100\n",
            "1563/1562 [==============================] - 124s 80ms/step - loss: 2.0870 - acc: 0.2209 - val_loss: 2.0547 - val_acc: 0.2762\n",
            "Epoch 81/100\n",
            "1563/1562 [==============================] - 135s 86ms/step - loss: 2.0841 - acc: 0.2202 - val_loss: 2.0530 - val_acc: 0.2765\n",
            "Epoch 82/100\n",
            "1563/1562 [==============================] - 127s 81ms/step - loss: 2.0833 - acc: 0.2212 - val_loss: 2.0513 - val_acc: 0.2769\n",
            "Epoch 83/100\n",
            "1563/1562 [==============================] - 126s 81ms/step - loss: 2.0802 - acc: 0.2246 - val_loss: 2.0497 - val_acc: 0.2773\n",
            "Epoch 84/100\n",
            "1563/1562 [==============================] - 125s 80ms/step - loss: 2.0809 - acc: 0.2224 - val_loss: 2.0482 - val_acc: 0.2772\n",
            "Epoch 85/100\n",
            "1563/1562 [==============================] - 125s 80ms/step - loss: 2.0814 - acc: 0.2214 - val_loss: 2.0466 - val_acc: 0.2769\n",
            "Epoch 86/100\n",
            "1563/1562 [==============================] - 127s 81ms/step - loss: 2.0781 - acc: 0.2248 - val_loss: 2.0453 - val_acc: 0.2775\n",
            "Epoch 87/100\n",
            "1563/1562 [==============================] - 123s 79ms/step - loss: 2.0764 - acc: 0.2235 - val_loss: 2.0440 - val_acc: 0.2777\n",
            "Epoch 88/100\n",
            "1563/1562 [==============================] - 126s 81ms/step - loss: 2.0764 - acc: 0.2225 - val_loss: 2.0426 - val_acc: 0.2781\n",
            "Epoch 89/100\n",
            "1563/1562 [==============================] - 125s 80ms/step - loss: 2.0730 - acc: 0.2241 - val_loss: 2.0411 - val_acc: 0.2784\n",
            "Epoch 90/100\n",
            "1563/1562 [==============================] - 128s 82ms/step - loss: 2.0763 - acc: 0.2225 - val_loss: 2.0399 - val_acc: 0.2786\n",
            "Epoch 91/100\n",
            "1563/1562 [==============================] - 126s 81ms/step - loss: 2.0731 - acc: 0.2273 - val_loss: 2.0386 - val_acc: 0.2790\n",
            "Epoch 92/100\n",
            "1563/1562 [==============================] - 127s 81ms/step - loss: 2.0734 - acc: 0.2237 - val_loss: 2.0375 - val_acc: 0.2791\n",
            "Epoch 93/100\n",
            "1563/1562 [==============================] - 125s 80ms/step - loss: 2.0691 - acc: 0.2278 - val_loss: 2.0362 - val_acc: 0.2800\n",
            "Epoch 94/100\n",
            "1563/1562 [==============================] - 125s 80ms/step - loss: 2.0694 - acc: 0.2259 - val_loss: 2.0350 - val_acc: 0.2799\n",
            "Epoch 95/100\n",
            "1563/1562 [==============================] - 125s 80ms/step - loss: 2.0690 - acc: 0.2238 - val_loss: 2.0339 - val_acc: 0.2803\n",
            "Epoch 96/100\n",
            "1563/1562 [==============================] - 136s 87ms/step - loss: 2.0661 - acc: 0.2290 - val_loss: 2.0326 - val_acc: 0.2799\n",
            "Epoch 97/100\n",
            "1563/1562 [==============================] - 83s 53ms/step - loss: 2.0664 - acc: 0.2300 - val_loss: 2.0315 - val_acc: 0.2808\n",
            "Epoch 98/100\n",
            "1563/1562 [==============================] - 81s 52ms/step - loss: 2.0665 - acc: 0.2268 - val_loss: 2.0305 - val_acc: 0.2808\n",
            "Epoch 99/100\n",
            "1563/1562 [==============================] - 81s 52ms/step - loss: 2.0648 - acc: 0.2287 - val_loss: 2.0294 - val_acc: 0.2818\n",
            "Epoch 100/100\n",
            "1563/1562 [==============================] - 81s 52ms/step - loss: 2.0648 - acc: 0.2293 - val_loss: 2.0284 - val_acc: 0.2816\n",
            "Saved trained model at /content/saved_models/keras_cifar10_trained_model.h5 \n",
            "10000/10000 [==============================] - 2s 221us/step\n",
            "Test loss: 2.028388260269165\n",
            "Test accuracy: 0.2816\n",
            "Epoch 1/100\n",
            "1563/1562 [==============================] - 82s 53ms/step - loss: 2.0652 - acc: 0.2286 - val_loss: 2.0274 - val_acc: 0.2823\n",
            "Epoch 2/100\n",
            "1563/1562 [==============================] - 81s 52ms/step - loss: 2.0630 - acc: 0.2267 - val_loss: 2.0264 - val_acc: 0.2823\n",
            "Epoch 3/100\n",
            "1563/1562 [==============================] - 81s 52ms/step - loss: 2.0607 - acc: 0.2290 - val_loss: 2.0254 - val_acc: 0.2824\n",
            "Epoch 4/100\n",
            "1563/1562 [==============================] - 81s 52ms/step - loss: 2.0602 - acc: 0.2292 - val_loss: 2.0244 - val_acc: 0.2823\n",
            "Epoch 5/100\n",
            "1563/1562 [==============================] - 82s 52ms/step - loss: 2.0595 - acc: 0.2311 - val_loss: 2.0234 - val_acc: 0.2825\n",
            "Epoch 6/100\n",
            "1563/1562 [==============================] - 81s 52ms/step - loss: 2.0570 - acc: 0.2313 - val_loss: 2.0224 - val_acc: 0.2826\n",
            "Epoch 7/100\n",
            "1563/1562 [==============================] - 81s 52ms/step - loss: 2.0596 - acc: 0.2301 - val_loss: 2.0216 - val_acc: 0.2829\n",
            "Epoch 8/100\n",
            "1563/1562 [==============================] - 81s 52ms/step - loss: 2.0577 - acc: 0.2324 - val_loss: 2.0207 - val_acc: 0.2831\n",
            "Epoch 9/100\n",
            "1563/1562 [==============================] - 82s 52ms/step - loss: 2.0573 - acc: 0.2329 - val_loss: 2.0197 - val_acc: 0.2835\n",
            "Epoch 10/100\n",
            "1563/1562 [==============================] - 81s 52ms/step - loss: 2.0577 - acc: 0.2340 - val_loss: 2.0188 - val_acc: 0.2837\n",
            "Epoch 11/100\n",
            "1563/1562 [==============================] - 80s 51ms/step - loss: 2.0559 - acc: 0.2313 - val_loss: 2.0180 - val_acc: 0.2846\n",
            "Epoch 12/100\n",
            "1563/1562 [==============================] - 81s 52ms/step - loss: 2.0550 - acc: 0.2343 - val_loss: 2.0173 - val_acc: 0.2845\n",
            "Epoch 13/100\n",
            "1563/1562 [==============================] - 81s 52ms/step - loss: 2.0557 - acc: 0.2322 - val_loss: 2.0164 - val_acc: 0.2842\n",
            "Epoch 14/100\n",
            "1563/1562 [==============================] - 81s 52ms/step - loss: 2.0540 - acc: 0.2363 - val_loss: 2.0157 - val_acc: 0.2844\n",
            "Epoch 15/100\n",
            "1563/1562 [==============================] - 81s 52ms/step - loss: 2.0523 - acc: 0.2358 - val_loss: 2.0148 - val_acc: 0.2847\n",
            "Epoch 16/100\n",
            "1563/1562 [==============================] - 82s 52ms/step - loss: 2.0515 - acc: 0.2348 - val_loss: 2.0141 - val_acc: 0.2844\n",
            "Epoch 17/100\n",
            "1563/1562 [==============================] - 82s 52ms/step - loss: 2.0525 - acc: 0.2297 - val_loss: 2.0133 - val_acc: 0.2853\n",
            "Epoch 18/100\n",
            "1563/1562 [==============================] - 81s 52ms/step - loss: 2.0503 - acc: 0.2370 - val_loss: 2.0126 - val_acc: 0.2857\n",
            "Epoch 19/100\n",
            "1563/1562 [==============================] - 81s 52ms/step - loss: 2.0493 - acc: 0.2354 - val_loss: 2.0117 - val_acc: 0.2853\n",
            "Epoch 20/100\n",
            "1563/1562 [==============================] - 81s 52ms/step - loss: 2.0492 - acc: 0.2359 - val_loss: 2.0110 - val_acc: 0.2857\n",
            "Epoch 21/100\n",
            "1563/1562 [==============================] - 81s 52ms/step - loss: 2.0497 - acc: 0.2338 - val_loss: 2.0103 - val_acc: 0.2864\n",
            "Epoch 22/100\n",
            "1563/1562 [==============================] - 81s 52ms/step - loss: 2.0471 - acc: 0.2380 - val_loss: 2.0095 - val_acc: 0.2863\n",
            "Epoch 23/100\n",
            "1563/1562 [==============================] - 82s 52ms/step - loss: 2.0486 - acc: 0.2362 - val_loss: 2.0088 - val_acc: 0.2867\n",
            "Epoch 24/100\n",
            "1563/1562 [==============================] - 81s 52ms/step - loss: 2.0485 - acc: 0.2348 - val_loss: 2.0082 - val_acc: 0.2873\n",
            "Epoch 25/100\n",
            "1563/1562 [==============================] - 82s 52ms/step - loss: 2.0455 - acc: 0.2386 - val_loss: 2.0074 - val_acc: 0.2868\n",
            "Epoch 26/100\n",
            "1563/1562 [==============================] - 81s 52ms/step - loss: 2.0460 - acc: 0.2368 - val_loss: 2.0067 - val_acc: 0.2871\n",
            "Epoch 27/100\n",
            "1563/1562 [==============================] - 80s 51ms/step - loss: 2.0455 - acc: 0.2390 - val_loss: 2.0060 - val_acc: 0.2872\n",
            "Epoch 28/100\n",
            "1563/1562 [==============================] - 43s 27ms/step - loss: 2.0473 - acc: 0.2349 - val_loss: 2.0054 - val_acc: 0.2878\n",
            "Epoch 29/100\n",
            "1563/1562 [==============================] - 43s 27ms/step - loss: 2.0439 - acc: 0.2385 - val_loss: 2.0047 - val_acc: 0.2881\n",
            "Epoch 30/100\n",
            "1563/1562 [==============================] - 43s 27ms/step - loss: 2.0460 - acc: 0.2371 - val_loss: 2.0042 - val_acc: 0.2878\n",
            "Epoch 31/100\n",
            "1563/1562 [==============================] - 43s 27ms/step - loss: 2.0434 - acc: 0.2360 - val_loss: 2.0035 - val_acc: 0.2879\n",
            "Epoch 32/100\n",
            "1563/1562 [==============================] - 43s 28ms/step - loss: 2.0399 - acc: 0.2378 - val_loss: 2.0027 - val_acc: 0.2885\n",
            "Epoch 33/100\n",
            "1563/1562 [==============================] - 43s 27ms/step - loss: 2.0442 - acc: 0.2388 - val_loss: 2.0022 - val_acc: 0.2885\n",
            "Epoch 34/100\n",
            "1563/1562 [==============================] - 42s 27ms/step - loss: 2.0427 - acc: 0.2391 - val_loss: 2.0017 - val_acc: 0.2884\n",
            "Epoch 35/100\n",
            "1563/1562 [==============================] - 43s 27ms/step - loss: 2.0403 - acc: 0.2420 - val_loss: 2.0009 - val_acc: 0.2888\n",
            "Epoch 36/100\n",
            "1563/1562 [==============================] - 43s 27ms/step - loss: 2.0392 - acc: 0.2409 - val_loss: 2.0004 - val_acc: 0.2884\n",
            "Epoch 37/100\n",
            "1563/1562 [==============================] - 43s 27ms/step - loss: 2.0397 - acc: 0.2418 - val_loss: 1.9997 - val_acc: 0.2886\n",
            "Epoch 38/100\n",
            "1563/1562 [==============================] - 43s 28ms/step - loss: 2.0405 - acc: 0.2416 - val_loss: 1.9991 - val_acc: 0.2883\n",
            "Epoch 39/100\n",
            "1347/1562 [========================>.....] - ETA: 8s - loss: 2.0419 - acc: 0.2398"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}