{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1_Part2_Adam_RMSprop_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "ehqoZJB7QmiX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 1. IMPORT LIBRARIES AND CIFAR 10"
      ]
    },
    {
      "metadata": {
        "id": "82BEVMpVQmiY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# general\n",
        "import numpy as np\n",
        "import keras\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "# dataset\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "# modeling tools\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bqrqQNkRQmib",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Check and Prepare Data:"
      ]
    },
    {
      "metadata": {
        "id": "K0PXyPpuQmid",
        "colab_type": "code",
        "outputId": "15ac58d7-b534-4a90-9931-5ef7fbc69747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# split data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "# check data dimension\n",
        "print('training data shape: {}'.format(x_train.shape))\n",
        "print('test data shape: {}'.format(x_test.shape))\n",
        "print('shape of one instance: {}'.format(x_train.shape[1:]))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training data shape: (50000, 32, 32, 3)\n",
            "test data shape: (10000, 32, 32, 3)\n",
            "shape of one instance: (32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EmIu1K_nQmii",
        "colab_type": "code",
        "outputId": "1934453d-8660-4079-94d2-868e95894796",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# check labels\n",
        "labels = []\n",
        "for y in y_train.flatten():\n",
        "    if y not in labels:\n",
        "        labels.append(y)\n",
        "print('training labels are: {}'.format(labels))\n",
        "print('# labels: {}'.format(len(labels)))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training labels are: [6, 9, 4, 1, 2, 7, 8, 3, 5, 0]\n",
            "# labels: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v7qrl3ZWQmil",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cifar_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MVsVa-MnQmio",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Convert class vectors to one-hot encoded vectors.\n",
        "y_train = to_categorical(y_train, num_classes = 10)\n",
        "y_test = to_categorical(y_test, num_classes = 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9eNTR6MhQmiq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # Transform images from (32,32,3) to 3072-dimensional vectors (32*32*3)\n",
        "\n",
        "# x_train = np.reshape(x_train,(50000,3072))\n",
        "# x_test = np.reshape(x_test,(10000,3072))\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Normalization of pixel values (to [0-1] range)\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X5SUWnWPQmit",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2. HELP FUNCTION"
      ]
    },
    {
      "metadata": {
        "id": "OFiGeH2kQmiu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plotAcc(history):\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.title('model_accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc = 'upper right')\n",
        "    \n",
        "def plotLoss(history):\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model_loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc = 'upper right')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OOHPAhjhQmix",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3. BUILD CNN CLASSIFIER"
      ]
    },
    {
      "metadata": {
        "id": "HgU1PaN9Qmiy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_classes= 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4ar26Z88Qmi3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### RMSprop - LR = 0.001"
      ]
    },
    {
      "metadata": {
        "id": "AwxCMLaEQmi5",
        "colab_type": "code",
        "outputId": "dad01a0e-8e37-4723-861c-2e7eb272e676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3502
        }
      },
      "cell_type": "code",
      "source": [
        "# hyper parameters to be tuned\n",
        "n_neuron = 256\n",
        "batch_size = 32\n",
        "dropout_rate = 0.2\n",
        "data_augmentation = True\n",
        "epochs = 100\n",
        "num_predictions = 20\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'keras_cifar10_trained_model.h5'\n",
        "\n",
        "# set up model structure\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# initiate RMSprop optimizer\n",
        "opt = keras.optimizers.rmsprop(lr=0.001, decay=1e-6)\n",
        "    \n",
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              steps_per_epoch = int(50000/batch_size),\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        # randomly shift images horizontally (fraction of total width)\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically (fraction of total height)\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.,  # set range for random shear\n",
        "        zoom_range=0.,  # set range for random zoom\n",
        "        channel_shift_range=0.,  # set range for random channel shifts\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        cval=0.,  # value used for fill_mode = \"constant\"\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False,  # randomly flip images\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        " \n",
        "    # Compute quantities required for feature-wise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    exp_optimizer_r = model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                                   batch_size=batch_size),\n",
        "                        epochs=epochs,\n",
        "                        steps_per_epoch = int(50000/batch_size),\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        workers=4)\n",
        "\n",
        "\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n",
            "Epoch 1/100\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 1.6273 - acc: 0.4086 - val_loss: 1.3811 - val_acc: 0.5069\n",
            "Epoch 2/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 1.2908 - acc: 0.5421 - val_loss: 1.0818 - val_acc: 0.6185\n",
            "Epoch 3/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 1.1752 - acc: 0.5871 - val_loss: 0.9303 - val_acc: 0.6809\n",
            "Epoch 4/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 1.1369 - acc: 0.6088 - val_loss: 0.9409 - val_acc: 0.6673\n",
            "Epoch 5/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 1.1373 - acc: 0.6133 - val_loss: 1.0834 - val_acc: 0.6434\n",
            "Epoch 6/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.1492 - acc: 0.6145 - val_loss: 1.0103 - val_acc: 0.6676\n",
            "Epoch 7/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.1831 - acc: 0.6056 - val_loss: 1.1338 - val_acc: 0.6397\n",
            "Epoch 8/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.2115 - acc: 0.5993 - val_loss: 1.2705 - val_acc: 0.5806\n",
            "Epoch 9/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.2491 - acc: 0.5897 - val_loss: 1.0461 - val_acc: 0.6469\n",
            "Epoch 10/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 1.2528 - acc: 0.5867 - val_loss: 1.0807 - val_acc: 0.6375\n",
            "Epoch 11/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 1.2944 - acc: 0.5733 - val_loss: 1.1632 - val_acc: 0.5984\n",
            "Epoch 12/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 1.3337 - acc: 0.5602 - val_loss: 1.2256 - val_acc: 0.5633\n",
            "Epoch 13/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 1.3882 - acc: 0.5464 - val_loss: 1.1534 - val_acc: 0.6198\n",
            "Epoch 14/100\n",
            "1562/1562 [==============================] - 42s 27ms/step - loss: 1.4295 - acc: 0.5314 - val_loss: 1.2891 - val_acc: 0.5738\n",
            "Epoch 15/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 1.4706 - acc: 0.5195 - val_loss: 1.4247 - val_acc: 0.5313\n",
            "Epoch 16/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 1.5067 - acc: 0.5095 - val_loss: 1.3217 - val_acc: 0.5564\n",
            "Epoch 17/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 1.5618 - acc: 0.4833 - val_loss: 1.4598 - val_acc: 0.4888\n",
            "Epoch 18/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 1.6372 - acc: 0.4589 - val_loss: 1.3392 - val_acc: 0.5340\n",
            "Epoch 19/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.6739 - acc: 0.4428 - val_loss: 1.3376 - val_acc: 0.5504\n",
            "Epoch 20/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 1.6986 - acc: 0.4352 - val_loss: 1.3539 - val_acc: 0.5098\n",
            "Epoch 21/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.7345 - acc: 0.4195 - val_loss: 1.4057 - val_acc: 0.5393\n",
            "Epoch 22/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.7084 - acc: 0.4230 - val_loss: 1.5779 - val_acc: 0.4179\n",
            "Epoch 23/100\n",
            "1562/1562 [==============================] - 45s 28ms/step - loss: 1.7452 - acc: 0.4130 - val_loss: 1.3637 - val_acc: 0.5174\n",
            "Epoch 24/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 1.7718 - acc: 0.4018 - val_loss: 1.4998 - val_acc: 0.4708\n",
            "Epoch 25/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.7719 - acc: 0.3973 - val_loss: 1.4492 - val_acc: 0.4818\n",
            "Epoch 26/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.7964 - acc: 0.3872 - val_loss: 1.8716 - val_acc: 0.4790\n",
            "Epoch 27/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.8077 - acc: 0.3848 - val_loss: 1.5157 - val_acc: 0.4775\n",
            "Epoch 28/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.8397 - acc: 0.3725 - val_loss: 1.5501 - val_acc: 0.4397\n",
            "Epoch 29/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 1.8775 - acc: 0.3546 - val_loss: 1.6249 - val_acc: 0.4304\n",
            "Epoch 30/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.8755 - acc: 0.3501 - val_loss: 1.4575 - val_acc: 0.4558\n",
            "Epoch 31/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9235 - acc: 0.3366 - val_loss: 1.5860 - val_acc: 0.4222\n",
            "Epoch 32/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9414 - acc: 0.3263 - val_loss: 1.6648 - val_acc: 0.4223\n",
            "Epoch 33/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9298 - acc: 0.3205 - val_loss: 1.6158 - val_acc: 0.4091\n",
            "Epoch 34/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9739 - acc: 0.3077 - val_loss: 1.6068 - val_acc: 0.4139\n",
            "Epoch 35/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9695 - acc: 0.3025 - val_loss: 1.7864 - val_acc: 0.3402\n",
            "Epoch 36/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 1.9660 - acc: 0.2961 - val_loss: 1.6741 - val_acc: 0.3957\n",
            "Epoch 37/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9719 - acc: 0.2975 - val_loss: 1.7141 - val_acc: 0.3577\n",
            "Epoch 38/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9720 - acc: 0.2928 - val_loss: 1.8992 - val_acc: 0.2609\n",
            "Epoch 39/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 2.0151 - acc: 0.2827 - val_loss: 1.6424 - val_acc: 0.3786\n",
            "Epoch 40/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 2.0124 - acc: 0.2800 - val_loss: 6.9782 - val_acc: 0.2755\n",
            "Epoch 41/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9928 - acc: 0.2790 - val_loss: 1.7074 - val_acc: 0.3816\n",
            "Epoch 42/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 2.0206 - acc: 0.2754 - val_loss: 1.7589 - val_acc: 0.3425\n",
            "Epoch 43/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 2.0552 - acc: 0.2571 - val_loss: 1.9504 - val_acc: 0.2636\n",
            "Epoch 44/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 2.0755 - acc: 0.2452 - val_loss: 1.8239 - val_acc: 0.3527\n",
            "Epoch 45/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 2.0535 - acc: 0.2372 - val_loss: 2.1246 - val_acc: 0.2591\n",
            "Epoch 46/100\n",
            "1562/1562 [==============================] - 45s 28ms/step - loss: 2.0268 - acc: 0.2440 - val_loss: 1.7562 - val_acc: 0.3257\n",
            "Epoch 47/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 2.0038 - acc: 0.2578 - val_loss: 1.7735 - val_acc: 0.3152\n",
            "Epoch 48/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9646 - acc: 0.2697 - val_loss: 1.7674 - val_acc: 0.3088\n",
            "Epoch 49/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9421 - acc: 0.2715 - val_loss: 1.7574 - val_acc: 0.3473\n",
            "Epoch 50/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9415 - acc: 0.2813 - val_loss: 2.0233 - val_acc: 0.2811\n",
            "Epoch 51/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9323 - acc: 0.2872 - val_loss: 1.7411 - val_acc: 0.3324\n",
            "Epoch 52/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 1.9156 - acc: 0.2873 - val_loss: 1.6649 - val_acc: 0.3681\n",
            "Epoch 53/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.8907 - acc: 0.2912 - val_loss: 1.7052 - val_acc: 0.3456\n",
            "Epoch 54/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.8773 - acc: 0.2951 - val_loss: 1.7885 - val_acc: 0.3385\n",
            "Epoch 55/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.8641 - acc: 0.2977 - val_loss: 1.6818 - val_acc: 0.3504\n",
            "Epoch 56/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.8695 - acc: 0.2981 - val_loss: 1.6960 - val_acc: 0.3510\n",
            "Epoch 57/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.8660 - acc: 0.2981 - val_loss: 2.0178 - val_acc: 0.2496\n",
            "Epoch 58/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.8713 - acc: 0.2960 - val_loss: 1.7619 - val_acc: 0.3339\n",
            "Epoch 59/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.8726 - acc: 0.2994 - val_loss: 1.7104 - val_acc: 0.3382\n",
            "Epoch 60/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.8734 - acc: 0.2945 - val_loss: 1.6844 - val_acc: 0.3511\n",
            "Epoch 61/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.8731 - acc: 0.2947 - val_loss: 1.7093 - val_acc: 0.3498\n",
            "Epoch 62/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.8758 - acc: 0.2919 - val_loss: 1.7247 - val_acc: 0.3475\n",
            "Epoch 63/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.8800 - acc: 0.2946 - val_loss: 1.7554 - val_acc: 0.3237\n",
            "Epoch 64/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 1.8813 - acc: 0.2910 - val_loss: 1.7300 - val_acc: 0.3349\n",
            "Epoch 65/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 1.9016 - acc: 0.2853 - val_loss: 1.7315 - val_acc: 0.3348\n",
            "Epoch 66/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9023 - acc: 0.2839 - val_loss: 1.8677 - val_acc: 0.3176\n",
            "Epoch 67/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9061 - acc: 0.2867 - val_loss: 1.7023 - val_acc: 0.3413\n",
            "Epoch 68/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9050 - acc: 0.2843 - val_loss: 1.9210 - val_acc: 0.2809\n",
            "Epoch 69/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9226 - acc: 0.2831 - val_loss: 1.7316 - val_acc: 0.3405\n",
            "Epoch 70/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9381 - acc: 0.2841 - val_loss: 1.7721 - val_acc: 0.3231\n",
            "Epoch 71/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9799 - acc: 0.2822 - val_loss: 1.7376 - val_acc: 0.3398\n",
            "Epoch 72/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9114 - acc: 0.2796 - val_loss: 1.7648 - val_acc: 0.3260\n",
            "Epoch 73/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9270 - acc: 0.2790 - val_loss: 1.8108 - val_acc: 0.3123\n",
            "Epoch 74/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9977 - acc: 0.2782 - val_loss: 1.8894 - val_acc: 0.3158\n",
            "Epoch 75/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9392 - acc: 0.2752 - val_loss: 1.7801 - val_acc: 0.3148\n",
            "Epoch 76/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9402 - acc: 0.2716 - val_loss: 1.7758 - val_acc: 0.3174\n",
            "Epoch 77/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9392 - acc: 0.2727 - val_loss: 1.7996 - val_acc: 0.3249\n",
            "Epoch 78/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9427 - acc: 0.2739 - val_loss: 1.8767 - val_acc: 0.2881\n",
            "Epoch 79/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9451 - acc: 0.2743 - val_loss: 1.9805 - val_acc: 0.3280\n",
            "Epoch 80/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9617 - acc: 0.2730 - val_loss: 1.9783 - val_acc: 0.2877\n",
            "Epoch 81/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9560 - acc: 0.2685 - val_loss: 1.8190 - val_acc: 0.3071\n",
            "Epoch 82/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9542 - acc: 0.2644 - val_loss: 1.7910 - val_acc: 0.3164\n",
            "Epoch 83/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 1.9627 - acc: 0.2683 - val_loss: 1.8714 - val_acc: 0.2927\n",
            "Epoch 84/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 1.9596 - acc: 0.2681 - val_loss: 2.0229 - val_acc: 0.2456\n",
            "Epoch 85/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9566 - acc: 0.2665 - val_loss: 1.8393 - val_acc: 0.3018\n",
            "Epoch 86/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9578 - acc: 0.2670 - val_loss: 1.8519 - val_acc: 0.3051\n",
            "Epoch 87/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 1.9660 - acc: 0.2678 - val_loss: 1.9010 - val_acc: 0.2825\n",
            "Epoch 88/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 1.9676 - acc: 0.2651 - val_loss: 2.0053 - val_acc: 0.2572\n",
            "Epoch 89/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 1.9402 - acc: 0.2678 - val_loss: 1.7568 - val_acc: 0.3241\n",
            "Epoch 90/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 1.9557 - acc: 0.2635 - val_loss: 1.7956 - val_acc: 0.3068\n",
            "Epoch 91/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 1.9467 - acc: 0.2672 - val_loss: 1.8917 - val_acc: 0.2760\n",
            "Epoch 92/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 1.9581 - acc: 0.2631 - val_loss: 1.8132 - val_acc: 0.3108\n",
            "Epoch 93/100\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 1.9533 - acc: 0.2617 - val_loss: 1.8307 - val_acc: 0.3062\n",
            "Epoch 94/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 1.9502 - acc: 0.2637 - val_loss: 1.8151 - val_acc: 0.3098\n",
            "Epoch 95/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 1.9765 - acc: 0.2579 - val_loss: 1.8200 - val_acc: 0.3034\n",
            "Epoch 96/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 1.9676 - acc: 0.2592 - val_loss: 1.8197 - val_acc: 0.3027\n",
            "Epoch 97/100\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 1.9529 - acc: 0.2631 - val_loss: 1.8967 - val_acc: 0.2856\n",
            "Epoch 98/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 1.9543 - acc: 0.2627 - val_loss: 1.8242 - val_acc: 0.3030\n",
            "Epoch 99/100\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 1.9629 - acc: 0.2580 - val_loss: 1.8334 - val_acc: 0.3009\n",
            "Epoch 100/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 1.9661 - acc: 0.2564 - val_loss: 1.8285 - val_acc: 0.3002\n",
            "Saved trained model at /content/saved_models/keras_cifar10_trained_model.h5 \n",
            "10000/10000 [==============================] - 1s 150us/step\n",
            "Test loss: 1.8285300170898438\n",
            "Test accuracy: 0.3002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bFbOB4O_PtOa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Adam - LR = 0.001"
      ]
    },
    {
      "metadata": {
        "id": "Yj6Ws7NMQmi9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3502
        },
        "outputId": "a7aed1f3-1a86-4a8d-b934-3c2dd78688f4"
      },
      "cell_type": "code",
      "source": [
        "# hyper parameters to be tuned\n",
        "n_neuron = 256\n",
        "batch_size = 32\n",
        "dropout_rate = 0.2\n",
        "data_augmentation = True\n",
        "epochs = 100\n",
        "num_predictions = 20\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models_2')\n",
        "model_name = 'keras_cifar10_trained_model_2.h5'\n",
        "\n",
        "# set up model structure\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# initiate RMSprop optimizer\n",
        "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "  \n",
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "if not data_augmentation:\n",
        "    print('Not using data augmentation.')\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "else:\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        # randomly shift images horizontally (fraction of total width)\n",
        "        width_shift_range=0.1,\n",
        "        # randomly shift images vertically (fraction of total height)\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.,  # set range for random shear\n",
        "        zoom_range=0.,  # set range for random zoom\n",
        "        channel_shift_range=0.,  # set range for random channel shifts\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        cval=0.,  # value used for fill_mode = \"constant\"\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False,  # randomly flip images\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        rescale=None,\n",
        "        # set function that will be applied on each input\n",
        "        preprocessing_function=None,\n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)\n",
        " \n",
        "    # Compute quantities required for feature-wise normalization\n",
        "    # (std, mean, and principal components if ZCA whitening is applied).\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model on the batches generated by datagen.flow().\n",
        "    exp_optimizer_a = model.fit_generator(datagen.flow(x_train, y_train,\n",
        "                                     batch_size=batch_size),\n",
        "                        epochs=epochs,\n",
        "                        steps_per_epoch = int(50000/batch_size),\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        workers=4)\n",
        "\n",
        "\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n",
            "Epoch 1/100\n",
            "1562/1562 [==============================] - 48s 30ms/step - loss: 1.6336 - acc: 0.3956 - val_loss: 1.2100 - val_acc: 0.5694\n",
            "Epoch 2/100\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 1.2844 - acc: 0.5405 - val_loss: 1.0644 - val_acc: 0.6191\n",
            "Epoch 3/100\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 1.1445 - acc: 0.5922 - val_loss: 0.9082 - val_acc: 0.6828\n",
            "Epoch 4/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 1.0591 - acc: 0.6245 - val_loss: 0.8969 - val_acc: 0.6835\n",
            "Epoch 5/100\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 0.9990 - acc: 0.6496 - val_loss: 0.8265 - val_acc: 0.7131\n",
            "Epoch 6/100\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 0.9601 - acc: 0.6628 - val_loss: 0.8198 - val_acc: 0.7087\n",
            "Epoch 7/100\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 0.9342 - acc: 0.6730 - val_loss: 0.8232 - val_acc: 0.7103\n",
            "Epoch 8/100\n",
            "1562/1562 [==============================] - 46s 29ms/step - loss: 0.9133 - acc: 0.6785 - val_loss: 0.8440 - val_acc: 0.7038\n",
            "Epoch 9/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.8914 - acc: 0.6891 - val_loss: 0.7535 - val_acc: 0.7344\n",
            "Epoch 10/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.8777 - acc: 0.6950 - val_loss: 0.7106 - val_acc: 0.7541\n",
            "Epoch 11/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.8630 - acc: 0.7005 - val_loss: 0.7352 - val_acc: 0.7500\n",
            "Epoch 12/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.8569 - acc: 0.7027 - val_loss: 0.7364 - val_acc: 0.7446\n",
            "Epoch 13/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.8449 - acc: 0.7051 - val_loss: 0.6656 - val_acc: 0.7690\n",
            "Epoch 14/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.8284 - acc: 0.7141 - val_loss: 0.7023 - val_acc: 0.7508\n",
            "Epoch 15/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.8222 - acc: 0.7137 - val_loss: 0.7176 - val_acc: 0.7476\n",
            "Epoch 16/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.8120 - acc: 0.7177 - val_loss: 0.7388 - val_acc: 0.7463\n",
            "Epoch 17/100\n",
            "1562/1562 [==============================] - 45s 28ms/step - loss: 0.8063 - acc: 0.7194 - val_loss: 0.6694 - val_acc: 0.7695\n",
            "Epoch 18/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.8034 - acc: 0.7225 - val_loss: 0.7057 - val_acc: 0.7534\n",
            "Epoch 19/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.7924 - acc: 0.7239 - val_loss: 0.7053 - val_acc: 0.7563\n",
            "Epoch 20/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.7942 - acc: 0.7244 - val_loss: 0.6891 - val_acc: 0.7569\n",
            "Epoch 21/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.7875 - acc: 0.7274 - val_loss: 0.6699 - val_acc: 0.7690\n",
            "Epoch 22/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.7855 - acc: 0.7301 - val_loss: 0.7004 - val_acc: 0.7553\n",
            "Epoch 23/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.7810 - acc: 0.7318 - val_loss: 0.7289 - val_acc: 0.7527\n",
            "Epoch 24/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.7680 - acc: 0.7341 - val_loss: 0.6880 - val_acc: 0.7652\n",
            "Epoch 25/100\n",
            "1562/1562 [==============================] - 45s 29ms/step - loss: 0.7739 - acc: 0.7331 - val_loss: 0.7110 - val_acc: 0.7599\n",
            "Epoch 26/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.7655 - acc: 0.7353 - val_loss: 0.6545 - val_acc: 0.7729\n",
            "Epoch 27/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.7676 - acc: 0.7347 - val_loss: 0.6541 - val_acc: 0.7738\n",
            "Epoch 28/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.7651 - acc: 0.7393 - val_loss: 0.6932 - val_acc: 0.7661\n",
            "Epoch 29/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.7551 - acc: 0.7410 - val_loss: 0.6670 - val_acc: 0.7682\n",
            "Epoch 30/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.7518 - acc: 0.7419 - val_loss: 0.6165 - val_acc: 0.7892\n",
            "Epoch 31/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.7523 - acc: 0.7417 - val_loss: 0.6373 - val_acc: 0.7816\n",
            "Epoch 32/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.7486 - acc: 0.7414 - val_loss: 0.6776 - val_acc: 0.7698\n",
            "Epoch 33/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.7498 - acc: 0.7416 - val_loss: 0.6755 - val_acc: 0.7686\n",
            "Epoch 34/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.7460 - acc: 0.7425 - val_loss: 0.6691 - val_acc: 0.7774\n",
            "Epoch 35/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.7423 - acc: 0.7441 - val_loss: 0.6936 - val_acc: 0.7668\n",
            "Epoch 36/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.7433 - acc: 0.7416 - val_loss: 0.6232 - val_acc: 0.7885\n",
            "Epoch 37/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.7411 - acc: 0.7416 - val_loss: 0.6594 - val_acc: 0.7705\n",
            "Epoch 38/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.7456 - acc: 0.7456 - val_loss: 0.6389 - val_acc: 0.7823\n",
            "Epoch 39/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.7409 - acc: 0.7462 - val_loss: 0.6631 - val_acc: 0.7786\n",
            "Epoch 40/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.7339 - acc: 0.7468 - val_loss: 0.6569 - val_acc: 0.7774\n",
            "Epoch 41/100\n",
            "1562/1562 [==============================] - 44s 28ms/step - loss: 0.7385 - acc: 0.7463 - val_loss: 0.6336 - val_acc: 0.7877\n",
            "Epoch 42/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 0.7326 - acc: 0.7457 - val_loss: 0.6435 - val_acc: 0.7801\n",
            "Epoch 43/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 0.7240 - acc: 0.7499 - val_loss: 0.6467 - val_acc: 0.7797\n",
            "Epoch 44/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 0.7261 - acc: 0.7497 - val_loss: 0.6401 - val_acc: 0.7792\n",
            "Epoch 45/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 0.7279 - acc: 0.7499 - val_loss: 0.6085 - val_acc: 0.7971\n",
            "Epoch 46/100\n",
            "1562/1562 [==============================] - 42s 27ms/step - loss: 0.7282 - acc: 0.7500 - val_loss: 0.6314 - val_acc: 0.7845\n",
            "Epoch 47/100\n",
            "1562/1562 [==============================] - 42s 27ms/step - loss: 0.7211 - acc: 0.7528 - val_loss: 0.6547 - val_acc: 0.7738\n",
            "Epoch 48/100\n",
            "1562/1562 [==============================] - 42s 27ms/step - loss: 0.7335 - acc: 0.7471 - val_loss: 0.5993 - val_acc: 0.7950\n",
            "Epoch 49/100\n",
            "1562/1562 [==============================] - 42s 27ms/step - loss: 0.7143 - acc: 0.7562 - val_loss: 0.6546 - val_acc: 0.7772\n",
            "Epoch 50/100\n",
            "1562/1562 [==============================] - 41s 27ms/step - loss: 0.7228 - acc: 0.7534 - val_loss: 0.5993 - val_acc: 0.7962\n",
            "Epoch 51/100\n",
            "1562/1562 [==============================] - 42s 27ms/step - loss: 0.7218 - acc: 0.7531 - val_loss: 0.6606 - val_acc: 0.7790\n",
            "Epoch 52/100\n",
            "1562/1562 [==============================] - 42s 27ms/step - loss: 0.7201 - acc: 0.7531 - val_loss: 0.6122 - val_acc: 0.7985\n",
            "Epoch 53/100\n",
            "1562/1562 [==============================] - 41s 26ms/step - loss: 0.7189 - acc: 0.7519 - val_loss: 0.7141 - val_acc: 0.7662\n",
            "Epoch 54/100\n",
            "1562/1562 [==============================] - 41s 26ms/step - loss: 0.7201 - acc: 0.7543 - val_loss: 0.6159 - val_acc: 0.7914\n",
            "Epoch 55/100\n",
            "1562/1562 [==============================] - 41s 26ms/step - loss: 0.7129 - acc: 0.7535 - val_loss: 0.6562 - val_acc: 0.7812\n",
            "Epoch 56/100\n",
            "1562/1562 [==============================] - 41s 26ms/step - loss: 0.7135 - acc: 0.7557 - val_loss: 0.7604 - val_acc: 0.7516\n",
            "Epoch 57/100\n",
            "1562/1562 [==============================] - 41s 26ms/step - loss: 0.7174 - acc: 0.7547 - val_loss: 0.6234 - val_acc: 0.7860\n",
            "Epoch 58/100\n",
            "1562/1562 [==============================] - 41s 26ms/step - loss: 0.7159 - acc: 0.7550 - val_loss: 0.5928 - val_acc: 0.7969\n",
            "Epoch 59/100\n",
            "1562/1562 [==============================] - 41s 26ms/step - loss: 0.7162 - acc: 0.7540 - val_loss: 0.6488 - val_acc: 0.7806\n",
            "Epoch 60/100\n",
            "1562/1562 [==============================] - 42s 27ms/step - loss: 0.7089 - acc: 0.7597 - val_loss: 0.6373 - val_acc: 0.7834\n",
            "Epoch 61/100\n",
            "1562/1562 [==============================] - 41s 26ms/step - loss: 0.7152 - acc: 0.7563 - val_loss: 0.6047 - val_acc: 0.7991\n",
            "Epoch 62/100\n",
            "1562/1562 [==============================] - 41s 26ms/step - loss: 0.7042 - acc: 0.7574 - val_loss: 0.6504 - val_acc: 0.7792\n",
            "Epoch 63/100\n",
            "1562/1562 [==============================] - 42s 27ms/step - loss: 0.6972 - acc: 0.7600 - val_loss: 0.6605 - val_acc: 0.7771\n",
            "Epoch 64/100\n",
            "1562/1562 [==============================] - 42s 27ms/step - loss: 0.7093 - acc: 0.7553 - val_loss: 0.6453 - val_acc: 0.7772\n",
            "Epoch 65/100\n",
            "1562/1562 [==============================] - 42s 27ms/step - loss: 0.7076 - acc: 0.7568 - val_loss: 0.7057 - val_acc: 0.7641\n",
            "Epoch 66/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 0.7053 - acc: 0.7565 - val_loss: 0.5962 - val_acc: 0.7969\n",
            "Epoch 67/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 0.7137 - acc: 0.7561 - val_loss: 0.6675 - val_acc: 0.7710\n",
            "Epoch 68/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 0.7100 - acc: 0.7553 - val_loss: 0.6119 - val_acc: 0.7933\n",
            "Epoch 69/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 0.7042 - acc: 0.7590 - val_loss: 0.6615 - val_acc: 0.7722\n",
            "Epoch 70/100\n",
            "1562/1562 [==============================] - 42s 27ms/step - loss: 0.6999 - acc: 0.7624 - val_loss: 0.6181 - val_acc: 0.7912\n",
            "Epoch 71/100\n",
            "1562/1562 [==============================] - 42s 27ms/step - loss: 0.7047 - acc: 0.7589 - val_loss: 0.6410 - val_acc: 0.7848\n",
            "Epoch 72/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 0.7071 - acc: 0.7599 - val_loss: 0.6327 - val_acc: 0.7836\n",
            "Epoch 73/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 0.7007 - acc: 0.7609 - val_loss: 0.6125 - val_acc: 0.7929\n",
            "Epoch 74/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 0.7038 - acc: 0.7598 - val_loss: 0.6020 - val_acc: 0.7954\n",
            "Epoch 75/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 0.7047 - acc: 0.7596 - val_loss: 0.6302 - val_acc: 0.7897\n",
            "Epoch 76/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 0.6995 - acc: 0.7618 - val_loss: 0.6913 - val_acc: 0.7755\n",
            "Epoch 77/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 0.7009 - acc: 0.7622 - val_loss: 0.6161 - val_acc: 0.7948\n",
            "Epoch 78/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 0.7008 - acc: 0.7611 - val_loss: 0.6489 - val_acc: 0.7884\n",
            "Epoch 79/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 0.6918 - acc: 0.7643 - val_loss: 0.6126 - val_acc: 0.7907\n",
            "Epoch 80/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 0.6973 - acc: 0.7586 - val_loss: 0.6600 - val_acc: 0.7804\n",
            "Epoch 81/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 0.6979 - acc: 0.7620 - val_loss: 0.6299 - val_acc: 0.7907\n",
            "Epoch 82/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 0.6949 - acc: 0.7635 - val_loss: 0.6051 - val_acc: 0.7965\n",
            "Epoch 83/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 0.6889 - acc: 0.7631 - val_loss: 0.5897 - val_acc: 0.8081\n",
            "Epoch 84/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 0.6946 - acc: 0.7616 - val_loss: 0.6216 - val_acc: 0.7889\n",
            "Epoch 85/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 0.6945 - acc: 0.7642 - val_loss: 0.6251 - val_acc: 0.7943\n",
            "Epoch 86/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 0.6985 - acc: 0.7627 - val_loss: 0.6278 - val_acc: 0.7891\n",
            "Epoch 87/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 0.6956 - acc: 0.7635 - val_loss: 0.5803 - val_acc: 0.8056\n",
            "Epoch 88/100\n",
            "1562/1562 [==============================] - 43s 27ms/step - loss: 0.6941 - acc: 0.7638 - val_loss: 0.5925 - val_acc: 0.8011\n",
            "Epoch 89/100\n",
            "1562/1562 [==============================] - 43s 28ms/step - loss: 0.7032 - acc: 0.7592 - val_loss: 0.5647 - val_acc: 0.8129\n",
            "Epoch 90/100\n",
            "1562/1562 [==============================] - 42s 27ms/step - loss: 0.7018 - acc: 0.7600 - val_loss: 0.6712 - val_acc: 0.7784\n",
            "Epoch 91/100\n",
            "1562/1562 [==============================] - 40s 26ms/step - loss: 0.6877 - acc: 0.7667 - val_loss: 0.6407 - val_acc: 0.7903\n",
            "Epoch 92/100\n",
            "1562/1562 [==============================] - 40s 26ms/step - loss: 0.6899 - acc: 0.7662 - val_loss: 0.6016 - val_acc: 0.8009\n",
            "Epoch 93/100\n",
            "1562/1562 [==============================] - 40s 26ms/step - loss: 0.6879 - acc: 0.7638 - val_loss: 0.6250 - val_acc: 0.7932\n",
            "Epoch 94/100\n",
            "1562/1562 [==============================] - 40s 26ms/step - loss: 0.6931 - acc: 0.7610 - val_loss: 0.6514 - val_acc: 0.7828\n",
            "Epoch 95/100\n",
            "1562/1562 [==============================] - 41s 26ms/step - loss: 0.6924 - acc: 0.7643 - val_loss: 0.6238 - val_acc: 0.7869\n",
            "Epoch 96/100\n",
            "1562/1562 [==============================] - 41s 26ms/step - loss: 0.6900 - acc: 0.7623 - val_loss: 0.6777 - val_acc: 0.7783\n",
            "Epoch 97/100\n",
            "1562/1562 [==============================] - 40s 26ms/step - loss: 0.6905 - acc: 0.7628 - val_loss: 0.5760 - val_acc: 0.8035\n",
            "Epoch 98/100\n",
            "1562/1562 [==============================] - 40s 25ms/step - loss: 0.6938 - acc: 0.7620 - val_loss: 0.5827 - val_acc: 0.8027\n",
            "Epoch 99/100\n",
            "1562/1562 [==============================] - 40s 26ms/step - loss: 0.6903 - acc: 0.7637 - val_loss: 0.6094 - val_acc: 0.7940\n",
            "Epoch 100/100\n",
            "1562/1562 [==============================] - 40s 26ms/step - loss: 0.6879 - acc: 0.7676 - val_loss: 0.6224 - val_acc: 0.7963\n",
            "Saved trained model at /content/saved_models_2/keras_cifar10_trained_model_2.h5 \n",
            "10000/10000 [==============================] - 1s 144us/step\n",
            "Test loss: 0.622355110502243\n",
            "Test accuracy: 0.7963\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}