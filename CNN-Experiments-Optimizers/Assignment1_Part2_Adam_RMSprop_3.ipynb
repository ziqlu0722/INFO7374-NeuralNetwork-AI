{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ehqoZJB7QmiX"
   },
   "source": [
    "#### 1. IMPORT LIBRARIES AND CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "82BEVMpVQmiY"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# general\n",
    "import numpy as np\n",
    "import keras\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "# dataset\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# modeling tools\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bqrqQNkRQmib"
   },
   "source": [
    "Check and Prepare Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "K0PXyPpuQmid",
    "outputId": "15ac58d7-b534-4a90-9931-5ef7fbc69747"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape: (50000, 32, 32, 3)\n",
      "test data shape: (10000, 32, 32, 3)\n",
      "shape of one instance: (32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "# check data dimension\n",
    "print('training data shape: {}'.format(x_train.shape))\n",
    "print('test data shape: {}'.format(x_test.shape))\n",
    "print('shape of one instance: {}'.format(x_train.shape[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "EmIu1K_nQmii",
    "outputId": "1934453d-8660-4079-94d2-868e95894796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training labels are: [6, 9, 4, 1, 2, 7, 8, 3, 5, 0]\n",
      "# labels: 10\n"
     ]
    }
   ],
   "source": [
    "# check labels\n",
    "labels = []\n",
    "for y in y_train.flatten():\n",
    "    if y not in labels:\n",
    "        labels.append(y)\n",
    "print('training labels are: {}'.format(labels))\n",
    "print('# labels: {}'.format(len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v7qrl3ZWQmil"
   },
   "outputs": [],
   "source": [
    "cifar_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MVsVa-MnQmio"
   },
   "outputs": [],
   "source": [
    "# Convert class vectors to one-hot encoded vectors.\n",
    "y_train = to_categorical(y_train, num_classes = 10)\n",
    "y_test = to_categorical(y_test, num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9eNTR6MhQmiq"
   },
   "outputs": [],
   "source": [
    "# # Transform images from (32,32,3) to 3072-dimensional vectors (32*32*3)\n",
    "\n",
    "# x_train = np.reshape(x_train,(50000,3072))\n",
    "# x_test = np.reshape(x_test,(10000,3072))\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalization of pixel values (to [0-1] range)\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X5SUWnWPQmit"
   },
   "source": [
    "#### 2. HELP FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OFiGeH2kQmiu"
   },
   "outputs": [],
   "source": [
    "def plotAcc(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model_accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc = 'upper right')\n",
    "    \n",
    "def plotLoss(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model_loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc = 'upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OOHPAhjhQmix"
   },
   "source": [
    "#### 3. BUILD CNN CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HgU1PaN9Qmiy"
   },
   "outputs": [],
   "source": [
    "num_classes= 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ar26Z88Qmi3"
   },
   "source": [
    "##### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3502
    },
    "colab_type": "code",
    "id": "AwxCMLaEQmi5",
    "outputId": "dad01a0e-8e37-4723-861c-2e7eb272e676"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Epoch 1/100\n",
      "1562/1562 [==============================] - 165s 106ms/step - loss: 14.5005 - acc: 0.0999 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 2/100\n",
      "1562/1562 [==============================] - 164s 105ms/step - loss: 14.5080 - acc: 0.0999 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 3/100\n",
      "1562/1562 [==============================] - 166s 106ms/step - loss: 14.5058 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 4/100\n",
      "1562/1562 [==============================] - 164s 105ms/step - loss: 14.5025 - acc: 0.1002 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 5/100\n",
      "1562/1562 [==============================] - 165s 105ms/step - loss: 14.5096 - acc: 0.0998 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 6/100\n",
      "1562/1562 [==============================] - 165s 105ms/step - loss: 14.5048 - acc: 0.1001 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 7/100\n",
      "1562/1562 [==============================] - 165s 105ms/step - loss: 14.5064 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 8/100\n",
      "1562/1562 [==============================] - 167s 107ms/step - loss: 14.5051 - acc: 0.1001 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 9/100\n",
      "1562/1562 [==============================] - 165s 106ms/step - loss: 14.5083 - acc: 0.0999 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 10/100\n",
      "1562/1562 [==============================] - 168s 108ms/step - loss: 14.5083 - acc: 0.0999 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 11/100\n",
      "1562/1562 [==============================] - 165s 106ms/step - loss: 14.5035 - acc: 0.1002 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 12/100\n",
      "1562/1562 [==============================] - 165s 106ms/step - loss: 14.5022 - acc: 0.1003 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 13/100\n",
      "1562/1562 [==============================] - 166s 106ms/step - loss: 14.5067 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 14/100\n",
      "1562/1562 [==============================] - 166s 106ms/step - loss: 14.5096 - acc: 0.0998 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 15/100\n",
      "1562/1562 [==============================] - 165s 106ms/step - loss: 14.5045 - acc: 0.1001 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 16/100\n",
      "1562/1562 [==============================] - 166s 107ms/step - loss: 14.5071 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 17/100\n",
      "1562/1562 [==============================] - 166s 106ms/step - loss: 14.5025 - acc: 0.1002 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 18/100\n",
      "1562/1562 [==============================] - 166s 106ms/step - loss: 14.5100 - acc: 0.0998 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 19/100\n",
      "1562/1562 [==============================] - 166s 106ms/step - loss: 14.5029 - acc: 0.1002 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 20/100\n",
      "1562/1562 [==============================] - 167s 107ms/step - loss: 14.5090 - acc: 0.0998 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 21/100\n",
      "1562/1562 [==============================] - 166s 106ms/step - loss: 14.5019 - acc: 0.1003 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 22/100\n",
      "1562/1562 [==============================] - 167s 107ms/step - loss: 14.5135 - acc: 0.0996 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 23/100\n",
      "1562/1562 [==============================] - 167s 107ms/step - loss: 14.5025 - acc: 0.1002 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 24/100\n",
      "1562/1562 [==============================] - 167s 107ms/step - loss: 14.5071 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 25/100\n",
      "1562/1562 [==============================] - 167s 107ms/step - loss: 14.5051 - acc: 0.1001 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 26/100\n",
      "1562/1562 [==============================] - 167s 107ms/step - loss: 14.5116 - acc: 0.0997 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 27/100\n",
      "1562/1562 [==============================] - 167s 107ms/step - loss: 14.5042 - acc: 0.1001 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 28/100\n",
      "1562/1562 [==============================] - 167s 107ms/step - loss: 14.5038 - acc: 0.1002 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 29/100\n",
      "1562/1562 [==============================] - 168s 107ms/step - loss: 14.5067 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 30/100\n",
      "1562/1562 [==============================] - 168s 108ms/step - loss: 14.5058 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 31/100\n",
      "1562/1562 [==============================] - 168s 107ms/step - loss: 14.5064 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 32/100\n",
      "1562/1562 [==============================] - 167s 107ms/step - loss: 14.5042 - acc: 0.1001 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 33/100\n",
      "1562/1562 [==============================] - 169s 108ms/step - loss: 14.5106 - acc: 0.0997 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 34/100\n",
      "1562/1562 [==============================] - 167s 107ms/step - loss: 14.5077 - acc: 0.0999 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 35/100\n",
      "1562/1562 [==============================] - 168s 107ms/step - loss: 14.5006 - acc: 0.1004 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 36/100\n",
      "1562/1562 [==============================] - 168s 108ms/step - loss: 14.5116 - acc: 0.0997 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 37/100\n",
      "1562/1562 [==============================] - 168s 108ms/step - loss: 14.5035 - acc: 0.1002 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 38/100\n",
      "1562/1562 [==============================] - 169s 108ms/step - loss: 14.5013 - acc: 0.1003 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 39/100\n",
      "1562/1562 [==============================] - 169s 108ms/step - loss: 14.5209 - acc: 0.0991 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 40/100\n",
      "1562/1562 [==============================] - 169s 108ms/step - loss: 14.5003 - acc: 0.1004 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 41/100\n",
      "1562/1562 [==============================] - 169s 108ms/step - loss: 14.5009 - acc: 0.1003 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 42/100\n",
      "1562/1562 [==============================] - 171s 110ms/step - loss: 14.5093 - acc: 0.0998 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 43/100\n",
      "1562/1562 [==============================] - 169s 108ms/step - loss: 14.5045 - acc: 0.1001 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 44/100\n",
      "1562/1562 [==============================] - 169s 108ms/step - loss: 14.5129 - acc: 0.0996 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 45/100\n",
      "1562/1562 [==============================] - 169s 108ms/step - loss: 14.5006 - acc: 0.1004 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 46/100\n",
      "1562/1562 [==============================] - 170s 109ms/step - loss: 14.5054 - acc: 0.1001 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 47/100\n",
      "1562/1562 [==============================] - 169s 108ms/step - loss: 14.5125 - acc: 0.0996 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 48/100\n",
      "1562/1562 [==============================] - 170s 109ms/step - loss: 14.4984 - acc: 0.1005 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 49/100\n",
      "1562/1562 [==============================] - 169s 108ms/step - loss: 14.5074 - acc: 0.0999 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 50/100\n",
      "1562/1562 [==============================] - 169s 109ms/step - loss: 14.5064 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 51/100\n",
      "1562/1562 [==============================] - 169s 108ms/step - loss: 14.5077 - acc: 0.0999 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 52/100\n",
      "1562/1562 [==============================] - 173s 111ms/step - loss: 14.5096 - acc: 0.0998 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 53/100\n",
      "1562/1562 [==============================] - 177s 113ms/step - loss: 14.5071 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 54/100\n",
      "1562/1562 [==============================] - 177s 113ms/step - loss: 14.5067 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 55/100\n",
      "1562/1562 [==============================] - 176s 113ms/step - loss: 14.4967 - acc: 0.1006 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 56/100\n",
      "1562/1562 [==============================] - 177s 113ms/step - loss: 14.5122 - acc: 0.0996 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 57/100\n",
      "1562/1562 [==============================] - 177s 114ms/step - loss: 14.5025 - acc: 0.1002 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 171s 109ms/step - loss: 14.5074 - acc: 0.0999 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 59/100\n",
      "1562/1562 [==============================] - 171s 109ms/step - loss: 14.5119 - acc: 0.0997 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 60/100\n",
      "1562/1562 [==============================] - 172s 110ms/step - loss: 14.5000 - acc: 0.1004 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 61/100\n",
      "1562/1562 [==============================] - 171s 110ms/step - loss: 14.5038 - acc: 0.1002 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 62/100\n",
      "1562/1562 [==============================] - 171s 110ms/step - loss: 14.5064 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 63/100\n",
      "1562/1562 [==============================] - 172s 110ms/step - loss: 14.5119 - acc: 0.0997 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 64/100\n",
      "1562/1562 [==============================] - 172s 110ms/step - loss: 14.5074 - acc: 0.0999 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 65/100\n",
      "1562/1562 [==============================] - 172s 110ms/step - loss: 14.4996 - acc: 0.1004 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 66/100\n",
      "1562/1562 [==============================] - 172s 110ms/step - loss: 14.5148 - acc: 0.0995 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 67/100\n",
      "1562/1562 [==============================] - 173s 111ms/step - loss: 14.5071 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 68/100\n",
      "1562/1562 [==============================] - 171s 109ms/step - loss: 14.4958 - acc: 0.1007 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 69/100\n",
      "1562/1562 [==============================] - 166s 106ms/step - loss: 14.5196 - acc: 0.0992 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 70/100\n",
      "1562/1562 [==============================] - 166s 106ms/step - loss: 14.4967 - acc: 0.1006 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 71/100\n",
      "1562/1562 [==============================] - 166s 106ms/step - loss: 14.5122 - acc: 0.0996 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 72/100\n",
      "1562/1562 [==============================] - 167s 107ms/step - loss: 14.5077 - acc: 0.0999 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 73/100\n",
      "1562/1562 [==============================] - 167s 107ms/step - loss: 14.5045 - acc: 0.1001 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 74/100\n",
      "1562/1562 [==============================] - 168s 107ms/step - loss: 14.5093 - acc: 0.0998 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 75/100\n",
      "1562/1562 [==============================] - 167s 107ms/step - loss: 14.4987 - acc: 0.1005 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 76/100\n",
      "1562/1562 [==============================] - 167s 107ms/step - loss: 14.5109 - acc: 0.0997 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 77/100\n",
      "1562/1562 [==============================] - 167s 107ms/step - loss: 14.4980 - acc: 0.1005 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 78/100\n",
      "1562/1562 [==============================] - 168s 108ms/step - loss: 14.5177 - acc: 0.0993 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 79/100\n",
      "1562/1562 [==============================] - 168s 107ms/step - loss: 14.5038 - acc: 0.1002 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 80/100\n",
      "1562/1562 [==============================] - 168s 108ms/step - loss: 14.4996 - acc: 0.1004 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 81/100\n",
      "1562/1562 [==============================] - 168s 108ms/step - loss: 14.5080 - acc: 0.0999 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 82/100\n",
      "1562/1562 [==============================] - 167s 107ms/step - loss: 14.5083 - acc: 0.0999 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 83/100\n",
      "1562/1562 [==============================] - 167s 107ms/step - loss: 14.5064 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 84/100\n",
      "1562/1562 [==============================] - 168s 108ms/step - loss: 14.5032 - acc: 0.1002 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 85/100\n",
      "1562/1562 [==============================] - 168s 107ms/step - loss: 14.5183 - acc: 0.0993 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 86/100\n",
      "1562/1562 [==============================] - 177s 113ms/step - loss: 14.4938 - acc: 0.1008 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 87/100\n",
      "1562/1562 [==============================] - 175s 112ms/step - loss: 14.5058 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 88/100\n",
      "1562/1562 [==============================] - 176s 113ms/step - loss: 14.5164 - acc: 0.0994 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 89/100\n",
      "1562/1562 [==============================] - 176s 112ms/step - loss: 14.4987 - acc: 0.1005 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 90/100\n",
      "1562/1562 [==============================] - 176s 113ms/step - loss: 14.5116 - acc: 0.0997 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 91/100\n",
      "1562/1562 [==============================] - 176s 113ms/step - loss: 14.5003 - acc: 0.1004 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 92/100\n",
      "1562/1562 [==============================] - 176s 113ms/step - loss: 14.5019 - acc: 0.1003 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 93/100\n",
      "1562/1562 [==============================] - 175s 112ms/step - loss: 14.5167 - acc: 0.0994 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 94/100\n",
      "1562/1562 [==============================] - 176s 113ms/step - loss: 14.5080 - acc: 0.0999 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 95/100\n",
      "1562/1562 [==============================] - 175s 112ms/step - loss: 14.5090 - acc: 0.0998 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 96/100\n",
      "1562/1562 [==============================] - 175s 112ms/step - loss: 14.4990 - acc: 0.1005 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 97/100\n",
      "1562/1562 [==============================] - 175s 112ms/step - loss: 14.5013 - acc: 0.1003 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 98/100\n",
      "1562/1562 [==============================] - 176s 113ms/step - loss: 14.5083 - acc: 0.0999 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 99/100\n",
      "1562/1562 [==============================] - 175s 112ms/step - loss: 14.5193 - acc: 0.0992 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 100/100\n",
      "1562/1562 [==============================] - 176s 112ms/step - loss: 14.4922 - acc: 0.1009 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Saved trained model at /Users/kevinshi721/Downloads/saved_models/keras_cifar10_trained_model.h5 \n",
      "10000/10000 [==============================] - 10s 964us/step\n",
      "Test loss: 14.506285690307617\n",
      "Test accuracy: 0.1\n"
     ]
    }
   ],
   "source": [
    "# hyper parameters to be tuned\n",
    "n_neuron = 256\n",
    "batch_size = 32\n",
    "dropout_rate = 0.2\n",
    "data_augmentation = True\n",
    "epochs = 100\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# set up model structure\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.01, decay=1e-6)\n",
    "    \n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              steps_per_epoch = int(50000/batch_size),\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    " \n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    exp_optimizer_r = model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                                   batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        steps_per_epoch = int(50000/batch_size),\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3502
    },
    "colab_type": "code",
    "id": "Yj6Ws7NMQmi9",
    "outputId": "a7aed1f3-1a86-4a8d-b934-3c2dd78688f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Epoch 1/100\n",
      "1562/1562 [==============================] - 171s 109ms/step - loss: 2.3107 - acc: 0.0976 - val_loss: 2.3036 - val_acc: 0.1000\n",
      "Epoch 2/100\n",
      "1562/1562 [==============================] - 170s 109ms/step - loss: 2.3040 - acc: 0.0997 - val_loss: 2.3035 - val_acc: 0.1000\n",
      "Epoch 3/100\n",
      "1562/1562 [==============================] - 169s 108ms/step - loss: 2.3040 - acc: 0.0996 - val_loss: 2.3032 - val_acc: 0.1000\n",
      "Epoch 4/100\n",
      "1562/1562 [==============================] - 170s 109ms/step - loss: 2.3039 - acc: 0.0994 - val_loss: 2.3029 - val_acc: 0.1000\n",
      "Epoch 5/100\n",
      "1562/1562 [==============================] - 171s 109ms/step - loss: 2.3041 - acc: 0.0980 - val_loss: 2.3034 - val_acc: 0.1000\n",
      "Epoch 6/100\n",
      "1562/1562 [==============================] - 170s 109ms/step - loss: 2.3039 - acc: 0.1002 - val_loss: 2.3038 - val_acc: 0.1000\n",
      "Epoch 7/100\n",
      "1562/1562 [==============================] - 171s 110ms/step - loss: 2.3039 - acc: 0.1015 - val_loss: 2.3036 - val_acc: 0.1000\n",
      "Epoch 8/100\n",
      "1562/1562 [==============================] - 171s 109ms/step - loss: 2.3041 - acc: 0.1007 - val_loss: 2.3030 - val_acc: 0.1000\n",
      "Epoch 9/100\n",
      "1562/1562 [==============================] - 171s 109ms/step - loss: 2.3040 - acc: 0.1005 - val_loss: 2.3043 - val_acc: 0.1000\n",
      "Epoch 10/100\n",
      "1562/1562 [==============================] - 171s 109ms/step - loss: 2.3039 - acc: 0.0972 - val_loss: 2.3037 - val_acc: 0.1000\n",
      "Epoch 11/100\n",
      "1562/1562 [==============================] - 171s 110ms/step - loss: 2.3040 - acc: 0.1004 - val_loss: 2.3030 - val_acc: 0.1000\n",
      "Epoch 12/100\n",
      "1562/1562 [==============================] - 171s 109ms/step - loss: 2.3040 - acc: 0.1003 - val_loss: 2.3046 - val_acc: 0.1000\n",
      "Epoch 13/100\n",
      "1562/1562 [==============================] - 172s 110ms/step - loss: 2.3040 - acc: 0.0986 - val_loss: 2.3034 - val_acc: 0.1000\n",
      "Epoch 14/100\n",
      "1562/1562 [==============================] - 172s 110ms/step - loss: 2.3039 - acc: 0.1004 - val_loss: 2.3042 - val_acc: 0.1000\n",
      "Epoch 15/100\n",
      "1562/1562 [==============================] - 172s 110ms/step - loss: 2.3039 - acc: 0.0995 - val_loss: 2.3028 - val_acc: 0.1000\n",
      "Epoch 16/100\n",
      "1562/1562 [==============================] - 172s 110ms/step - loss: 2.3039 - acc: 0.1011 - val_loss: 2.3049 - val_acc: 0.1000\n",
      "Epoch 17/100\n",
      "1562/1562 [==============================] - 172s 110ms/step - loss: 2.3039 - acc: 0.1005 - val_loss: 2.3030 - val_acc: 0.1000\n",
      "Epoch 18/100\n",
      "1562/1562 [==============================] - 172s 110ms/step - loss: 2.3040 - acc: 0.1010 - val_loss: 2.3033 - val_acc: 0.1000\n",
      "Epoch 19/100\n",
      "1562/1562 [==============================] - 172s 110ms/step - loss: 2.3038 - acc: 0.0986 - val_loss: 2.3050 - val_acc: 0.1000\n",
      "Epoch 20/100\n",
      "1562/1562 [==============================] - 172s 110ms/step - loss: 2.3041 - acc: 0.0986 - val_loss: 2.3036 - val_acc: 0.1000\n",
      "Epoch 21/100\n",
      "1562/1562 [==============================] - 172s 110ms/step - loss: 2.3040 - acc: 0.1000 - val_loss: 2.3036 - val_acc: 0.1000\n",
      "Epoch 22/100\n",
      "1562/1562 [==============================] - 173s 111ms/step - loss: 2.3042 - acc: 0.0981 - val_loss: 2.3033 - val_acc: 0.1000\n",
      "Epoch 23/100\n",
      "1562/1562 [==============================] - 172s 110ms/step - loss: 2.3038 - acc: 0.0995 - val_loss: 2.3052 - val_acc: 0.1000\n",
      "Epoch 24/100\n",
      "1562/1562 [==============================] - 173s 111ms/step - loss: 2.3041 - acc: 0.0971 - val_loss: 2.3033 - val_acc: 0.1000\n",
      "Epoch 25/100\n",
      "1562/1562 [==============================] - 173s 111ms/step - loss: 2.3041 - acc: 0.0967 - val_loss: 2.3033 - val_acc: 0.1000\n",
      "Epoch 26/100\n",
      "1562/1562 [==============================] - 174s 111ms/step - loss: 2.3041 - acc: 0.0991 - val_loss: 2.3045 - val_acc: 0.1000\n",
      "Epoch 27/100\n",
      "1562/1562 [==============================] - 173s 111ms/step - loss: 2.3041 - acc: 0.0977 - val_loss: 2.3041 - val_acc: 0.1000\n",
      "Epoch 28/100\n",
      "1562/1562 [==============================] - 173s 111ms/step - loss: 2.3041 - acc: 0.0976 - val_loss: 2.3040 - val_acc: 0.1000\n",
      "Epoch 29/100\n",
      "1562/1562 [==============================] - 174s 111ms/step - loss: 2.3039 - acc: 0.1011 - val_loss: 2.3042 - val_acc: 0.1000\n",
      "Epoch 30/100\n",
      "1562/1562 [==============================] - 174s 111ms/step - loss: 2.3039 - acc: 0.0994 - val_loss: 2.3033 - val_acc: 0.1000\n",
      "Epoch 31/100\n",
      "1562/1562 [==============================] - 173s 111ms/step - loss: 2.3039 - acc: 0.1012 - val_loss: 2.3048 - val_acc: 0.1000\n",
      "Epoch 32/100\n",
      "1562/1562 [==============================] - 174s 112ms/step - loss: 2.3040 - acc: 0.1005 - val_loss: 2.3035 - val_acc: 0.1000\n",
      "Epoch 33/100\n",
      "1562/1562 [==============================] - 173s 110ms/step - loss: 2.3039 - acc: 0.1010 - val_loss: 2.3044 - val_acc: 0.1000\n",
      "Epoch 34/100\n",
      "1562/1562 [==============================] - 174s 111ms/step - loss: 2.3039 - acc: 0.1015 - val_loss: 2.3037 - val_acc: 0.1000\n",
      "Epoch 35/100\n",
      "1562/1562 [==============================] - 173s 111ms/step - loss: 2.3040 - acc: 0.0985 - val_loss: 2.3041 - val_acc: 0.1000\n",
      "Epoch 36/100\n",
      "1562/1562 [==============================] - 174s 111ms/step - loss: 2.3040 - acc: 0.0985 - val_loss: 2.3038 - val_acc: 0.1000\n",
      "Epoch 37/100\n",
      "1562/1562 [==============================] - 173s 111ms/step - loss: 2.3040 - acc: 0.0984 - val_loss: 2.3032 - val_acc: 0.1000\n",
      "Epoch 38/100\n",
      "1562/1562 [==============================] - 173s 111ms/step - loss: 2.3041 - acc: 0.0973 - val_loss: 2.3033 - val_acc: 0.1000\n",
      "Epoch 39/100\n",
      "1562/1562 [==============================] - 182s 117ms/step - loss: 2.3041 - acc: 0.0995 - val_loss: 2.3032 - val_acc: 0.1000\n",
      "Epoch 40/100\n",
      "1562/1562 [==============================] - 184s 118ms/step - loss: 2.3041 - acc: 0.0972 - val_loss: 2.3037 - val_acc: 0.1000\n",
      "Epoch 41/100\n",
      "1562/1562 [==============================] - 171s 110ms/step - loss: 2.3041 - acc: 0.0987 - val_loss: 2.3038 - val_acc: 0.1000\n",
      "Epoch 42/100\n",
      "1562/1562 [==============================] - 169s 108ms/step - loss: 2.3039 - acc: 0.0994 - val_loss: 2.3048 - val_acc: 0.1000\n",
      "Epoch 43/100\n",
      "1562/1562 [==============================] - 206s 132ms/step - loss: 2.3042 - acc: 0.0997 - val_loss: 2.3030 - val_acc: 0.1000\n",
      "Epoch 44/100\n",
      "1562/1562 [==============================] - 209s 134ms/step - loss: 2.3039 - acc: 0.1024 - val_loss: 2.3045 - val_acc: 0.1000\n",
      "Epoch 45/100\n",
      "1562/1562 [==============================] - 201s 129ms/step - loss: 2.3041 - acc: 0.0985 - val_loss: 2.3032 - val_acc: 0.1000\n",
      "Epoch 46/100\n",
      "1562/1562 [==============================] - 198s 127ms/step - loss: 2.3040 - acc: 0.0987 - val_loss: 2.3043 - val_acc: 0.1000\n",
      "Epoch 47/100\n",
      "1562/1562 [==============================] - 198s 127ms/step - loss: 2.3040 - acc: 0.0998 - val_loss: 2.3040 - val_acc: 0.1000\n",
      "Epoch 48/100\n",
      "1562/1562 [==============================] - 197s 126ms/step - loss: 2.3039 - acc: 0.0986 - val_loss: 2.3043 - val_acc: 0.1000\n",
      "Epoch 49/100\n",
      "1562/1562 [==============================] - 199s 127ms/step - loss: 2.3040 - acc: 0.1006 - val_loss: 2.3045 - val_acc: 0.1000\n",
      "Epoch 50/100\n",
      "1562/1562 [==============================] - 183s 117ms/step - loss: 2.3040 - acc: 0.0993 - val_loss: 2.3039 - val_acc: 0.1000\n",
      "Epoch 51/100\n",
      "1562/1562 [==============================] - 183s 117ms/step - loss: 2.3040 - acc: 0.0979 - val_loss: 2.3039 - val_acc: 0.1000\n",
      "Epoch 52/100\n",
      "1562/1562 [==============================] - 183s 117ms/step - loss: 2.3039 - acc: 0.0989 - val_loss: 2.3041 - val_acc: 0.1000\n",
      "Epoch 53/100\n",
      "1562/1562 [==============================] - 183s 117ms/step - loss: 2.3041 - acc: 0.0996 - val_loss: 2.3037 - val_acc: 0.1000\n",
      "Epoch 54/100\n",
      "1562/1562 [==============================] - 182s 117ms/step - loss: 2.3039 - acc: 0.1014 - val_loss: 2.3034 - val_acc: 0.1000\n",
      "Epoch 55/100\n",
      "1562/1562 [==============================] - 184s 117ms/step - loss: 2.3041 - acc: 0.0977 - val_loss: 2.3030 - val_acc: 0.1000\n",
      "Epoch 56/100\n",
      "1562/1562 [==============================] - 183s 117ms/step - loss: 2.3039 - acc: 0.1005 - val_loss: 2.3047 - val_acc: 0.1000\n",
      "Epoch 57/100\n",
      "1562/1562 [==============================] - 183s 117ms/step - loss: 2.3041 - acc: 0.1006 - val_loss: 2.3044 - val_acc: 0.1000\n",
      "Epoch 58/100\n",
      "1562/1562 [==============================] - 183s 117ms/step - loss: 2.3039 - acc: 0.0993 - val_loss: 2.3035 - val_acc: 0.1000\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 170s 109ms/step - loss: 2.3039 - acc: 0.1010 - val_loss: 2.3046 - val_acc: 0.1000\n",
      "Epoch 60/100\n",
      "1562/1562 [==============================] - 170s 109ms/step - loss: 2.3040 - acc: 0.0989 - val_loss: 2.3051 - val_acc: 0.1000\n",
      "Epoch 61/100\n",
      "1562/1562 [==============================] - 171s 109ms/step - loss: 2.3041 - acc: 0.0969 - val_loss: 2.3034 - val_acc: 0.1000\n",
      "Epoch 62/100\n",
      "1562/1562 [==============================] - 170s 109ms/step - loss: 2.3039 - acc: 0.1008 - val_loss: 2.3039 - val_acc: 0.1000\n",
      "Epoch 63/100\n",
      "1562/1562 [==============================] - 172s 110ms/step - loss: 2.3042 - acc: 0.0965 - val_loss: 2.3032 - val_acc: 0.1000\n",
      "Epoch 64/100\n",
      "1562/1562 [==============================] - 171s 109ms/step - loss: 2.3041 - acc: 0.0996 - val_loss: 2.3030 - val_acc: 0.1000\n",
      "Epoch 65/100\n",
      "1562/1562 [==============================] - 170s 109ms/step - loss: 2.3040 - acc: 0.1006 - val_loss: 2.3032 - val_acc: 0.1000\n",
      "Epoch 66/100\n",
      "1562/1562 [==============================] - 171s 110ms/step - loss: 2.3040 - acc: 0.1000 - val_loss: 2.3040 - val_acc: 0.1000\n",
      "Epoch 67/100\n",
      "1562/1562 [==============================] - 171s 109ms/step - loss: 2.3041 - acc: 0.0992 - val_loss: 2.3034 - val_acc: 0.1000\n",
      "Epoch 68/100\n",
      "1562/1562 [==============================] - 171s 110ms/step - loss: 2.3039 - acc: 0.1014 - val_loss: 2.3045 - val_acc: 0.1000\n",
      "Epoch 69/100\n",
      "1562/1562 [==============================] - 171s 110ms/step - loss: 2.3041 - acc: 0.0995 - val_loss: 2.3035 - val_acc: 0.1000\n",
      "Epoch 70/100\n",
      "1562/1562 [==============================] - 172s 110ms/step - loss: 2.3041 - acc: 0.0988 - val_loss: 2.3038 - val_acc: 0.1000\n",
      "Epoch 71/100\n",
      "1562/1562 [==============================] - 172s 110ms/step - loss: 2.3040 - acc: 0.0997 - val_loss: 2.3037 - val_acc: 0.1000\n",
      "Epoch 72/100\n",
      "1562/1562 [==============================] - 179s 115ms/step - loss: 2.3038 - acc: 0.1015 - val_loss: 2.3037 - val_acc: 0.1000\n",
      "Epoch 73/100\n",
      "1562/1562 [==============================] - 180s 115ms/step - loss: 2.3039 - acc: 0.1011 - val_loss: 2.3034 - val_acc: 0.1000\n",
      "Epoch 74/100\n",
      "1562/1562 [==============================] - 187s 120ms/step - loss: 2.3039 - acc: 0.0998 - val_loss: 2.3040 - val_acc: 0.1000\n",
      "Epoch 75/100\n",
      "1562/1562 [==============================] - 219s 140ms/step - loss: 2.3041 - acc: 0.0977 - val_loss: 2.3046 - val_acc: 0.1000\n",
      "Epoch 76/100\n",
      "1562/1562 [==============================] - 218s 139ms/step - loss: 2.3041 - acc: 0.0988 - val_loss: 2.3036 - val_acc: 0.1000\n",
      "Epoch 77/100\n",
      "1562/1562 [==============================] - 212s 136ms/step - loss: 2.3040 - acc: 0.1017 - val_loss: 2.3034 - val_acc: 0.1000\n",
      "Epoch 78/100\n",
      "1562/1562 [==============================] - 204s 131ms/step - loss: 2.3040 - acc: 0.1014 - val_loss: 2.3036 - val_acc: 0.1000\n",
      "Epoch 79/100\n",
      "1562/1562 [==============================] - 220s 141ms/step - loss: 2.3040 - acc: 0.0998 - val_loss: 2.3034 - val_acc: 0.1000\n",
      "Epoch 80/100\n",
      "1562/1562 [==============================] - 226s 145ms/step - loss: 2.3039 - acc: 0.1008 - val_loss: 2.3032 - val_acc: 0.1000\n",
      "Epoch 81/100\n",
      "1562/1562 [==============================] - 206s 132ms/step - loss: 2.3039 - acc: 0.0983 - val_loss: 2.3044 - val_acc: 0.1000\n",
      "Epoch 82/100\n",
      "1562/1562 [==============================] - 208s 133ms/step - loss: 2.3040 - acc: 0.0992 - val_loss: 2.3042 - val_acc: 0.1000\n",
      "Epoch 83/100\n",
      "1562/1562 [==============================] - 194s 124ms/step - loss: 2.3040 - acc: 0.0990 - val_loss: 2.3030 - val_acc: 0.1000\n",
      "Epoch 84/100\n",
      "1562/1562 [==============================] - 203s 130ms/step - loss: 2.3040 - acc: 0.0993 - val_loss: 2.3036 - val_acc: 0.1000\n",
      "Epoch 85/100\n",
      "1562/1562 [==============================] - 213s 137ms/step - loss: 2.3040 - acc: 0.0991 - val_loss: 2.3040 - val_acc: 0.1000\n",
      "Epoch 86/100\n",
      "1562/1562 [==============================] - 221s 141ms/step - loss: 2.3041 - acc: 0.1007 - val_loss: 2.3035 - val_acc: 0.1000\n",
      "Epoch 87/100\n",
      "1562/1562 [==============================] - 223s 143ms/step - loss: 2.3039 - acc: 0.1003 - val_loss: 2.3041 - val_acc: 0.1000\n",
      "Epoch 88/100\n",
      "1562/1562 [==============================] - 222s 142ms/step - loss: 2.3039 - acc: 0.1015 - val_loss: 2.3047 - val_acc: 0.1000\n",
      "Epoch 89/100\n",
      "1562/1562 [==============================] - 210s 135ms/step - loss: 2.3038 - acc: 0.1017 - val_loss: 2.3044 - val_acc: 0.1000\n",
      "Epoch 90/100\n",
      "1562/1562 [==============================] - 219s 140ms/step - loss: 2.3039 - acc: 0.0976 - val_loss: 2.3029 - val_acc: 0.1000\n",
      "Epoch 91/100\n",
      "1562/1562 [==============================] - 213s 136ms/step - loss: 2.3039 - acc: 0.0995 - val_loss: 2.3047 - val_acc: 0.1000\n",
      "Epoch 92/100\n",
      "1562/1562 [==============================] - 199s 128ms/step - loss: 2.3041 - acc: 0.1006 - val_loss: 2.3035 - val_acc: 0.1000\n",
      "Epoch 93/100\n",
      "1562/1562 [==============================] - 214s 137ms/step - loss: 2.3041 - acc: 0.0977 - val_loss: 2.3034 - val_acc: 0.1000\n",
      "Epoch 94/100\n",
      "1562/1562 [==============================] - 201s 129ms/step - loss: 2.3040 - acc: 0.0992 - val_loss: 2.3039 - val_acc: 0.1000\n",
      "Epoch 95/100\n",
      "1562/1562 [==============================] - 169s 108ms/step - loss: 2.3040 - acc: 0.0994 - val_loss: 2.3039 - val_acc: 0.1000\n",
      "Epoch 96/100\n",
      "1562/1562 [==============================] - 170s 109ms/step - loss: 2.3042 - acc: 0.0986 - val_loss: 2.3036 - val_acc: 0.1000\n",
      "Epoch 97/100\n",
      "1562/1562 [==============================] - 169s 108ms/step - loss: 2.3042 - acc: 0.0967 - val_loss: 2.3035 - val_acc: 0.1000\n",
      "Epoch 98/100\n",
      "1562/1562 [==============================] - 170s 109ms/step - loss: 2.3040 - acc: 0.0987 - val_loss: 2.3043 - val_acc: 0.1000\n",
      "Epoch 99/100\n",
      "1562/1562 [==============================] - 208s 133ms/step - loss: 2.3041 - acc: 0.0980 - val_loss: 2.3036 - val_acc: 0.1000\n",
      "Epoch 100/100\n",
      "1562/1562 [==============================] - 202s 129ms/step - loss: 2.3040 - acc: 0.1004 - val_loss: 2.3046 - val_acc: 0.1000\n",
      "Saved trained model at /Users/kevinshi721/Downloads/saved_models_2/keras_cifar10_trained_model_2.h5 \n",
      "10000/10000 [==============================] - 9s 926us/step\n",
      "Test loss: 2.3046276069641114\n",
      "Test accuracy: 0.1\n"
     ]
    }
   ],
   "source": [
    "# hyper parameters to be tuned\n",
    "n_neuron = 256\n",
    "batch_size = 32\n",
    "dropout_rate = 0.2\n",
    "data_augmentation = True\n",
    "epochs = 100\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models_2')\n",
    "model_name = 'keras_cifar10_trained_model_2.h5'\n",
    "\n",
    "# set up model structure\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "  \n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    " \n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    exp_optimizer_a = model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        steps_per_epoch = int(50000/batch_size),\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment1_Part2_0.001.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
