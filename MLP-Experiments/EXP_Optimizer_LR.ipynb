{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OBtuUR3t_BRI"
   },
   "source": [
    "### 1. IMPORT LIBRARIES AND CIFAR 10, PREPARE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nvP4aa6j_BRK",
    "outputId": "f54d7d3c-e834-4078-9a28-43eae1a892f3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# general\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "import keras\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "# dataset\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# modeling tools\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UStl-r6A_BRR"
   },
   "source": [
    "Check and Prepare Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "ehvzlp3z_BRS",
    "outputId": "44b3cff1-0b40-4e38-f502-7b46d24a01dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 29s 0us/step\n",
      "training data shape: (50000, 32, 32, 3)\n",
      "test data shape: (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "# check data dimension\n",
    "print('training data shape: {}'.format(x_train.shape))\n",
    "print('test data shape: {}'.format(x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "3nNo-3oP_BRX",
    "outputId": "100c4508-bfa9-4317-ee6d-f837e2ba3bea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training labels are: [6, 9, 4, 1, 2, 7, 8, 3, 5, 0]\n",
      "# labels: 10\n"
     ]
    }
   ],
   "source": [
    "# check labels\n",
    "labels = []\n",
    "for y in y_train.flatten():\n",
    "    if y not in labels:\n",
    "        labels.append(y)\n",
    "print('training labels are: {}'.format(labels))\n",
    "print('# labels: {}'.format(len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wmIhaSHI_BRd"
   },
   "outputs": [],
   "source": [
    "# Convert class vectors to one-hot encoded vectors.\n",
    "y_train = to_categorical(y_train, num_classes = 10)\n",
    "y_test = to_categorical(y_test, num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nvRGZnnb_BRf"
   },
   "outputs": [],
   "source": [
    "# Transform images from (32,32,3) to 3072-dimensional vectors (32*32*3)\n",
    "\n",
    "x_train = np.reshape(x_train,(50000,3072))\n",
    "x_test = np.reshape(x_test,(10000,3072))\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalization of pixel values (to [0-1] range)\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m1Wr2pAL_BRi"
   },
   "source": [
    "### 2. HELP FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kIMZCmmJ_BRk"
   },
   "outputs": [],
   "source": [
    "# plot the accuracy and loss of training process\n",
    "\n",
    "def plotAcc(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model_accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc = 'lower right')\n",
    "    \n",
    "def plotLoss(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model_loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc = 'upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EatQgplXoG_4"
   },
   "outputs": [],
   "source": [
    "# get the best accuracy result\n",
    "\n",
    "def getBest(record):\n",
    "    max_acc = 0\n",
    "    experiment = None\n",
    "    for value in record.values():\n",
    "        max_acc = max(max_acc, value[1])\n",
    "\n",
    "    for key in record.keys():\n",
    "        if record[key][1] == max_acc:\n",
    "            experiment = key\n",
    "      \n",
    "    return max_acc, experiment, record[experiment][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AbWr1lnRy9nD"
   },
   "outputs": [],
   "source": [
    "# get the summary of all experiments\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def get_allResult(record):\n",
    "    df_result = pd.DataFrame()\n",
    "    length = len(list(record.keys()))\n",
    "    df_result['Experiment'] = list(record.keys())\n",
    "    df_result['Loss'] = [list(record.values())[i][0] for i in range(length)]\n",
    "    df_result['Accuracy'] = [list(record.values())[i][1] for i in range(length)]\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "28f9kZE__BRn"
   },
   "source": [
    "### 3. EXPERIMENTS - TEST WITH OPTIMIZER & LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3-HRmKY8_BRn"
   },
   "source": [
    "#### 1. Set up hyperparameters for optimizer & learnig rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Sp6dvlsE_BRo",
    "outputId": "e5dda18a-7df9-4cd2-9972-cd2780a6ade3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SGD with learning rate @ 1e-05', 'Momentum with learning rate @ 1e-05', 'RMSprop with learning rate @ 1e-05', 'Adam with learning rate @ 1e-05', 'Adagrad with learning rate @ 1e-05', 'Adadelta with learning rate @ 1e-05']\n"
     ]
    }
   ],
   "source": [
    "# record optimizer information in string list\n",
    "list_lr = [0.00001, 0.00003, 0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1]\n",
    "list_opt_name = ['SGD', 'Momentum', 'RMSprop', 'Adam', 'Adagrad', 'Adadelta']\n",
    "\n",
    "list_test = []\n",
    "for i in list_lr:\n",
    "    for j in list_opt_name:\n",
    "        list_test += [str(j) + ' with learning rate @ ' + str(i)]\n",
    "        \n",
    "print(list_test[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "AEftvPK1_BRt",
    "outputId": "437b8e4d-40fa-47d2-e244-ce0f938f7612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<keras.optimizers.SGD object at 0x7fb5987d66d8>, <keras.optimizers.SGD object at 0x7fb5d67c5c88>, <keras.optimizers.RMSprop object at 0x7fb5d67c5828>, <keras.optimizers.Adam object at 0x7fb5d67c5a20>, <keras.optimizers.Adagrad object at 0x7fb5d67c5dd8>, <keras.optimizers.Adadelta object at 0x7fb59879beb8>]\n"
     ]
    }
   ],
   "source": [
    "# build list of optimizers\n",
    "list_optimizer = []\n",
    "for i in list_lr:\n",
    "    list_optimizer += \\\n",
    "    [keras.optimizers.SGD(lr = i)] +  \\\n",
    "    [keras.optimizers.SGD(lr = i, momentum = 0.9)] + \\\n",
    "    [keras.optimizers.RMSprop(lr = i)] + \\\n",
    "    [keras.optimizers.Adam(lr = i)] + \\\n",
    "    [keras.optimizers.Adagrad(lr = i)] + \\\n",
    "    [keras.optimizers.Adadelta(lr = i)]\n",
    "    \n",
    "print(list_optimizer[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kLjOareg_BRw"
   },
   "source": [
    "#### 2. Control other conditions and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VHCIlZ3F_BRx"
   },
   "outputs": [],
   "source": [
    "# set conditions\n",
    "n_class = 10\n",
    "input_dimension = 32*32*3\n",
    "\n",
    "# hyper parameters\n",
    "n_neuron = 256\n",
    "batch_size = 128\n",
    "dropout_rate = 0.2\n",
    "epochs = 20\n",
    "activation = 'relu'\n",
    "\n",
    "# record hyper parameter information in dictionary                 \n",
    "param_dict = {\n",
    "        'num_neurons': 256,\n",
    "        'num_layers ': 2,\n",
    "        'batch_size': 128,\n",
    "        'dropout_rate': 0.2,\n",
    "        'epochs': 20,\n",
    "        'activation function': 'relu'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RizsvpzP_BRz"
   },
   "source": [
    "#### 3. Set up experiment function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zhA0wjEV_BR0"
   },
   "outputs": [],
   "source": [
    "def experiment_optimizer(list_optimizer):\n",
    "    \n",
    "    record = {}    \n",
    "    for i, _ in enumerate(list_optimizer):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(n_neuron, activation = activation, input_dim = input_dimension))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(n_neuron, activation = activation))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(n_class, activation = 'softmax'))\n",
    "\n",
    "        model.compile(optimizer = _,\n",
    "                     loss = 'categorical_crossentropy',\n",
    "                     metrics = ['accuracy'])\n",
    "\n",
    "        start = time.clock()\n",
    "        exp_optimizer = model.fit(x_train, \n",
    "                                  y_train, \n",
    "                                  epochs = epochs, \n",
    "                                  batch_size = batch_size,\n",
    "                                  verbose = 2,\n",
    "                                  validation_data = (x_test, y_test))\n",
    "        elapsed = (time.clock() - start)\n",
    "        scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "        \n",
    "        # record experiment information:\n",
    "        record[str(list_test[i])] = [scores[0], scores[1], elapsed, exp_optimizer]\n",
    "        \n",
    "        # print experiment name:\n",
    "        print('*******************************************************')\n",
    "        print('=======================================================')\n",
    "        print('Experiment' + str(i) + ':'  + '\\t' + str(list_test[i]))\n",
    "        print('=======================================================')\n",
    "        # print all used parameters for this model\n",
    "#         for i,j in param_dict.items():\n",
    "#             print(str(i) + '\\t' + str(j)) \n",
    "#             print('-------------------------------------------------------')\n",
    "            \n",
    "        # print running time used\n",
    "        print('Time Used: {}'.format(elapsed))\n",
    "        print('-------------------------------------------------------')\n",
    "        \n",
    "        # print best loss and accuracy result\n",
    "        print('Test loss:', scores[0])\n",
    "        print('-------------------------------------------------------')\n",
    "        print('Test accuracy:', scores[1])\n",
    "        \n",
    "        print('*******************************************************')\n",
    "\n",
    "        \n",
    "    return record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wphu8ICg_BR3"
   },
   "source": [
    "#### 4. Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 47753
    },
    "colab_type": "code",
    "id": "edQXYnwU_BR4",
    "outputId": "bf3f27ba-5141-4d73-ce99-457761936087"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 5s - loss: 2.4596 - acc: 0.0998 - val_loss: 2.3952 - val_acc: 0.0985\n",
      "Epoch 2/20\n",
      " - 2s - loss: 2.4374 - acc: 0.1000 - val_loss: 2.3765 - val_acc: 0.0980\n",
      "Epoch 3/20\n",
      " - 2s - loss: 2.4194 - acc: 0.0984 - val_loss: 2.3624 - val_acc: 0.0966\n",
      "Epoch 4/20\n",
      " - 2s - loss: 2.4072 - acc: 0.1015 - val_loss: 2.3511 - val_acc: 0.0957\n",
      "Epoch 5/20\n",
      " - 2s - loss: 2.3933 - acc: 0.0999 - val_loss: 2.3419 - val_acc: 0.0968\n",
      "Epoch 6/20\n",
      " - 2s - loss: 2.3821 - acc: 0.1018 - val_loss: 2.3343 - val_acc: 0.0981\n",
      "Epoch 7/20\n",
      " - 2s - loss: 2.3770 - acc: 0.0987 - val_loss: 2.3278 - val_acc: 0.1033\n",
      "Epoch 8/20\n",
      " - 2s - loss: 2.3690 - acc: 0.1033 - val_loss: 2.3221 - val_acc: 0.1094\n",
      "Epoch 9/20\n",
      " - 2s - loss: 2.3634 - acc: 0.1029 - val_loss: 2.3172 - val_acc: 0.1138\n",
      "Epoch 10/20\n",
      " - 2s - loss: 2.3584 - acc: 0.1046 - val_loss: 2.3128 - val_acc: 0.1186\n",
      "Epoch 11/20\n",
      " - 2s - loss: 2.3531 - acc: 0.1063 - val_loss: 2.3088 - val_acc: 0.1205\n",
      "Epoch 12/20\n",
      " - 2s - loss: 2.3506 - acc: 0.1051 - val_loss: 2.3053 - val_acc: 0.1244\n",
      "Epoch 13/20\n",
      " - 2s - loss: 2.3445 - acc: 0.1095 - val_loss: 2.3020 - val_acc: 0.1271\n",
      "Epoch 14/20\n",
      " - 2s - loss: 2.3428 - acc: 0.1098 - val_loss: 2.2990 - val_acc: 0.1304\n",
      "Epoch 15/20\n",
      " - 2s - loss: 2.3385 - acc: 0.1102 - val_loss: 2.2962 - val_acc: 0.1317\n",
      "Epoch 16/20\n",
      " - 2s - loss: 2.3358 - acc: 0.1110 - val_loss: 2.2936 - val_acc: 0.1333\n",
      "Epoch 17/20\n",
      " - 2s - loss: 2.3310 - acc: 0.1126 - val_loss: 2.2911 - val_acc: 0.1359\n",
      "Epoch 18/20\n",
      " - 2s - loss: 2.3284 - acc: 0.1137 - val_loss: 2.2888 - val_acc: 0.1362\n",
      "Epoch 19/20\n",
      " - 2s - loss: 2.3268 - acc: 0.1143 - val_loss: 2.2866 - val_acc: 0.1372\n",
      "Epoch 20/20\n",
      " - 2s - loss: 2.3254 - acc: 0.1147 - val_loss: 2.2845 - val_acc: 0.1375\n",
      "10000/10000 [==============================] - 1s 58us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment0:\tSGD with learning rate @ 1e-05\n",
      "=======================================================\n",
      "Time Used: 57.494068\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.2845199951171873\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1375\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 2s - loss: 2.3715 - acc: 0.1050 - val_loss: 2.2951 - val_acc: 0.1310\n",
      "Epoch 2/20\n",
      " - 2s - loss: 2.3233 - acc: 0.1188 - val_loss: 2.2746 - val_acc: 0.1512\n",
      "Epoch 3/20\n",
      " - 2s - loss: 2.3059 - acc: 0.1274 - val_loss: 2.2605 - val_acc: 0.1546\n",
      "Epoch 4/20\n",
      " - 2s - loss: 2.2902 - acc: 0.1370 - val_loss: 2.2476 - val_acc: 0.1631\n",
      "Epoch 5/20\n",
      " - 2s - loss: 2.2764 - acc: 0.1439 - val_loss: 2.2358 - val_acc: 0.1729\n",
      "Epoch 6/20\n",
      " - 2s - loss: 2.2607 - acc: 0.1541 - val_loss: 2.2249 - val_acc: 0.1812\n",
      "Epoch 7/20\n",
      " - 2s - loss: 2.2527 - acc: 0.1597 - val_loss: 2.2148 - val_acc: 0.1910\n",
      "Epoch 8/20\n",
      " - 2s - loss: 2.2420 - acc: 0.1663 - val_loss: 2.2053 - val_acc: 0.1979\n",
      "Epoch 9/20\n",
      " - 2s - loss: 2.2323 - acc: 0.1712 - val_loss: 2.1961 - val_acc: 0.2026\n",
      "Epoch 10/20\n",
      " - 2s - loss: 2.2219 - acc: 0.1795 - val_loss: 2.1871 - val_acc: 0.2106\n",
      "Epoch 11/20\n",
      " - 2s - loss: 2.2142 - acc: 0.1817 - val_loss: 2.1787 - val_acc: 0.2170\n",
      "Epoch 12/20\n",
      " - 2s - loss: 2.2037 - acc: 0.1915 - val_loss: 2.1707 - val_acc: 0.2224\n",
      "Epoch 13/20\n",
      " - 2s - loss: 2.2001 - acc: 0.1934 - val_loss: 2.1627 - val_acc: 0.2296\n",
      "Epoch 14/20\n",
      " - 2s - loss: 2.1914 - acc: 0.1958 - val_loss: 2.1552 - val_acc: 0.2366\n",
      "Epoch 15/20\n",
      " - 2s - loss: 2.1839 - acc: 0.2031 - val_loss: 2.1476 - val_acc: 0.2420\n",
      "Epoch 16/20\n",
      " - 2s - loss: 2.1775 - acc: 0.2049 - val_loss: 2.1403 - val_acc: 0.2470\n",
      "Epoch 17/20\n",
      " - 2s - loss: 2.1707 - acc: 0.2072 - val_loss: 2.1334 - val_acc: 0.2493\n",
      "Epoch 18/20\n",
      " - 2s - loss: 2.1620 - acc: 0.2110 - val_loss: 2.1260 - val_acc: 0.2559\n",
      "Epoch 19/20\n",
      " - 2s - loss: 2.1586 - acc: 0.2118 - val_loss: 2.1192 - val_acc: 0.2591\n",
      "Epoch 20/20\n",
      " - 2s - loss: 2.1512 - acc: 0.2176 - val_loss: 2.1122 - val_acc: 0.2641\n",
      "10000/10000 [==============================] - 1s 59us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment1:\tMomentum with learning rate @ 1e-05\n",
      "=======================================================\n",
      "Time Used: 56.956528000000006\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.112185428237915\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.2641\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 3s - loss: 2.1916 - acc: 0.1978 - val_loss: 2.0574 - val_acc: 0.2853\n",
      "Epoch 2/20\n",
      " - 2s - loss: 2.0550 - acc: 0.2575 - val_loss: 1.9594 - val_acc: 0.3229\n",
      "Epoch 3/20\n",
      " - 2s - loss: 1.9828 - acc: 0.2878 - val_loss: 1.8995 - val_acc: 0.3412\n",
      "Epoch 4/20\n",
      " - 2s - loss: 1.9341 - acc: 0.3094 - val_loss: 1.8641 - val_acc: 0.3555\n",
      "Epoch 5/20\n",
      " - 2s - loss: 1.8997 - acc: 0.3219 - val_loss: 1.8318 - val_acc: 0.3646\n",
      "Epoch 6/20\n",
      " - 2s - loss: 1.8699 - acc: 0.3364 - val_loss: 1.8100 - val_acc: 0.3716\n",
      "Epoch 7/20\n",
      " - 2s - loss: 1.8475 - acc: 0.3457 - val_loss: 1.7856 - val_acc: 0.3811\n",
      "Epoch 8/20\n",
      " - 2s - loss: 1.8259 - acc: 0.3529 - val_loss: 1.7649 - val_acc: 0.3835\n",
      "Epoch 9/20\n",
      " - 2s - loss: 1.8098 - acc: 0.3576 - val_loss: 1.7482 - val_acc: 0.3887\n",
      "Epoch 10/20\n",
      " - 2s - loss: 1.7921 - acc: 0.3648 - val_loss: 1.7340 - val_acc: 0.3903\n",
      "Epoch 11/20\n",
      " - 2s - loss: 1.7764 - acc: 0.3717 - val_loss: 1.7242 - val_acc: 0.3997\n",
      "Epoch 12/20\n",
      " - 2s - loss: 1.7633 - acc: 0.3768 - val_loss: 1.7068 - val_acc: 0.4059\n",
      "Epoch 13/20\n",
      " - 2s - loss: 1.7486 - acc: 0.3815 - val_loss: 1.6930 - val_acc: 0.4086\n",
      "Epoch 14/20\n",
      " - 2s - loss: 1.7369 - acc: 0.3864 - val_loss: 1.6846 - val_acc: 0.4117\n",
      "Epoch 15/20\n",
      " - 2s - loss: 1.7264 - acc: 0.3893 - val_loss: 1.6722 - val_acc: 0.4141\n",
      "Epoch 16/20\n",
      " - 2s - loss: 1.7149 - acc: 0.3937 - val_loss: 1.6650 - val_acc: 0.4175\n",
      "Epoch 17/20\n",
      " - 2s - loss: 1.7053 - acc: 0.3981 - val_loss: 1.6530 - val_acc: 0.4221\n",
      "Epoch 18/20\n",
      " - 2s - loss: 1.6975 - acc: 0.4002 - val_loss: 1.6462 - val_acc: 0.4231\n",
      "Epoch 19/20\n",
      " - 2s - loss: 1.6885 - acc: 0.4008 - val_loss: 1.6377 - val_acc: 0.4254\n",
      "Epoch 20/20\n",
      " - 2s - loss: 1.6838 - acc: 0.4063 - val_loss: 1.6313 - val_acc: 0.4251\n",
      "10000/10000 [==============================] - 1s 60us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment2:\tRMSprop with learning rate @ 1e-05\n",
      "=======================================================\n",
      "Time Used: 63.179619\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.6312962524414063\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.4251\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 3s - loss: 2.2049 - acc: 0.1873 - val_loss: 2.0612 - val_acc: 0.2909\n",
      "Epoch 2/20\n",
      " - 3s - loss: 2.0570 - acc: 0.2573 - val_loss: 1.9647 - val_acc: 0.3145\n",
      "Epoch 3/20\n",
      " - 3s - loss: 1.9856 - acc: 0.2895 - val_loss: 1.9065 - val_acc: 0.3362\n",
      "Epoch 4/20\n",
      " - 3s - loss: 1.9399 - acc: 0.3063 - val_loss: 1.8690 - val_acc: 0.3476\n",
      "Epoch 5/20\n",
      " - 3s - loss: 1.9017 - acc: 0.3221 - val_loss: 1.8431 - val_acc: 0.3563\n",
      "Epoch 6/20\n",
      " - 3s - loss: 1.8741 - acc: 0.3331 - val_loss: 1.8137 - val_acc: 0.3691\n",
      "Epoch 7/20\n",
      " - 3s - loss: 1.8532 - acc: 0.3408 - val_loss: 1.7925 - val_acc: 0.3745\n",
      "Epoch 8/20\n",
      " - 3s - loss: 1.8293 - acc: 0.3512 - val_loss: 1.7733 - val_acc: 0.3830\n",
      "Epoch 9/20\n",
      " - 3s - loss: 1.8120 - acc: 0.3572 - val_loss: 1.7528 - val_acc: 0.3895\n",
      "Epoch 10/20\n",
      " - 3s - loss: 1.7961 - acc: 0.3622 - val_loss: 1.7382 - val_acc: 0.3939\n",
      "Epoch 11/20\n",
      " - 3s - loss: 1.7774 - acc: 0.3699 - val_loss: 1.7222 - val_acc: 0.3989\n",
      "Epoch 12/20\n",
      " - 3s - loss: 1.7637 - acc: 0.3766 - val_loss: 1.7110 - val_acc: 0.4057\n",
      "Epoch 13/20\n",
      " - 3s - loss: 1.7507 - acc: 0.3807 - val_loss: 1.6971 - val_acc: 0.4073\n",
      "Epoch 14/20\n",
      " - 3s - loss: 1.7386 - acc: 0.3886 - val_loss: 1.6833 - val_acc: 0.4127\n",
      "Epoch 15/20\n",
      " - 3s - loss: 1.7237 - acc: 0.3930 - val_loss: 1.6729 - val_acc: 0.4173\n",
      "Epoch 16/20\n",
      " - 3s - loss: 1.7153 - acc: 0.3946 - val_loss: 1.6604 - val_acc: 0.4220\n",
      "Epoch 17/20\n",
      " - 3s - loss: 1.7017 - acc: 0.3995 - val_loss: 1.6501 - val_acc: 0.4227\n",
      "Epoch 18/20\n",
      " - 3s - loss: 1.6944 - acc: 0.4002 - val_loss: 1.6410 - val_acc: 0.4275\n",
      "Epoch 19/20\n",
      " - 3s - loss: 1.6856 - acc: 0.4075 - val_loss: 1.6328 - val_acc: 0.4308\n",
      "Epoch 20/20\n",
      " - 3s - loss: 1.6766 - acc: 0.4118 - val_loss: 1.6243 - val_acc: 0.4352\n",
      "10000/10000 [==============================] - 1s 61us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment3:\tAdam with learning rate @ 1e-05\n",
      "=======================================================\n",
      "Time Used: 68.96155200000001\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.6242679374694824\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.4352\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 3s - loss: 2.3854 - acc: 0.1105 - val_loss: 2.2807 - val_acc: 0.1433\n",
      "Epoch 2/20\n",
      " - 2s - loss: 2.3251 - acc: 0.1204 - val_loss: 2.2571 - val_acc: 0.1694\n",
      "Epoch 3/20\n",
      " - 2s - loss: 2.3064 - acc: 0.1292 - val_loss: 2.2419 - val_acc: 0.1829\n",
      "Epoch 4/20\n",
      " - 2s - loss: 2.2868 - acc: 0.1415 - val_loss: 2.2301 - val_acc: 0.1945\n",
      "Epoch 5/20\n",
      " - 2s - loss: 2.2758 - acc: 0.1438 - val_loss: 2.2211 - val_acc: 0.2005\n",
      "Epoch 6/20\n",
      " - 2s - loss: 2.2640 - acc: 0.1510 - val_loss: 2.2134 - val_acc: 0.2040\n",
      "Epoch 7/20\n",
      " - 2s - loss: 2.2595 - acc: 0.1542 - val_loss: 2.2068 - val_acc: 0.2087\n",
      "Epoch 8/20\n",
      " - 2s - loss: 2.2520 - acc: 0.1584 - val_loss: 2.2007 - val_acc: 0.2140\n",
      "Epoch 9/20\n",
      " - 2s - loss: 2.2461 - acc: 0.1631 - val_loss: 2.1951 - val_acc: 0.2172\n",
      "Epoch 10/20\n",
      " - 2s - loss: 2.2398 - acc: 0.1670 - val_loss: 2.1901 - val_acc: 0.2201\n",
      "Epoch 11/20\n",
      " - 2s - loss: 2.2336 - acc: 0.1697 - val_loss: 2.1854 - val_acc: 0.2234\n",
      "Epoch 12/20\n",
      " - 2s - loss: 2.2276 - acc: 0.1738 - val_loss: 2.1810 - val_acc: 0.2262\n",
      "Epoch 13/20\n",
      " - 2s - loss: 2.2259 - acc: 0.1746 - val_loss: 2.1769 - val_acc: 0.2295\n",
      "Epoch 14/20\n",
      " - 2s - loss: 2.2196 - acc: 0.1781 - val_loss: 2.1732 - val_acc: 0.2295\n",
      "Epoch 15/20\n",
      " - 2s - loss: 2.2152 - acc: 0.1798 - val_loss: 2.1697 - val_acc: 0.2323\n",
      "Epoch 16/20\n",
      " - 2s - loss: 2.2122 - acc: 0.1841 - val_loss: 2.1663 - val_acc: 0.2342\n",
      "Epoch 17/20\n",
      " - 2s - loss: 2.2083 - acc: 0.1879 - val_loss: 2.1632 - val_acc: 0.2367\n",
      "Epoch 18/20\n",
      " - 2s - loss: 2.2078 - acc: 0.1855 - val_loss: 2.1602 - val_acc: 0.2379\n",
      "Epoch 19/20\n",
      " - 2s - loss: 2.2039 - acc: 0.1861 - val_loss: 2.1574 - val_acc: 0.2386\n",
      "Epoch 20/20\n",
      " - 2s - loss: 2.2010 - acc: 0.1899 - val_loss: 2.1546 - val_acc: 0.2403\n",
      "10000/10000 [==============================] - 1s 61us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment4:\tAdagrad with learning rate @ 1e-05\n",
      "=======================================================\n",
      "Time Used: 61.846221000000014\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.15463243560791\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.2403\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 3s - loss: 2.4308 - acc: 0.0945 - val_loss: 2.3777 - val_acc: 0.0892\n",
      "Epoch 2/20\n",
      " - 3s - loss: 2.4288 - acc: 0.0936 - val_loss: 2.3737 - val_acc: 0.0886\n",
      "Epoch 3/20\n",
      " - 3s - loss: 2.4228 - acc: 0.0959 - val_loss: 2.3698 - val_acc: 0.0887\n",
      "Epoch 4/20\n",
      " - 3s - loss: 2.4218 - acc: 0.0947 - val_loss: 2.3661 - val_acc: 0.0889\n",
      "Epoch 5/20\n",
      " - 3s - loss: 2.4154 - acc: 0.0946 - val_loss: 2.3625 - val_acc: 0.0889\n",
      "Epoch 6/20\n",
      " - 3s - loss: 2.4131 - acc: 0.0967 - val_loss: 2.3591 - val_acc: 0.0888\n",
      "Epoch 7/20\n",
      " - 3s - loss: 2.4089 - acc: 0.0983 - val_loss: 2.3559 - val_acc: 0.0886\n",
      "Epoch 8/20\n",
      " - 3s - loss: 2.4060 - acc: 0.0959 - val_loss: 2.3529 - val_acc: 0.0875\n",
      "Epoch 9/20\n",
      " - 3s - loss: 2.4034 - acc: 0.0984 - val_loss: 2.3500 - val_acc: 0.0869\n",
      "Epoch 10/20\n",
      " - 3s - loss: 2.4001 - acc: 0.0969 - val_loss: 2.3473 - val_acc: 0.0868\n",
      "Epoch 11/20\n",
      " - 3s - loss: 2.3969 - acc: 0.0954 - val_loss: 2.3447 - val_acc: 0.0865\n",
      "Epoch 12/20\n",
      " - 3s - loss: 2.3957 - acc: 0.0975 - val_loss: 2.3422 - val_acc: 0.0878\n",
      "Epoch 13/20\n",
      " - 3s - loss: 2.3913 - acc: 0.0977 - val_loss: 2.3398 - val_acc: 0.0885\n",
      "Epoch 14/20\n",
      " - 3s - loss: 2.3907 - acc: 0.0994 - val_loss: 2.3375 - val_acc: 0.0888\n",
      "Epoch 15/20\n",
      " - 3s - loss: 2.3878 - acc: 0.1003 - val_loss: 2.3354 - val_acc: 0.0895\n",
      "Epoch 16/20\n",
      " - 3s - loss: 2.3859 - acc: 0.1008 - val_loss: 2.3333 - val_acc: 0.0900\n",
      "Epoch 17/20\n",
      " - 3s - loss: 2.3838 - acc: 0.1010 - val_loss: 2.3313 - val_acc: 0.0913\n",
      "Epoch 18/20\n",
      " - 3s - loss: 2.3779 - acc: 0.1019 - val_loss: 2.3293 - val_acc: 0.0922\n",
      "Epoch 19/20\n",
      " - 3s - loss: 2.3769 - acc: 0.1013 - val_loss: 2.3275 - val_acc: 0.0942\n",
      "Epoch 20/20\n",
      " - 3s - loss: 2.3765 - acc: 0.1019 - val_loss: 2.3257 - val_acc: 0.0955\n",
      "10000/10000 [==============================] - 1s 60us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment5:\tAdadelta with learning rate @ 1e-05\n",
      "=======================================================\n",
      "Time Used: 73.62557400000003\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.3257282485961914\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.0955\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 3s - loss: 2.4480 - acc: 0.1036 - val_loss: 2.3563 - val_acc: 0.1099\n",
      "Epoch 2/20\n",
      " - 2s - loss: 2.3867 - acc: 0.1073 - val_loss: 2.3207 - val_acc: 0.1248\n",
      "Epoch 3/20\n",
      " - 2s - loss: 2.3581 - acc: 0.1125 - val_loss: 2.3013 - val_acc: 0.1358\n",
      "Epoch 4/20\n",
      " - 2s - loss: 2.3382 - acc: 0.1154 - val_loss: 2.2895 - val_acc: 0.1428\n",
      "Epoch 5/20\n",
      " - 2s - loss: 2.3241 - acc: 0.1228 - val_loss: 2.2811 - val_acc: 0.1469\n",
      "Epoch 6/20\n",
      " - 2s - loss: 2.3169 - acc: 0.1233 - val_loss: 2.2745 - val_acc: 0.1498\n",
      "Epoch 7/20\n",
      " - 2s - loss: 2.3093 - acc: 0.1263 - val_loss: 2.2688 - val_acc: 0.1563\n",
      "Epoch 8/20\n",
      " - 2s - loss: 2.3023 - acc: 0.1296 - val_loss: 2.2638 - val_acc: 0.1610\n",
      "Epoch 9/20\n",
      " - 2s - loss: 2.2973 - acc: 0.1304 - val_loss: 2.2592 - val_acc: 0.1675\n",
      "Epoch 10/20\n",
      " - 2s - loss: 2.2932 - acc: 0.1320 - val_loss: 2.2549 - val_acc: 0.1713\n",
      "Epoch 11/20\n",
      " - 2s - loss: 2.2854 - acc: 0.1383 - val_loss: 2.2508 - val_acc: 0.1744\n",
      "Epoch 12/20\n",
      " - 2s - loss: 2.2806 - acc: 0.1392 - val_loss: 2.2470 - val_acc: 0.1787\n",
      "Epoch 13/20\n",
      " - 2s - loss: 2.2787 - acc: 0.1412 - val_loss: 2.2433 - val_acc: 0.1835\n",
      "Epoch 14/20\n",
      " - 2s - loss: 2.2737 - acc: 0.1472 - val_loss: 2.2399 - val_acc: 0.1870\n",
      "Epoch 15/20\n",
      " - 2s - loss: 2.2691 - acc: 0.1490 - val_loss: 2.2365 - val_acc: 0.1897\n",
      "Epoch 16/20\n",
      " - 2s - loss: 2.2671 - acc: 0.1475 - val_loss: 2.2332 - val_acc: 0.1919\n",
      "Epoch 17/20\n",
      " - 2s - loss: 2.2630 - acc: 0.1526 - val_loss: 2.2299 - val_acc: 0.1942\n",
      "Epoch 18/20\n",
      " - 2s - loss: 2.2596 - acc: 0.1544 - val_loss: 2.2268 - val_acc: 0.1978\n",
      "Epoch 19/20\n",
      " - 2s - loss: 2.2568 - acc: 0.1568 - val_loss: 2.2237 - val_acc: 0.1993\n",
      "Epoch 20/20\n",
      " - 2s - loss: 2.2526 - acc: 0.1583 - val_loss: 2.2207 - val_acc: 0.2006\n",
      "10000/10000 [==============================] - 1s 60us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment6:\tSGD with learning rate @ 3e-05\n",
      "=======================================================\n",
      "Time Used: 57.657722000000035\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.2207009628295897\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.2006\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 3s - loss: 2.3475 - acc: 0.1157 - val_loss: 2.2516 - val_acc: 0.1620\n",
      "Epoch 2/20\n",
      " - 2s - loss: 2.2769 - acc: 0.1408 - val_loss: 2.2103 - val_acc: 0.1900\n",
      "Epoch 3/20\n",
      " - 2s - loss: 2.2384 - acc: 0.1624 - val_loss: 2.1804 - val_acc: 0.2123\n",
      "Epoch 4/20\n",
      " - 2s - loss: 2.2089 - acc: 0.1765 - val_loss: 2.1557 - val_acc: 0.2313\n",
      "Epoch 5/20\n",
      " - 2s - loss: 2.1869 - acc: 0.1882 - val_loss: 2.1335 - val_acc: 0.2426\n",
      "Epoch 6/20\n",
      " - 2s - loss: 2.1657 - acc: 0.1965 - val_loss: 2.1135 - val_acc: 0.2522\n",
      "Epoch 7/20\n",
      " - 2s - loss: 2.1442 - acc: 0.2082 - val_loss: 2.0948 - val_acc: 0.2646\n",
      "Epoch 8/20\n",
      " - 2s - loss: 2.1297 - acc: 0.2167 - val_loss: 2.0785 - val_acc: 0.2740\n",
      "Epoch 9/20\n",
      " - 2s - loss: 2.1168 - acc: 0.2201 - val_loss: 2.0636 - val_acc: 0.2800\n",
      "Epoch 10/20\n",
      " - 2s - loss: 2.1013 - acc: 0.2278 - val_loss: 2.0496 - val_acc: 0.2862\n",
      "Epoch 11/20\n",
      " - 2s - loss: 2.0881 - acc: 0.2360 - val_loss: 2.0362 - val_acc: 0.2930\n",
      "Epoch 12/20\n",
      " - 2s - loss: 2.0756 - acc: 0.2384 - val_loss: 2.0246 - val_acc: 0.2979\n",
      "Epoch 13/20\n",
      " - 2s - loss: 2.0643 - acc: 0.2479 - val_loss: 2.0127 - val_acc: 0.3024\n",
      "Epoch 14/20\n",
      " - 2s - loss: 2.0557 - acc: 0.2526 - val_loss: 2.0026 - val_acc: 0.3050\n",
      "Epoch 15/20\n",
      " - 2s - loss: 2.0454 - acc: 0.2567 - val_loss: 1.9928 - val_acc: 0.3104\n",
      "Epoch 16/20\n",
      " - 2s - loss: 2.0342 - acc: 0.2604 - val_loss: 1.9832 - val_acc: 0.3135\n",
      "Epoch 17/20\n",
      " - 2s - loss: 2.0269 - acc: 0.2653 - val_loss: 1.9745 - val_acc: 0.3168\n",
      "Epoch 18/20\n",
      " - 2s - loss: 2.0192 - acc: 0.2673 - val_loss: 1.9667 - val_acc: 0.3217\n",
      "Epoch 19/20\n",
      " - 2s - loss: 2.0123 - acc: 0.2702 - val_loss: 1.9595 - val_acc: 0.3237\n",
      "Epoch 20/20\n",
      " - 2s - loss: 2.0032 - acc: 0.2747 - val_loss: 1.9515 - val_acc: 0.3251\n",
      "10000/10000 [==============================] - 1s 61us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment7:\tMomentum with learning rate @ 3e-05\n",
      "=======================================================\n",
      "Time Used: 57.75197000000003\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.9515228561401368\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.3251\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 3s - loss: 2.0913 - acc: 0.2373 - val_loss: 1.9300 - val_acc: 0.3317\n",
      "Epoch 2/20\n",
      " - 2s - loss: 1.9252 - acc: 0.3107 - val_loss: 1.8331 - val_acc: 0.3605\n",
      "Epoch 3/20\n",
      " - 3s - loss: 1.8525 - acc: 0.3410 - val_loss: 1.7756 - val_acc: 0.3817\n",
      "Epoch 4/20\n",
      " - 2s - loss: 1.8045 - acc: 0.3597 - val_loss: 1.7352 - val_acc: 0.3900\n",
      "Epoch 5/20\n",
      " - 2s - loss: 1.7665 - acc: 0.3740 - val_loss: 1.7123 - val_acc: 0.3944\n",
      "Epoch 6/20\n",
      " - 2s - loss: 1.7388 - acc: 0.3844 - val_loss: 1.6720 - val_acc: 0.4159\n",
      "Epoch 7/20\n",
      " - 2s - loss: 1.7120 - acc: 0.3947 - val_loss: 1.6514 - val_acc: 0.4221\n",
      "Epoch 8/20\n",
      " - 2s - loss: 1.6908 - acc: 0.4031 - val_loss: 1.6272 - val_acc: 0.4319\n",
      "Epoch 9/20\n",
      " - 2s - loss: 1.6684 - acc: 0.4102 - val_loss: 1.6061 - val_acc: 0.4384\n",
      "Epoch 10/20\n",
      " - 2s - loss: 1.6483 - acc: 0.4185 - val_loss: 1.5930 - val_acc: 0.4376\n",
      "Epoch 11/20\n",
      " - 2s - loss: 1.6334 - acc: 0.4252 - val_loss: 1.5761 - val_acc: 0.4461\n",
      "Epoch 12/20\n",
      " - 2s - loss: 1.6177 - acc: 0.4283 - val_loss: 1.5673 - val_acc: 0.4512\n",
      "Epoch 13/20\n",
      " - 2s - loss: 1.6019 - acc: 0.4373 - val_loss: 1.5476 - val_acc: 0.4562\n",
      "Epoch 14/20\n",
      " - 2s - loss: 1.5910 - acc: 0.4378 - val_loss: 1.5365 - val_acc: 0.4618\n",
      "Epoch 15/20\n",
      " - 2s - loss: 1.5754 - acc: 0.4428 - val_loss: 1.5260 - val_acc: 0.4649\n",
      "Epoch 16/20\n",
      " - 2s - loss: 1.5669 - acc: 0.4474 - val_loss: 1.5175 - val_acc: 0.4668\n",
      "Epoch 17/20\n",
      " - 2s - loss: 1.5526 - acc: 0.4533 - val_loss: 1.5136 - val_acc: 0.4666\n",
      "Epoch 18/20\n",
      " - 2s - loss: 1.5437 - acc: 0.4556 - val_loss: 1.4988 - val_acc: 0.4764\n",
      "Epoch 19/20\n",
      " - 2s - loss: 1.5350 - acc: 0.4605 - val_loss: 1.4927 - val_acc: 0.4765\n",
      "Epoch 20/20\n",
      " - 2s - loss: 1.5223 - acc: 0.4630 - val_loss: 1.4831 - val_acc: 0.4784\n",
      "10000/10000 [==============================] - 1s 62us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment8:\tRMSprop with learning rate @ 3e-05\n",
      "=======================================================\n",
      "Time Used: 63.70132000000001\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.4830950325012207\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.4784\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 3s - loss: 2.0914 - acc: 0.2403 - val_loss: 1.9094 - val_acc: 0.3373\n",
      "Epoch 2/20\n",
      " - 3s - loss: 1.9074 - acc: 0.3194 - val_loss: 1.8128 - val_acc: 0.3707\n",
      "Epoch 3/20\n",
      " - 3s - loss: 1.8335 - acc: 0.3478 - val_loss: 1.7553 - val_acc: 0.3817\n",
      "Epoch 4/20\n",
      " - 3s - loss: 1.7853 - acc: 0.3673 - val_loss: 1.7053 - val_acc: 0.4022\n",
      "Epoch 5/20\n",
      " - 3s - loss: 1.7449 - acc: 0.3818 - val_loss: 1.6707 - val_acc: 0.4154\n",
      "Epoch 6/20\n",
      " - 3s - loss: 1.7108 - acc: 0.3933 - val_loss: 1.6445 - val_acc: 0.4228\n",
      "Epoch 7/20\n",
      " - 3s - loss: 1.6854 - acc: 0.4039 - val_loss: 1.6239 - val_acc: 0.4301\n",
      "Epoch 8/20\n",
      " - 3s - loss: 1.6639 - acc: 0.4141 - val_loss: 1.6008 - val_acc: 0.4359\n",
      "Epoch 9/20\n",
      " - 3s - loss: 1.6465 - acc: 0.4190 - val_loss: 1.5804 - val_acc: 0.4473\n",
      "Epoch 10/20\n",
      " - 3s - loss: 1.6224 - acc: 0.4263 - val_loss: 1.5673 - val_acc: 0.4474\n",
      "Epoch 11/20\n",
      " - 3s - loss: 1.6080 - acc: 0.4299 - val_loss: 1.5538 - val_acc: 0.4545\n",
      "Epoch 12/20\n",
      " - 3s - loss: 1.5912 - acc: 0.4405 - val_loss: 1.5315 - val_acc: 0.4598\n",
      "Epoch 13/20\n",
      " - 3s - loss: 1.5747 - acc: 0.4459 - val_loss: 1.5240 - val_acc: 0.4644\n",
      "Epoch 14/20\n",
      " - 3s - loss: 1.5609 - acc: 0.4504 - val_loss: 1.5157 - val_acc: 0.4641\n",
      "Epoch 15/20\n",
      " - 3s - loss: 1.5481 - acc: 0.4536 - val_loss: 1.5076 - val_acc: 0.4649\n",
      "Epoch 16/20\n",
      " - 3s - loss: 1.5343 - acc: 0.4576 - val_loss: 1.4978 - val_acc: 0.4715\n",
      "Epoch 17/20\n",
      " - 3s - loss: 1.5235 - acc: 0.4615 - val_loss: 1.4866 - val_acc: 0.4744\n",
      "Epoch 18/20\n",
      " - 3s - loss: 1.5106 - acc: 0.4673 - val_loss: 1.4753 - val_acc: 0.4742\n",
      "Epoch 19/20\n",
      " - 3s - loss: 1.5046 - acc: 0.4691 - val_loss: 1.4682 - val_acc: 0.4831\n",
      "Epoch 20/20\n",
      " - 3s - loss: 1.4959 - acc: 0.4717 - val_loss: 1.4571 - val_acc: 0.4872\n",
      "10000/10000 [==============================] - 1s 62us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment9:\tAdam with learning rate @ 3e-05\n",
      "=======================================================\n",
      "Time Used: 69.77401099999997\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.4571369762420654\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.4872\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 3s - loss: 2.2743 - acc: 0.1536 - val_loss: 2.1898 - val_acc: 0.2442\n",
      "Epoch 2/20\n",
      " - 2s - loss: 2.2148 - acc: 0.1863 - val_loss: 2.1520 - val_acc: 0.2616\n",
      "Epoch 3/20\n",
      " - 2s - loss: 2.1827 - acc: 0.2020 - val_loss: 2.1273 - val_acc: 0.2729\n",
      "Epoch 4/20\n",
      " - 2s - loss: 2.1636 - acc: 0.2153 - val_loss: 2.1081 - val_acc: 0.2814\n",
      "Epoch 5/20\n",
      " - 2s - loss: 2.1464 - acc: 0.2216 - val_loss: 2.0937 - val_acc: 0.2836\n",
      "Epoch 6/20\n",
      " - 2s - loss: 2.1328 - acc: 0.2302 - val_loss: 2.0805 - val_acc: 0.2893\n",
      "Epoch 7/20\n",
      " - 2s - loss: 2.1215 - acc: 0.2325 - val_loss: 2.0700 - val_acc: 0.2915\n",
      "Epoch 8/20\n",
      " - 2s - loss: 2.1142 - acc: 0.2344 - val_loss: 2.0605 - val_acc: 0.2947\n",
      "Epoch 9/20\n",
      " - 2s - loss: 2.1031 - acc: 0.2407 - val_loss: 2.0520 - val_acc: 0.2992\n",
      "Epoch 10/20\n",
      " - 2s - loss: 2.0971 - acc: 0.2403 - val_loss: 2.0443 - val_acc: 0.3021\n",
      "Epoch 11/20\n",
      " - 2s - loss: 2.0901 - acc: 0.2449 - val_loss: 2.0373 - val_acc: 0.3057\n",
      "Epoch 12/20\n",
      " - 2s - loss: 2.0871 - acc: 0.2450 - val_loss: 2.0314 - val_acc: 0.3067\n",
      "Epoch 13/20\n",
      " - 2s - loss: 2.0784 - acc: 0.2511 - val_loss: 2.0257 - val_acc: 0.3074\n",
      "Epoch 14/20\n",
      " - 2s - loss: 2.0700 - acc: 0.2523 - val_loss: 2.0203 - val_acc: 0.3108\n",
      "Epoch 15/20\n",
      " - 2s - loss: 2.0671 - acc: 0.2573 - val_loss: 2.0153 - val_acc: 0.3120\n",
      "Epoch 16/20\n",
      " - 2s - loss: 2.0644 - acc: 0.2550 - val_loss: 2.0110 - val_acc: 0.3120\n",
      "Epoch 17/20\n",
      " - 2s - loss: 2.0609 - acc: 0.2587 - val_loss: 2.0068 - val_acc: 0.3127\n",
      "Epoch 18/20\n",
      " - 2s - loss: 2.0552 - acc: 0.2625 - val_loss: 2.0029 - val_acc: 0.3142\n",
      "Epoch 19/20\n",
      " - 2s - loss: 2.0529 - acc: 0.2634 - val_loss: 1.9989 - val_acc: 0.3164\n",
      "Epoch 20/20\n",
      " - 2s - loss: 2.0476 - acc: 0.2618 - val_loss: 1.9954 - val_acc: 0.3173\n",
      "10000/10000 [==============================] - 1s 62us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment10:\tAdagrad with learning rate @ 3e-05\n",
      "=======================================================\n",
      "Time Used: 62.41646099999991\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.995355824279785\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.3173\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 4s - loss: 2.4450 - acc: 0.0926 - val_loss: 2.3781 - val_acc: 0.0864\n",
      "Epoch 2/20\n",
      " - 3s - loss: 2.4331 - acc: 0.0914 - val_loss: 2.3678 - val_acc: 0.0845\n",
      "Epoch 3/20\n",
      " - 3s - loss: 2.4225 - acc: 0.0930 - val_loss: 2.3588 - val_acc: 0.0855\n",
      "Epoch 4/20\n",
      " - 3s - loss: 2.4119 - acc: 0.0961 - val_loss: 2.3510 - val_acc: 0.0863\n",
      "Epoch 5/20\n",
      " - 3s - loss: 2.4063 - acc: 0.0954 - val_loss: 2.3441 - val_acc: 0.0868\n",
      "Epoch 6/20\n",
      " - 3s - loss: 2.3970 - acc: 0.0922 - val_loss: 2.3382 - val_acc: 0.0878\n",
      "Epoch 7/20\n",
      " - 3s - loss: 2.3886 - acc: 0.0976 - val_loss: 2.3329 - val_acc: 0.0901\n",
      "Epoch 8/20\n",
      " - 3s - loss: 2.3827 - acc: 0.0970 - val_loss: 2.3282 - val_acc: 0.0906\n",
      "Epoch 9/20\n",
      " - 3s - loss: 2.3778 - acc: 0.0975 - val_loss: 2.3240 - val_acc: 0.0920\n",
      "Epoch 10/20\n",
      " - 3s - loss: 2.3744 - acc: 0.0982 - val_loss: 2.3202 - val_acc: 0.0949\n",
      "Epoch 11/20\n",
      " - 3s - loss: 2.3693 - acc: 0.1007 - val_loss: 2.3167 - val_acc: 0.0953\n",
      "Epoch 12/20\n",
      " - 3s - loss: 2.3659 - acc: 0.0992 - val_loss: 2.3134 - val_acc: 0.0985\n",
      "Epoch 13/20\n",
      " - 3s - loss: 2.3628 - acc: 0.1021 - val_loss: 2.3104 - val_acc: 0.1020\n",
      "Epoch 14/20\n",
      " - 3s - loss: 2.3585 - acc: 0.1032 - val_loss: 2.3076 - val_acc: 0.1035\n",
      "Epoch 15/20\n",
      " - 3s - loss: 2.3547 - acc: 0.1033 - val_loss: 2.3049 - val_acc: 0.1045\n",
      "Epoch 16/20\n",
      " - 3s - loss: 2.3525 - acc: 0.1046 - val_loss: 2.3023 - val_acc: 0.1064\n",
      "Epoch 17/20\n",
      " - 3s - loss: 2.3482 - acc: 0.1050 - val_loss: 2.2999 - val_acc: 0.1087\n",
      "Epoch 18/20\n",
      " - 3s - loss: 2.3439 - acc: 0.1096 - val_loss: 2.2975 - val_acc: 0.1102\n",
      "Epoch 19/20\n",
      " - 3s - loss: 2.3426 - acc: 0.1070 - val_loss: 2.2953 - val_acc: 0.1119\n",
      "Epoch 20/20\n",
      " - 3s - loss: 2.3404 - acc: 0.1080 - val_loss: 2.2931 - val_acc: 0.1149\n",
      "10000/10000 [==============================] - 1s 62us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment11:\tAdadelta with learning rate @ 3e-05\n",
      "=======================================================\n",
      "Time Used: 74.54869899999994\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.293114827346802\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1149\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 3s - loss: 2.4362 - acc: 0.0920 - val_loss: 2.3273 - val_acc: 0.1059\n",
      "Epoch 2/20\n",
      " - 2s - loss: 2.3590 - acc: 0.0992 - val_loss: 2.2966 - val_acc: 0.1170\n",
      "Epoch 3/20\n",
      " - 2s - loss: 2.3314 - acc: 0.1076 - val_loss: 2.2783 - val_acc: 0.1406\n",
      "Epoch 4/20\n",
      " - 2s - loss: 2.3096 - acc: 0.1208 - val_loss: 2.2630 - val_acc: 0.1576\n",
      "Epoch 5/20\n",
      " - 2s - loss: 2.2954 - acc: 0.1306 - val_loss: 2.2496 - val_acc: 0.1722\n",
      "Epoch 6/20\n",
      " - 2s - loss: 2.2792 - acc: 0.1402 - val_loss: 2.2373 - val_acc: 0.1831\n",
      "Epoch 7/20\n",
      " - 2s - loss: 2.2664 - acc: 0.1467 - val_loss: 2.2264 - val_acc: 0.1937\n",
      "Epoch 8/20\n",
      " - 2s - loss: 2.2556 - acc: 0.1546 - val_loss: 2.2162 - val_acc: 0.2037\n",
      "Epoch 9/20\n",
      " - 2s - loss: 2.2433 - acc: 0.1632 - val_loss: 2.2068 - val_acc: 0.2138\n",
      "Epoch 10/20\n",
      " - 2s - loss: 2.2362 - acc: 0.1681 - val_loss: 2.1978 - val_acc: 0.2212\n",
      "Epoch 11/20\n",
      " - 2s - loss: 2.2229 - acc: 0.1777 - val_loss: 2.1891 - val_acc: 0.2276\n",
      "Epoch 12/20\n",
      " - 2s - loss: 2.2189 - acc: 0.1791 - val_loss: 2.1808 - val_acc: 0.2339\n",
      "Epoch 13/20\n",
      " - 2s - loss: 2.2104 - acc: 0.1842 - val_loss: 2.1727 - val_acc: 0.2400\n",
      "Epoch 14/20\n",
      " - 2s - loss: 2.2031 - acc: 0.1890 - val_loss: 2.1647 - val_acc: 0.2469\n",
      "Epoch 15/20\n",
      " - 2s - loss: 2.1947 - acc: 0.1932 - val_loss: 2.1572 - val_acc: 0.2501\n",
      "Epoch 16/20\n",
      " - 2s - loss: 2.1873 - acc: 0.1987 - val_loss: 2.1499 - val_acc: 0.2543\n",
      "Epoch 17/20\n",
      " - 2s - loss: 2.1806 - acc: 0.2009 - val_loss: 2.1429 - val_acc: 0.2583\n",
      "Epoch 18/20\n",
      " - 2s - loss: 2.1740 - acc: 0.2063 - val_loss: 2.1362 - val_acc: 0.2612\n",
      "Epoch 19/20\n",
      " - 2s - loss: 2.1681 - acc: 0.2087 - val_loss: 2.1296 - val_acc: 0.2657\n",
      "Epoch 20/20\n",
      " - 2s - loss: 2.1637 - acc: 0.2106 - val_loss: 2.1234 - val_acc: 0.2673\n",
      "10000/10000 [==============================] - 1s 62us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment12:\tSGD with learning rate @ 0.0001\n",
      "=======================================================\n",
      "Time Used: 58.481791000000044\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.1233974811553957\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.2673\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 3s - loss: 2.3051 - acc: 0.1363 - val_loss: 2.1960 - val_acc: 0.2110\n",
      "Epoch 2/20\n",
      " - 2s - loss: 2.1999 - acc: 0.1927 - val_loss: 2.1310 - val_acc: 0.2476\n",
      "Epoch 3/20\n",
      " - 2s - loss: 2.1463 - acc: 0.2194 - val_loss: 2.0830 - val_acc: 0.2619\n",
      "Epoch 4/20\n",
      " - 2s - loss: 2.1038 - acc: 0.2351 - val_loss: 2.0429 - val_acc: 0.2852\n",
      "Epoch 5/20\n",
      " - 2s - loss: 2.0719 - acc: 0.2506 - val_loss: 2.0112 - val_acc: 0.2955\n",
      "Epoch 6/20\n",
      " - 2s - loss: 2.0436 - acc: 0.2605 - val_loss: 1.9836 - val_acc: 0.3105\n",
      "Epoch 7/20\n",
      " - 2s - loss: 2.0176 - acc: 0.2733 - val_loss: 1.9606 - val_acc: 0.3162\n",
      "Epoch 8/20\n",
      " - 2s - loss: 2.0018 - acc: 0.2786 - val_loss: 1.9407 - val_acc: 0.3255\n",
      "Epoch 9/20\n",
      " - 2s - loss: 1.9785 - acc: 0.2888 - val_loss: 1.9219 - val_acc: 0.3278\n",
      "Epoch 10/20\n",
      " - 2s - loss: 1.9646 - acc: 0.2936 - val_loss: 1.9067 - val_acc: 0.3333\n",
      "Epoch 11/20\n",
      " - 2s - loss: 1.9490 - acc: 0.3041 - val_loss: 1.8926 - val_acc: 0.3411\n",
      "Epoch 12/20\n",
      " - 2s - loss: 1.9356 - acc: 0.3099 - val_loss: 1.8791 - val_acc: 0.3469\n",
      "Epoch 13/20\n",
      " - 2s - loss: 1.9222 - acc: 0.3144 - val_loss: 1.8674 - val_acc: 0.3480\n",
      "Epoch 14/20\n",
      " - 2s - loss: 1.9102 - acc: 0.3175 - val_loss: 1.8560 - val_acc: 0.3511\n",
      "Epoch 15/20\n",
      " - 2s - loss: 1.9020 - acc: 0.3213 - val_loss: 1.8460 - val_acc: 0.3547\n",
      "Epoch 16/20\n",
      " - 2s - loss: 1.8911 - acc: 0.3267 - val_loss: 1.8365 - val_acc: 0.3591\n",
      "Epoch 17/20\n",
      " - 2s - loss: 1.8802 - acc: 0.3285 - val_loss: 1.8278 - val_acc: 0.3613\n",
      "Epoch 18/20\n",
      " - 2s - loss: 1.8764 - acc: 0.3286 - val_loss: 1.8207 - val_acc: 0.3603\n",
      "Epoch 19/20\n",
      " - 2s - loss: 1.8653 - acc: 0.3382 - val_loss: 1.8116 - val_acc: 0.3675\n",
      "Epoch 20/20\n",
      " - 2s - loss: 1.8581 - acc: 0.3395 - val_loss: 1.8050 - val_acc: 0.3713\n",
      "10000/10000 [==============================] - 1s 63us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment13:\tMomentum with learning rate @ 0.0001\n",
      "=======================================================\n",
      "Time Used: 58.52535599999999\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.8050002151489257\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.3713\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 3s - loss: 1.9894 - acc: 0.2762 - val_loss: 1.8340 - val_acc: 0.3466\n",
      "Epoch 2/20\n",
      " - 3s - loss: 1.8145 - acc: 0.3507 - val_loss: 1.7083 - val_acc: 0.3888\n",
      "Epoch 3/20\n",
      " - 3s - loss: 1.7432 - acc: 0.3798 - val_loss: 1.6586 - val_acc: 0.4125\n",
      "Epoch 4/20\n",
      " - 3s - loss: 1.6900 - acc: 0.4014 - val_loss: 1.6386 - val_acc: 0.4153\n",
      "Epoch 5/20\n",
      " - 3s - loss: 1.6579 - acc: 0.4134 - val_loss: 1.5981 - val_acc: 0.4345\n",
      "Epoch 6/20\n",
      " - 3s - loss: 1.6237 - acc: 0.4240 - val_loss: 1.5457 - val_acc: 0.4558\n",
      "Epoch 7/20\n",
      " - 3s - loss: 1.5988 - acc: 0.4352 - val_loss: 1.5347 - val_acc: 0.4511\n",
      "Epoch 8/20\n",
      " - 3s - loss: 1.5726 - acc: 0.4426 - val_loss: 1.5183 - val_acc: 0.4574\n",
      "Epoch 9/20\n",
      " - 3s - loss: 1.5495 - acc: 0.4525 - val_loss: 1.5237 - val_acc: 0.4528\n",
      "Epoch 10/20\n",
      " - 3s - loss: 1.5330 - acc: 0.4590 - val_loss: 1.5068 - val_acc: 0.4702\n",
      "Epoch 11/20\n",
      " - 3s - loss: 1.5174 - acc: 0.4639 - val_loss: 1.5183 - val_acc: 0.4650\n",
      "Epoch 12/20\n",
      " - 3s - loss: 1.4966 - acc: 0.4694 - val_loss: 1.5744 - val_acc: 0.4386\n",
      "Epoch 13/20\n",
      " - 3s - loss: 1.4785 - acc: 0.4770 - val_loss: 1.4453 - val_acc: 0.4872\n",
      "Epoch 14/20\n",
      " - 3s - loss: 1.4661 - acc: 0.4835 - val_loss: 1.4924 - val_acc: 0.4646\n",
      "Epoch 15/20\n",
      " - 3s - loss: 1.4551 - acc: 0.4854 - val_loss: 1.4820 - val_acc: 0.4763\n",
      "Epoch 16/20\n",
      " - 3s - loss: 1.4371 - acc: 0.4902 - val_loss: 1.4091 - val_acc: 0.5025\n",
      "Epoch 17/20\n",
      " - 3s - loss: 1.4270 - acc: 0.4967 - val_loss: 1.4027 - val_acc: 0.5084\n",
      "Epoch 18/20\n",
      " - 3s - loss: 1.4145 - acc: 0.5003 - val_loss: 1.4023 - val_acc: 0.5076\n",
      "Epoch 19/20\n",
      " - 3s - loss: 1.4015 - acc: 0.5053 - val_loss: 1.4161 - val_acc: 0.4995\n",
      "Epoch 20/20\n",
      " - 3s - loss: 1.3893 - acc: 0.5104 - val_loss: 1.4263 - val_acc: 0.4947\n",
      "10000/10000 [==============================] - 1s 64us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment14:\tRMSprop with learning rate @ 0.0001\n",
      "=======================================================\n",
      "Time Used: 64.85611700000004\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.4263064142227173\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.4947\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 4s - loss: 1.9944 - acc: 0.2771 - val_loss: 1.8230 - val_acc: 0.3675\n",
      "Epoch 2/20\n",
      " - 3s - loss: 1.8094 - acc: 0.3551 - val_loss: 1.7012 - val_acc: 0.4057\n",
      "Epoch 3/20\n",
      " - 3s - loss: 1.7368 - acc: 0.3832 - val_loss: 1.6491 - val_acc: 0.4113\n",
      "Epoch 4/20\n",
      " - 3s - loss: 1.6819 - acc: 0.4030 - val_loss: 1.6057 - val_acc: 0.4348\n",
      "Epoch 5/20\n",
      " - 3s - loss: 1.6446 - acc: 0.4192 - val_loss: 1.5666 - val_acc: 0.4485\n",
      "Epoch 6/20\n",
      " - 3s - loss: 1.6126 - acc: 0.4295 - val_loss: 1.5471 - val_acc: 0.4540\n",
      "Epoch 7/20\n",
      " - 3s - loss: 1.5840 - acc: 0.4403 - val_loss: 1.5241 - val_acc: 0.4646\n",
      "Epoch 8/20\n",
      " - 3s - loss: 1.5611 - acc: 0.4469 - val_loss: 1.5055 - val_acc: 0.4632\n",
      "Epoch 9/20\n",
      " - 3s - loss: 1.5380 - acc: 0.4571 - val_loss: 1.4961 - val_acc: 0.4754\n",
      "Epoch 10/20\n",
      " - 3s - loss: 1.5173 - acc: 0.4670 - val_loss: 1.4624 - val_acc: 0.4862\n",
      "Epoch 11/20\n",
      " - 3s - loss: 1.4996 - acc: 0.4694 - val_loss: 1.4555 - val_acc: 0.4854\n",
      "Epoch 12/20\n",
      " - 3s - loss: 1.4852 - acc: 0.4765 - val_loss: 1.4458 - val_acc: 0.4835\n",
      "Epoch 13/20\n",
      " - 3s - loss: 1.4622 - acc: 0.4848 - val_loss: 1.4354 - val_acc: 0.4877\n",
      "Epoch 14/20\n",
      " - 3s - loss: 1.4499 - acc: 0.4875 - val_loss: 1.4170 - val_acc: 0.4990\n",
      "Epoch 15/20\n",
      " - 3s - loss: 1.4369 - acc: 0.4930 - val_loss: 1.4155 - val_acc: 0.4995\n",
      "Epoch 16/20\n",
      " - 3s - loss: 1.4221 - acc: 0.4990 - val_loss: 1.4049 - val_acc: 0.5028\n",
      "Epoch 17/20\n",
      " - 3s - loss: 1.4121 - acc: 0.5014 - val_loss: 1.4099 - val_acc: 0.4989\n",
      "Epoch 18/20\n",
      " - 3s - loss: 1.4004 - acc: 0.5048 - val_loss: 1.3850 - val_acc: 0.5074\n",
      "Epoch 19/20\n",
      " - 3s - loss: 1.3845 - acc: 0.5126 - val_loss: 1.3787 - val_acc: 0.5093\n",
      "Epoch 20/20\n",
      " - 3s - loss: 1.3767 - acc: 0.5137 - val_loss: 1.3883 - val_acc: 0.5040\n",
      "10000/10000 [==============================] - 1s 64us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment15:\tAdam with learning rate @ 0.0001\n",
      "=======================================================\n",
      "Time Used: 71.0912810000001\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.3882944646835327\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.504\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 3s - loss: 2.1621 - acc: 0.2055 - val_loss: 2.0520 - val_acc: 0.2806\n",
      "Epoch 2/20\n",
      " - 2s - loss: 2.0712 - acc: 0.2489 - val_loss: 2.0005 - val_acc: 0.3029\n",
      "Epoch 3/20\n",
      " - 2s - loss: 2.0396 - acc: 0.2625 - val_loss: 1.9714 - val_acc: 0.3171\n",
      "Epoch 4/20\n",
      " - 2s - loss: 2.0138 - acc: 0.2728 - val_loss: 1.9498 - val_acc: 0.3224\n",
      "Epoch 5/20\n",
      " - 2s - loss: 1.9941 - acc: 0.2836 - val_loss: 1.9340 - val_acc: 0.3254\n",
      "Epoch 6/20\n",
      " - 2s - loss: 1.9823 - acc: 0.2900 - val_loss: 1.9221 - val_acc: 0.3337\n",
      "Epoch 7/20\n",
      " - 2s - loss: 1.9704 - acc: 0.2917 - val_loss: 1.9103 - val_acc: 0.3360\n",
      "Epoch 8/20\n",
      " - 2s - loss: 1.9578 - acc: 0.2999 - val_loss: 1.9016 - val_acc: 0.3407\n",
      "Epoch 9/20\n",
      " - 2s - loss: 1.9497 - acc: 0.3014 - val_loss: 1.8933 - val_acc: 0.3414\n",
      "Epoch 10/20\n",
      " - 2s - loss: 1.9450 - acc: 0.3024 - val_loss: 1.8867 - val_acc: 0.3463\n",
      "Epoch 11/20\n",
      " - 2s - loss: 1.9376 - acc: 0.3077 - val_loss: 1.8800 - val_acc: 0.3473\n",
      "Epoch 12/20\n",
      " - 2s - loss: 1.9291 - acc: 0.3119 - val_loss: 1.8737 - val_acc: 0.3496\n",
      "Epoch 13/20\n",
      " - 2s - loss: 1.9227 - acc: 0.3141 - val_loss: 1.8680 - val_acc: 0.3520\n",
      "Epoch 14/20\n",
      " - 2s - loss: 1.9192 - acc: 0.3158 - val_loss: 1.8629 - val_acc: 0.3506\n",
      "Epoch 15/20\n",
      " - 2s - loss: 1.9141 - acc: 0.3187 - val_loss: 1.8580 - val_acc: 0.3533\n",
      "Epoch 16/20\n",
      " - 2s - loss: 1.9070 - acc: 0.3200 - val_loss: 1.8532 - val_acc: 0.3556\n",
      "Epoch 17/20\n",
      " - 2s - loss: 1.9046 - acc: 0.3225 - val_loss: 1.8488 - val_acc: 0.3553\n",
      "Epoch 18/20\n",
      " - 2s - loss: 1.9013 - acc: 0.3237 - val_loss: 1.8448 - val_acc: 0.3583\n",
      "Epoch 19/20\n",
      " - 2s - loss: 1.8960 - acc: 0.3269 - val_loss: 1.8408 - val_acc: 0.3579\n",
      "Epoch 20/20\n",
      " - 2s - loss: 1.8899 - acc: 0.3262 - val_loss: 1.8369 - val_acc: 0.3613\n",
      "10000/10000 [==============================] - 1s 64us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment16:\tAdagrad with learning rate @ 0.0001\n",
      "=======================================================\n",
      "Time Used: 63.708857999999964\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.8368770179748535\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.3613\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 4s - loss: 2.5168 - acc: 0.0994 - val_loss: 2.4142 - val_acc: 0.0976\n",
      "Epoch 2/20\n",
      " - 3s - loss: 2.4594 - acc: 0.0997 - val_loss: 2.3672 - val_acc: 0.0953\n",
      "Epoch 3/20\n",
      " - 3s - loss: 2.4208 - acc: 0.1018 - val_loss: 2.3383 - val_acc: 0.0974\n",
      "Epoch 4/20\n",
      " - 3s - loss: 2.3956 - acc: 0.1079 - val_loss: 2.3187 - val_acc: 0.1002\n",
      "Epoch 5/20\n",
      " - 3s - loss: 2.3785 - acc: 0.1059 - val_loss: 2.3047 - val_acc: 0.1099\n",
      "Epoch 6/20\n",
      " - 3s - loss: 2.3564 - acc: 0.1164 - val_loss: 2.2937 - val_acc: 0.1227\n",
      "Epoch 7/20\n",
      " - 3s - loss: 2.3475 - acc: 0.1157 - val_loss: 2.2846 - val_acc: 0.1365\n",
      "Epoch 8/20\n",
      " - 3s - loss: 2.3404 - acc: 0.1193 - val_loss: 2.2768 - val_acc: 0.1461\n",
      "Epoch 9/20\n",
      " - 3s - loss: 2.3307 - acc: 0.1244 - val_loss: 2.2699 - val_acc: 0.1535\n",
      "Epoch 10/20\n",
      " - 3s - loss: 2.3197 - acc: 0.1277 - val_loss: 2.2636 - val_acc: 0.1614\n",
      "Epoch 11/20\n",
      " - 3s - loss: 2.3133 - acc: 0.1298 - val_loss: 2.2579 - val_acc: 0.1710\n",
      "Epoch 12/20\n",
      " - 3s - loss: 2.3085 - acc: 0.1325 - val_loss: 2.2525 - val_acc: 0.1795\n",
      "Epoch 13/20\n",
      " - 3s - loss: 2.3022 - acc: 0.1343 - val_loss: 2.2474 - val_acc: 0.1849\n",
      "Epoch 14/20\n",
      " - 3s - loss: 2.2966 - acc: 0.1354 - val_loss: 2.2426 - val_acc: 0.1871\n",
      "Epoch 15/20\n",
      " - 3s - loss: 2.2892 - acc: 0.1394 - val_loss: 2.2380 - val_acc: 0.1896\n",
      "Epoch 16/20\n",
      " - 3s - loss: 2.2845 - acc: 0.1421 - val_loss: 2.2336 - val_acc: 0.1917\n",
      "Epoch 17/20\n",
      " - 3s - loss: 2.2821 - acc: 0.1426 - val_loss: 2.2294 - val_acc: 0.1959\n",
      "Epoch 18/20\n",
      " - 3s - loss: 2.2762 - acc: 0.1480 - val_loss: 2.2254 - val_acc: 0.2007\n",
      "Epoch 19/20\n",
      " - 3s - loss: 2.2737 - acc: 0.1499 - val_loss: 2.2214 - val_acc: 0.2038\n",
      "Epoch 20/20\n",
      " - 3s - loss: 2.2677 - acc: 0.1534 - val_loss: 2.2176 - val_acc: 0.2059\n",
      "10000/10000 [==============================] - 1s 65us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment17:\tAdadelta with learning rate @ 0.0001\n",
      "=======================================================\n",
      "Time Used: 75.73260400000004\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.2176450057983397\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.2059\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 3s - loss: 2.3409 - acc: 0.1119 - val_loss: 2.2682 - val_acc: 0.1608\n",
      "Epoch 2/20\n",
      " - 2s - loss: 2.2841 - acc: 0.1397 - val_loss: 2.2366 - val_acc: 0.1857\n",
      "Epoch 3/20\n",
      " - 2s - loss: 2.2534 - acc: 0.1625 - val_loss: 2.2097 - val_acc: 0.2110\n",
      "Epoch 4/20\n",
      " - 2s - loss: 2.2292 - acc: 0.1808 - val_loss: 2.1869 - val_acc: 0.2356\n",
      "Epoch 5/20\n",
      " - 2s - loss: 2.2089 - acc: 0.1938 - val_loss: 2.1666 - val_acc: 0.2473\n",
      "Epoch 6/20\n",
      " - 2s - loss: 2.1920 - acc: 0.2027 - val_loss: 2.1480 - val_acc: 0.2598\n",
      "Epoch 7/20\n",
      " - 2s - loss: 2.1742 - acc: 0.2127 - val_loss: 2.1301 - val_acc: 0.2659\n",
      "Epoch 8/20\n",
      " - 2s - loss: 2.1560 - acc: 0.2202 - val_loss: 2.1132 - val_acc: 0.2704\n",
      "Epoch 9/20\n",
      " - 2s - loss: 2.1410 - acc: 0.2280 - val_loss: 2.0964 - val_acc: 0.2792\n",
      "Epoch 10/20\n",
      " - 2s - loss: 2.1283 - acc: 0.2307 - val_loss: 2.0807 - val_acc: 0.2856\n",
      "Epoch 11/20\n",
      " - 2s - loss: 2.1148 - acc: 0.2357 - val_loss: 2.0663 - val_acc: 0.2874\n",
      "Epoch 12/20\n",
      " - 2s - loss: 2.1004 - acc: 0.2412 - val_loss: 2.0525 - val_acc: 0.2898\n",
      "Epoch 13/20\n",
      " - 2s - loss: 2.0877 - acc: 0.2465 - val_loss: 2.0399 - val_acc: 0.2926\n",
      "Epoch 14/20\n",
      " - 2s - loss: 2.0772 - acc: 0.2498 - val_loss: 2.0280 - val_acc: 0.2959\n",
      "Epoch 15/20\n",
      " - 2s - loss: 2.0658 - acc: 0.2557 - val_loss: 2.0170 - val_acc: 0.2984\n",
      "Epoch 16/20\n",
      " - 2s - loss: 2.0556 - acc: 0.2580 - val_loss: 2.0061 - val_acc: 0.3038\n",
      "Epoch 17/20\n",
      " - 2s - loss: 2.0468 - acc: 0.2661 - val_loss: 1.9963 - val_acc: 0.3050\n",
      "Epoch 18/20\n",
      " - 2s - loss: 2.0342 - acc: 0.2675 - val_loss: 1.9867 - val_acc: 0.3102\n",
      "Epoch 19/20\n",
      " - 2s - loss: 2.0283 - acc: 0.2676 - val_loss: 1.9785 - val_acc: 0.3122\n",
      "Epoch 20/20\n",
      " - 2s - loss: 2.0190 - acc: 0.2720 - val_loss: 1.9700 - val_acc: 0.3181\n",
      "10000/10000 [==============================] - 1s 64us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment18:\tSGD with learning rate @ 0.0003\n",
      "=======================================================\n",
      "Time Used: 59.54585099999986\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.9699858245849609\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.3181\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 3s - loss: 2.1898 - acc: 0.2016 - val_loss: 2.0557 - val_acc: 0.2821\n",
      "Epoch 2/20\n",
      " - 2s - loss: 2.0540 - acc: 0.2583 - val_loss: 1.9599 - val_acc: 0.3188\n",
      "Epoch 3/20\n",
      " - 2s - loss: 1.9830 - acc: 0.2874 - val_loss: 1.9062 - val_acc: 0.3421\n",
      "Epoch 4/20\n",
      " - 2s - loss: 1.9353 - acc: 0.3059 - val_loss: 1.8655 - val_acc: 0.3562\n",
      "Epoch 5/20\n",
      " - 2s - loss: 1.9035 - acc: 0.3208 - val_loss: 1.8362 - val_acc: 0.3663\n",
      "Epoch 6/20\n",
      " - 2s - loss: 1.8735 - acc: 0.3327 - val_loss: 1.8127 - val_acc: 0.3700\n",
      "Epoch 7/20\n",
      " - 2s - loss: 1.8537 - acc: 0.3416 - val_loss: 1.7954 - val_acc: 0.3750\n",
      "Epoch 8/20\n",
      " - 2s - loss: 1.8328 - acc: 0.3480 - val_loss: 1.7765 - val_acc: 0.3786\n",
      "Epoch 9/20\n",
      " - 2s - loss: 1.8153 - acc: 0.3550 - val_loss: 1.7570 - val_acc: 0.3898\n",
      "Epoch 10/20\n",
      " - 2s - loss: 1.8014 - acc: 0.3624 - val_loss: 1.7453 - val_acc: 0.3953\n",
      "Epoch 11/20\n",
      " - 2s - loss: 1.7869 - acc: 0.3674 - val_loss: 1.7281 - val_acc: 0.3989\n",
      "Epoch 12/20\n",
      " - 2s - loss: 1.7743 - acc: 0.3729 - val_loss: 1.7159 - val_acc: 0.4000\n",
      "Epoch 13/20\n",
      " - 2s - loss: 1.7604 - acc: 0.3768 - val_loss: 1.7018 - val_acc: 0.4083\n",
      "Epoch 14/20\n",
      " - 2s - loss: 1.7483 - acc: 0.3801 - val_loss: 1.6937 - val_acc: 0.4049\n",
      "Epoch 15/20\n",
      " - 2s - loss: 1.7363 - acc: 0.3864 - val_loss: 1.6801 - val_acc: 0.4141\n",
      "Epoch 16/20\n",
      " - 2s - loss: 1.7301 - acc: 0.3876 - val_loss: 1.6709 - val_acc: 0.4154\n",
      "Epoch 17/20\n",
      " - 2s - loss: 1.7164 - acc: 0.3918 - val_loss: 1.6620 - val_acc: 0.4156\n",
      "Epoch 18/20\n",
      " - 2s - loss: 1.7078 - acc: 0.3980 - val_loss: 1.6510 - val_acc: 0.4236\n",
      "Epoch 19/20\n",
      " - 2s - loss: 1.7008 - acc: 0.3981 - val_loss: 1.6410 - val_acc: 0.4263\n",
      "Epoch 20/20\n",
      " - 2s - loss: 1.6888 - acc: 0.4048 - val_loss: 1.6338 - val_acc: 0.4297\n",
      "10000/10000 [==============================] - 1s 64us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment19:\tMomentum with learning rate @ 0.0003\n",
      "=======================================================\n",
      "Time Used: 59.64130399999999\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.6338176376342775\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.4297\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 4s - loss: 1.9946 - acc: 0.2725 - val_loss: 1.8418 - val_acc: 0.3433\n",
      "Epoch 2/20\n",
      " - 3s - loss: 1.8156 - acc: 0.3480 - val_loss: 1.7368 - val_acc: 0.3807\n",
      "Epoch 3/20\n",
      " - 3s - loss: 1.7406 - acc: 0.3788 - val_loss: 1.6624 - val_acc: 0.4013\n",
      "Epoch 4/20\n",
      " - 3s - loss: 1.6965 - acc: 0.3943 - val_loss: 1.6562 - val_acc: 0.4000\n",
      "Epoch 5/20\n",
      " - 3s - loss: 1.6556 - acc: 0.4127 - val_loss: 1.5876 - val_acc: 0.4334\n",
      "Epoch 6/20\n",
      " - 3s - loss: 1.6200 - acc: 0.4228 - val_loss: 1.5680 - val_acc: 0.4416\n",
      "Epoch 7/20\n",
      " - 3s - loss: 1.5910 - acc: 0.4337 - val_loss: 1.5114 - val_acc: 0.4635\n",
      "Epoch 8/20\n",
      " - 3s - loss: 1.5697 - acc: 0.4457 - val_loss: 1.5237 - val_acc: 0.4521\n",
      "Epoch 9/20\n",
      " - 3s - loss: 1.5493 - acc: 0.4495 - val_loss: 1.5016 - val_acc: 0.4620\n",
      "Epoch 10/20\n",
      " - 3s - loss: 1.5279 - acc: 0.4571 - val_loss: 1.6181 - val_acc: 0.4242\n",
      "Epoch 11/20\n",
      " - 3s - loss: 1.5102 - acc: 0.4643 - val_loss: 1.4361 - val_acc: 0.4869\n",
      "Epoch 12/20\n",
      " - 3s - loss: 1.4893 - acc: 0.4732 - val_loss: 1.4591 - val_acc: 0.4848\n",
      "Epoch 13/20\n",
      " - 3s - loss: 1.4774 - acc: 0.4759 - val_loss: 1.4602 - val_acc: 0.4731\n",
      "Epoch 14/20\n",
      " - 3s - loss: 1.4619 - acc: 0.4834 - val_loss: 1.4238 - val_acc: 0.4906\n",
      "Epoch 15/20\n",
      " - 3s - loss: 1.4473 - acc: 0.4859 - val_loss: 1.4535 - val_acc: 0.4823\n",
      "Epoch 16/20\n",
      " - 3s - loss: 1.4363 - acc: 0.4902 - val_loss: 1.4437 - val_acc: 0.4846\n",
      "Epoch 17/20\n",
      " - 3s - loss: 1.4235 - acc: 0.4917 - val_loss: 1.4593 - val_acc: 0.4859\n",
      "Epoch 18/20\n",
      " - 3s - loss: 1.4159 - acc: 0.4941 - val_loss: 1.4057 - val_acc: 0.5067\n",
      "Epoch 19/20\n",
      " - 3s - loss: 1.4028 - acc: 0.5022 - val_loss: 1.4444 - val_acc: 0.4801\n",
      "Epoch 20/20\n",
      " - 3s - loss: 1.3906 - acc: 0.5045 - val_loss: 1.4002 - val_acc: 0.5050\n",
      "10000/10000 [==============================] - 1s 65us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment20:\tRMSprop with learning rate @ 0.0003\n",
      "=======================================================\n",
      "Time Used: 65.99426199999994\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.400198949623108\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.505\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 4s - loss: 1.9661 - acc: 0.2821 - val_loss: 1.7899 - val_acc: 0.3621\n",
      "Epoch 2/20\n",
      " - 3s - loss: 1.7951 - acc: 0.3542 - val_loss: 1.6783 - val_acc: 0.4046\n",
      "Epoch 3/20\n",
      " - 3s - loss: 1.7292 - acc: 0.3827 - val_loss: 1.6185 - val_acc: 0.4303\n",
      "Epoch 4/20\n",
      " - 3s - loss: 1.6828 - acc: 0.3990 - val_loss: 1.5895 - val_acc: 0.4360\n",
      "Epoch 5/20\n",
      " - 3s - loss: 1.6446 - acc: 0.4162 - val_loss: 1.5585 - val_acc: 0.4456\n",
      "Epoch 6/20\n",
      " - 3s - loss: 1.6138 - acc: 0.4249 - val_loss: 1.5336 - val_acc: 0.4603\n",
      "Epoch 7/20\n",
      " - 3s - loss: 1.5916 - acc: 0.4309 - val_loss: 1.5138 - val_acc: 0.4584\n",
      "Epoch 8/20\n",
      " - 3s - loss: 1.5640 - acc: 0.4418 - val_loss: 1.4941 - val_acc: 0.4672\n",
      "Epoch 9/20\n",
      " - 3s - loss: 1.5478 - acc: 0.4476 - val_loss: 1.5077 - val_acc: 0.4653\n",
      "Epoch 10/20\n",
      " - 3s - loss: 1.5285 - acc: 0.4567 - val_loss: 1.4757 - val_acc: 0.4793\n",
      "Epoch 11/20\n",
      " - 3s - loss: 1.5133 - acc: 0.4616 - val_loss: 1.4591 - val_acc: 0.4867\n",
      "Epoch 12/20\n",
      " - 3s - loss: 1.4941 - acc: 0.4682 - val_loss: 1.4410 - val_acc: 0.4943\n",
      "Epoch 13/20\n",
      " - 3s - loss: 1.4899 - acc: 0.4698 - val_loss: 1.4219 - val_acc: 0.4987\n",
      "Epoch 14/20\n",
      " - 3s - loss: 1.4708 - acc: 0.4750 - val_loss: 1.4173 - val_acc: 0.4981\n",
      "Epoch 15/20\n",
      " - 3s - loss: 1.4569 - acc: 0.4813 - val_loss: 1.4119 - val_acc: 0.4963\n",
      "Epoch 16/20\n",
      " - 3s - loss: 1.4492 - acc: 0.4814 - val_loss: 1.4122 - val_acc: 0.4997\n",
      "Epoch 17/20\n",
      " - 3s - loss: 1.4353 - acc: 0.4863 - val_loss: 1.3854 - val_acc: 0.5083\n",
      "Epoch 18/20\n",
      " - 3s - loss: 1.4237 - acc: 0.4938 - val_loss: 1.3913 - val_acc: 0.5031\n",
      "Epoch 19/20\n",
      " - 3s - loss: 1.4199 - acc: 0.4943 - val_loss: 1.3875 - val_acc: 0.5090\n",
      "Epoch 20/20\n",
      " - 3s - loss: 1.4088 - acc: 0.4974 - val_loss: 1.3893 - val_acc: 0.5105\n",
      "10000/10000 [==============================] - 1s 64us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment21:\tAdam with learning rate @ 0.0003\n",
      "=======================================================\n",
      "Time Used: 71.90157599999998\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.3893477443695068\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.5105\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 4s - loss: 2.0615 - acc: 0.2507 - val_loss: 1.9269 - val_acc: 0.3326\n",
      "Epoch 2/20\n",
      " - 2s - loss: 1.9461 - acc: 0.3030 - val_loss: 1.8717 - val_acc: 0.3497\n",
      "Epoch 3/20\n",
      " - 2s - loss: 1.9037 - acc: 0.3226 - val_loss: 1.8399 - val_acc: 0.3659\n",
      "Epoch 4/20\n",
      " - 2s - loss: 1.8796 - acc: 0.3330 - val_loss: 1.8175 - val_acc: 0.3722\n",
      "Epoch 5/20\n",
      " - 2s - loss: 1.8578 - acc: 0.3407 - val_loss: 1.8020 - val_acc: 0.3741\n",
      "Epoch 6/20\n",
      " - 2s - loss: 1.8416 - acc: 0.3454 - val_loss: 1.7860 - val_acc: 0.3823\n",
      "Epoch 7/20\n",
      " - 2s - loss: 1.8269 - acc: 0.3539 - val_loss: 1.7719 - val_acc: 0.3870\n",
      "Epoch 8/20\n",
      " - 2s - loss: 1.8149 - acc: 0.3549 - val_loss: 1.7611 - val_acc: 0.3920\n",
      "Epoch 9/20\n",
      " - 2s - loss: 1.8052 - acc: 0.3608 - val_loss: 1.7510 - val_acc: 0.3937\n",
      "Epoch 10/20\n",
      " - 2s - loss: 1.7960 - acc: 0.3662 - val_loss: 1.7417 - val_acc: 0.3948\n",
      "Epoch 11/20\n",
      " - 2s - loss: 1.7879 - acc: 0.3665 - val_loss: 1.7342 - val_acc: 0.3994\n",
      "Epoch 12/20\n",
      " - 2s - loss: 1.7770 - acc: 0.3717 - val_loss: 1.7256 - val_acc: 0.4008\n",
      "Epoch 13/20\n",
      " - 2s - loss: 1.7709 - acc: 0.3748 - val_loss: 1.7184 - val_acc: 0.4030\n",
      "Epoch 14/20\n",
      " - 2s - loss: 1.7645 - acc: 0.3768 - val_loss: 1.7114 - val_acc: 0.4036\n",
      "Epoch 15/20\n",
      " - 2s - loss: 1.7563 - acc: 0.3788 - val_loss: 1.7050 - val_acc: 0.4079\n",
      "Epoch 16/20\n",
      " - 2s - loss: 1.7524 - acc: 0.3831 - val_loss: 1.6987 - val_acc: 0.4076\n",
      "Epoch 17/20\n",
      " - 2s - loss: 1.7440 - acc: 0.3828 - val_loss: 1.6918 - val_acc: 0.4102\n",
      "Epoch 18/20\n",
      " - 2s - loss: 1.7410 - acc: 0.3844 - val_loss: 1.6875 - val_acc: 0.4122\n",
      "Epoch 19/20\n",
      " - 2s - loss: 1.7360 - acc: 0.3865 - val_loss: 1.6821 - val_acc: 0.4117\n",
      "Epoch 20/20\n",
      " - 2s - loss: 1.7317 - acc: 0.3872 - val_loss: 1.6783 - val_acc: 0.4131\n",
      "10000/10000 [==============================] - 1s 65us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment22:\tAdagrad with learning rate @ 0.0003\n",
      "=======================================================\n",
      "Time Used: 64.51520200000004\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.6783363857269287\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.4131\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 4s - loss: 2.4583 - acc: 0.0941 - val_loss: 2.3395 - val_acc: 0.0880\n",
      "Epoch 2/20\n",
      " - 3s - loss: 2.3610 - acc: 0.1003 - val_loss: 2.2899 - val_acc: 0.1186\n",
      "Epoch 3/20\n",
      " - 3s - loss: 2.3188 - acc: 0.1137 - val_loss: 2.2670 - val_acc: 0.1476\n",
      "Epoch 4/20\n",
      " - 3s - loss: 2.2992 - acc: 0.1252 - val_loss: 2.2505 - val_acc: 0.1610\n",
      "Epoch 5/20\n",
      " - 3s - loss: 2.2828 - acc: 0.1344 - val_loss: 2.2367 - val_acc: 0.1752\n",
      "Epoch 6/20\n",
      " - 3s - loss: 2.2666 - acc: 0.1485 - val_loss: 2.2247 - val_acc: 0.1867\n",
      "Epoch 7/20\n",
      " - 3s - loss: 2.2578 - acc: 0.1515 - val_loss: 2.2137 - val_acc: 0.1961\n",
      "Epoch 8/20\n",
      " - 3s - loss: 2.2431 - acc: 0.1601 - val_loss: 2.2039 - val_acc: 0.2070\n",
      "Epoch 9/20\n",
      " - 3s - loss: 2.2360 - acc: 0.1669 - val_loss: 2.1950 - val_acc: 0.2122\n",
      "Epoch 10/20\n",
      " - 3s - loss: 2.2244 - acc: 0.1731 - val_loss: 2.1863 - val_acc: 0.2163\n",
      "Epoch 11/20\n",
      " - 3s - loss: 2.2179 - acc: 0.1751 - val_loss: 2.1781 - val_acc: 0.2244\n",
      "Epoch 12/20\n",
      " - 3s - loss: 2.2100 - acc: 0.1820 - val_loss: 2.1704 - val_acc: 0.2274\n",
      "Epoch 13/20\n",
      " - 3s - loss: 2.2009 - acc: 0.1839 - val_loss: 2.1630 - val_acc: 0.2333\n",
      "Epoch 14/20\n",
      " - 3s - loss: 2.1953 - acc: 0.1885 - val_loss: 2.1560 - val_acc: 0.2378\n",
      "Epoch 15/20\n",
      " - 3s - loss: 2.1884 - acc: 0.1950 - val_loss: 2.1490 - val_acc: 0.2391\n",
      "Epoch 16/20\n",
      " - 3s - loss: 2.1823 - acc: 0.1925 - val_loss: 2.1421 - val_acc: 0.2428\n",
      "Epoch 17/20\n",
      " - 3s - loss: 2.1766 - acc: 0.1975 - val_loss: 2.1356 - val_acc: 0.2436\n",
      "Epoch 18/20\n",
      " - 3s - loss: 2.1699 - acc: 0.1982 - val_loss: 2.1292 - val_acc: 0.2476\n",
      "Epoch 19/20\n",
      " - 3s - loss: 2.1627 - acc: 0.2059 - val_loss: 2.1235 - val_acc: 0.2482\n",
      "Epoch 20/20\n",
      " - 3s - loss: 2.1587 - acc: 0.2054 - val_loss: 2.1180 - val_acc: 0.2521\n",
      "10000/10000 [==============================] - 1s 66us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment23:\tAdadelta with learning rate @ 0.0003\n",
      "=======================================================\n",
      "Time Used: 76.76471600000013\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.117975177001953\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.2521\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 4s - loss: 2.2825 - acc: 0.1424 - val_loss: 2.1849 - val_acc: 0.2200\n",
      "Epoch 2/20\n",
      " - 2s - loss: 2.1839 - acc: 0.1938 - val_loss: 2.1132 - val_acc: 0.2566\n",
      "Epoch 3/20\n",
      " - 2s - loss: 2.1275 - acc: 0.2227 - val_loss: 2.0612 - val_acc: 0.2725\n",
      "Epoch 4/20\n",
      " - 2s - loss: 2.0814 - acc: 0.2405 - val_loss: 2.0210 - val_acc: 0.2956\n",
      "Epoch 5/20\n",
      " - 2s - loss: 2.0504 - acc: 0.2576 - val_loss: 1.9905 - val_acc: 0.3129\n",
      "Epoch 6/20\n",
      " - 2s - loss: 2.0234 - acc: 0.2667 - val_loss: 1.9643 - val_acc: 0.3183\n",
      "Epoch 7/20\n",
      " - 2s - loss: 2.0010 - acc: 0.2790 - val_loss: 1.9437 - val_acc: 0.3285\n",
      "Epoch 8/20\n",
      " - 2s - loss: 1.9815 - acc: 0.2853 - val_loss: 1.9228 - val_acc: 0.3324\n",
      "Epoch 9/20\n",
      " - 2s - loss: 1.9618 - acc: 0.2947 - val_loss: 1.9066 - val_acc: 0.3379\n",
      "Epoch 10/20\n",
      " - 2s - loss: 1.9465 - acc: 0.3023 - val_loss: 1.8916 - val_acc: 0.3446\n",
      "Epoch 11/20\n",
      " - 2s - loss: 1.9300 - acc: 0.3110 - val_loss: 1.8769 - val_acc: 0.3516\n",
      "Epoch 12/20\n",
      " - 2s - loss: 1.9188 - acc: 0.3128 - val_loss: 1.8656 - val_acc: 0.3552\n",
      "Epoch 13/20\n",
      " - 2s - loss: 1.9071 - acc: 0.3208 - val_loss: 1.8540 - val_acc: 0.3571\n",
      "Epoch 14/20\n",
      " - 2s - loss: 1.8971 - acc: 0.3233 - val_loss: 1.8434 - val_acc: 0.3616\n",
      "Epoch 15/20\n",
      " - 2s - loss: 1.8844 - acc: 0.3283 - val_loss: 1.8331 - val_acc: 0.3650\n",
      "Epoch 16/20\n",
      " - 2s - loss: 1.8766 - acc: 0.3311 - val_loss: 1.8240 - val_acc: 0.3649\n",
      "Epoch 17/20\n",
      " - 2s - loss: 1.8701 - acc: 0.3331 - val_loss: 1.8154 - val_acc: 0.3692\n",
      "Epoch 18/20\n",
      " - 2s - loss: 1.8588 - acc: 0.3382 - val_loss: 1.8070 - val_acc: 0.3719\n",
      "Epoch 19/20\n",
      " - 2s - loss: 1.8518 - acc: 0.3404 - val_loss: 1.7989 - val_acc: 0.3717\n",
      "Epoch 20/20\n",
      " - 2s - loss: 1.8435 - acc: 0.3435 - val_loss: 1.7925 - val_acc: 0.3782\n",
      "10000/10000 [==============================] - 1s 65us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment24:\tSGD with learning rate @ 0.001\n",
      "=======================================================\n",
      "Time Used: 60.528801999999814\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.7924569826126098\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.3782\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 4s - loss: 2.0771 - acc: 0.2404 - val_loss: 1.9111 - val_acc: 0.3338\n",
      "Epoch 2/20\n",
      " - 2s - loss: 1.9078 - acc: 0.3166 - val_loss: 1.8142 - val_acc: 0.3644\n",
      "Epoch 3/20\n",
      " - 2s - loss: 1.8360 - acc: 0.3459 - val_loss: 1.7552 - val_acc: 0.3893\n",
      "Epoch 4/20\n",
      " - 2s - loss: 1.7873 - acc: 0.3648 - val_loss: 1.7174 - val_acc: 0.3929\n",
      "Epoch 5/20\n",
      " - 2s - loss: 1.7522 - acc: 0.3752 - val_loss: 1.6780 - val_acc: 0.4076\n",
      "Epoch 6/20\n",
      " - 2s - loss: 1.7209 - acc: 0.3895 - val_loss: 1.6487 - val_acc: 0.4193\n",
      "Epoch 7/20\n",
      " - 2s - loss: 1.6971 - acc: 0.3977 - val_loss: 1.6329 - val_acc: 0.4220\n",
      "Epoch 8/20\n",
      " - 2s - loss: 1.6752 - acc: 0.4045 - val_loss: 1.6063 - val_acc: 0.4334\n",
      "Epoch 9/20\n",
      " - 2s - loss: 1.6520 - acc: 0.4159 - val_loss: 1.5841 - val_acc: 0.4446\n",
      "Epoch 10/20\n",
      " - 2s - loss: 1.6383 - acc: 0.4207 - val_loss: 1.5789 - val_acc: 0.4471\n",
      "Epoch 11/20\n",
      " - 2s - loss: 1.6193 - acc: 0.4246 - val_loss: 1.5592 - val_acc: 0.4536\n",
      "Epoch 12/20\n",
      " - 2s - loss: 1.6042 - acc: 0.4346 - val_loss: 1.5475 - val_acc: 0.4520\n",
      "Epoch 13/20\n",
      " - 2s - loss: 1.5879 - acc: 0.4392 - val_loss: 1.5271 - val_acc: 0.4620\n",
      "Epoch 14/20\n",
      " - 2s - loss: 1.5758 - acc: 0.4445 - val_loss: 1.5234 - val_acc: 0.4620\n",
      "Epoch 15/20\n",
      " - 2s - loss: 1.5614 - acc: 0.4484 - val_loss: 1.5133 - val_acc: 0.4597\n",
      "Epoch 16/20\n",
      " - 2s - loss: 1.5483 - acc: 0.4510 - val_loss: 1.4961 - val_acc: 0.4742\n",
      "Epoch 17/20\n",
      " - 2s - loss: 1.5423 - acc: 0.4534 - val_loss: 1.4956 - val_acc: 0.4730\n",
      "Epoch 18/20\n",
      " - 2s - loss: 1.5272 - acc: 0.4609 - val_loss: 1.4783 - val_acc: 0.4756\n",
      "Epoch 19/20\n",
      " - 2s - loss: 1.5174 - acc: 0.4634 - val_loss: 1.4777 - val_acc: 0.4788\n",
      "Epoch 20/20\n",
      " - 2s - loss: 1.5045 - acc: 0.4667 - val_loss: 1.4650 - val_acc: 0.4876\n",
      "10000/10000 [==============================] - 1s 65us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment25:\tMomentum with learning rate @ 0.001\n",
      "=======================================================\n",
      "Time Used: 60.700556000000006\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.4650259763717652\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.4876\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 4s - loss: 2.1265 - acc: 0.2417 - val_loss: 1.8811 - val_acc: 0.3207\n",
      "Epoch 2/20\n",
      " - 3s - loss: 1.8880 - acc: 0.3164 - val_loss: 1.8648 - val_acc: 0.3259\n",
      "Epoch 3/20\n",
      " - 3s - loss: 1.8145 - acc: 0.3451 - val_loss: 1.7591 - val_acc: 0.3743\n",
      "Epoch 4/20\n",
      " - 3s - loss: 1.7635 - acc: 0.3626 - val_loss: 1.6707 - val_acc: 0.4101\n",
      "Epoch 5/20\n",
      " - 3s - loss: 1.7307 - acc: 0.3770 - val_loss: 1.6757 - val_acc: 0.4053\n",
      "Epoch 6/20\n",
      " - 3s - loss: 1.7043 - acc: 0.3894 - val_loss: 1.6429 - val_acc: 0.4218\n",
      "Epoch 7/20\n",
      " - 3s - loss: 1.6820 - acc: 0.3955 - val_loss: 1.6381 - val_acc: 0.4189\n",
      "Epoch 8/20\n",
      " - 3s - loss: 1.6648 - acc: 0.4044 - val_loss: 1.6174 - val_acc: 0.4087\n",
      "Epoch 9/20\n",
      " - 3s - loss: 1.6485 - acc: 0.4087 - val_loss: 1.5865 - val_acc: 0.4441\n",
      "Epoch 10/20\n",
      " - 3s - loss: 1.6331 - acc: 0.4142 - val_loss: 1.6036 - val_acc: 0.4372\n",
      "Epoch 11/20\n",
      " - 3s - loss: 1.6249 - acc: 0.4187 - val_loss: 1.5871 - val_acc: 0.4354\n",
      "Epoch 12/20\n",
      " - 3s - loss: 1.6184 - acc: 0.4212 - val_loss: 1.6252 - val_acc: 0.4183\n",
      "Epoch 13/20\n",
      " - 3s - loss: 1.6058 - acc: 0.4240 - val_loss: 1.5407 - val_acc: 0.4660\n",
      "Epoch 14/20\n",
      " - 3s - loss: 1.5999 - acc: 0.4256 - val_loss: 1.5607 - val_acc: 0.4444\n",
      "Epoch 15/20\n",
      " - 3s - loss: 1.5915 - acc: 0.4321 - val_loss: 1.5569 - val_acc: 0.4521\n",
      "Epoch 16/20\n",
      " - 3s - loss: 1.5855 - acc: 0.4324 - val_loss: 1.5718 - val_acc: 0.4430\n",
      "Epoch 17/20\n",
      " - 3s - loss: 1.5757 - acc: 0.4371 - val_loss: 1.5622 - val_acc: 0.4406\n",
      "Epoch 18/20\n",
      " - 3s - loss: 1.5682 - acc: 0.4398 - val_loss: 1.5453 - val_acc: 0.4578\n",
      "Epoch 19/20\n",
      " - 3s - loss: 1.5642 - acc: 0.4408 - val_loss: 1.5905 - val_acc: 0.4298\n",
      "Epoch 20/20\n",
      " - 3s - loss: 1.5592 - acc: 0.4422 - val_loss: 1.5182 - val_acc: 0.4630\n",
      "10000/10000 [==============================] - 1s 66us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment26:\tRMSprop with learning rate @ 0.001\n",
      "=======================================================\n",
      "Time Used: 66.87957400000005\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.5182076189041138\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.463\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 5s - loss: 1.9697 - acc: 0.2803 - val_loss: 1.7891 - val_acc: 0.3680\n",
      "Epoch 2/20\n",
      " - 3s - loss: 1.8243 - acc: 0.3378 - val_loss: 1.7222 - val_acc: 0.3910\n",
      "Epoch 3/20\n",
      " - 3s - loss: 1.7817 - acc: 0.3548 - val_loss: 1.6747 - val_acc: 0.3997\n",
      "Epoch 4/20\n",
      " - 3s - loss: 1.7358 - acc: 0.3702 - val_loss: 1.6565 - val_acc: 0.4112\n",
      "Epoch 5/20\n",
      " - 3s - loss: 1.7144 - acc: 0.3821 - val_loss: 1.6371 - val_acc: 0.4161\n",
      "Epoch 6/20\n",
      " - 3s - loss: 1.6966 - acc: 0.3883 - val_loss: 1.6101 - val_acc: 0.4285\n",
      "Epoch 7/20\n",
      " - 3s - loss: 1.6748 - acc: 0.3963 - val_loss: 1.5974 - val_acc: 0.4396\n",
      "Epoch 8/20\n",
      " - 3s - loss: 1.6544 - acc: 0.4044 - val_loss: 1.5933 - val_acc: 0.4347\n",
      "Epoch 9/20\n",
      " - 3s - loss: 1.6516 - acc: 0.4054 - val_loss: 1.5782 - val_acc: 0.4420\n",
      "Epoch 10/20\n",
      " - 3s - loss: 1.6466 - acc: 0.4064 - val_loss: 1.5826 - val_acc: 0.4391\n",
      "Epoch 11/20\n",
      " - 3s - loss: 1.6369 - acc: 0.4107 - val_loss: 1.5719 - val_acc: 0.4303\n",
      "Epoch 12/20\n",
      " - 3s - loss: 1.6253 - acc: 0.4119 - val_loss: 1.5679 - val_acc: 0.4500\n",
      "Epoch 13/20\n",
      " - 3s - loss: 1.6127 - acc: 0.4187 - val_loss: 1.5593 - val_acc: 0.4476\n",
      "Epoch 14/20\n",
      " - 3s - loss: 1.6003 - acc: 0.4241 - val_loss: 1.5676 - val_acc: 0.4400\n",
      "Epoch 15/20\n",
      " - 3s - loss: 1.5974 - acc: 0.4230 - val_loss: 1.5523 - val_acc: 0.4442\n",
      "Epoch 16/20\n",
      " - 3s - loss: 1.5933 - acc: 0.4276 - val_loss: 1.5409 - val_acc: 0.4559\n",
      "Epoch 17/20\n",
      " - 3s - loss: 1.5907 - acc: 0.4296 - val_loss: 1.5553 - val_acc: 0.4612\n",
      "Epoch 18/20\n",
      " - 3s - loss: 1.5809 - acc: 0.4298 - val_loss: 1.5465 - val_acc: 0.4510\n",
      "Epoch 19/20\n",
      " - 3s - loss: 1.5787 - acc: 0.4313 - val_loss: 1.5147 - val_acc: 0.4691\n",
      "Epoch 20/20\n",
      " - 3s - loss: 1.5742 - acc: 0.4333 - val_loss: 1.5380 - val_acc: 0.4605\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment27:\tAdam with learning rate @ 0.001\n",
      "=======================================================\n",
      "Time Used: 73.36447799999996\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.5379644918441773\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.4605\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 4s - loss: 2.0236 - acc: 0.2636 - val_loss: 1.8592 - val_acc: 0.3518\n",
      "Epoch 2/20\n",
      " - 3s - loss: 1.8653 - acc: 0.3343 - val_loss: 1.7876 - val_acc: 0.3730\n",
      "Epoch 3/20\n",
      " - 3s - loss: 1.8120 - acc: 0.3573 - val_loss: 1.7382 - val_acc: 0.3894\n",
      "Epoch 4/20\n",
      " - 3s - loss: 1.7758 - acc: 0.3717 - val_loss: 1.7023 - val_acc: 0.4062\n",
      "Epoch 5/20\n",
      " - 3s - loss: 1.7461 - acc: 0.3822 - val_loss: 1.6787 - val_acc: 0.4112\n",
      "Epoch 6/20\n",
      " - 3s - loss: 1.7260 - acc: 0.3867 - val_loss: 1.6633 - val_acc: 0.4170\n",
      "Epoch 7/20\n",
      " - 3s - loss: 1.7055 - acc: 0.3973 - val_loss: 1.6403 - val_acc: 0.4208\n",
      "Epoch 8/20\n",
      " - 3s - loss: 1.6915 - acc: 0.4060 - val_loss: 1.6328 - val_acc: 0.4280\n",
      "Epoch 9/20\n",
      " - 3s - loss: 1.6763 - acc: 0.4088 - val_loss: 1.6125 - val_acc: 0.4389\n",
      "Epoch 10/20\n",
      " - 3s - loss: 1.6640 - acc: 0.4138 - val_loss: 1.6052 - val_acc: 0.4382\n",
      "Epoch 11/20\n",
      " - 3s - loss: 1.6539 - acc: 0.4168 - val_loss: 1.5943 - val_acc: 0.4410\n",
      "Epoch 12/20\n",
      " - 3s - loss: 1.6407 - acc: 0.4213 - val_loss: 1.5809 - val_acc: 0.4453\n",
      "Epoch 13/20\n",
      " - 3s - loss: 1.6329 - acc: 0.4252 - val_loss: 1.5735 - val_acc: 0.4458\n",
      "Epoch 14/20\n",
      " - 3s - loss: 1.6251 - acc: 0.4289 - val_loss: 1.5656 - val_acc: 0.4512\n",
      "Epoch 15/20\n",
      " - 3s - loss: 1.6162 - acc: 0.4308 - val_loss: 1.5613 - val_acc: 0.4536\n",
      "Epoch 16/20\n",
      " - 3s - loss: 1.6089 - acc: 0.4339 - val_loss: 1.5560 - val_acc: 0.4537\n",
      "Epoch 17/20\n",
      " - 3s - loss: 1.6000 - acc: 0.4365 - val_loss: 1.5447 - val_acc: 0.4621\n",
      "Epoch 18/20\n",
      " - 3s - loss: 1.5905 - acc: 0.4420 - val_loss: 1.5370 - val_acc: 0.4575\n",
      "Epoch 19/20\n",
      " - 3s - loss: 1.5866 - acc: 0.4426 - val_loss: 1.5312 - val_acc: 0.4636\n",
      "Epoch 20/20\n",
      " - 3s - loss: 1.5830 - acc: 0.4437 - val_loss: 1.5281 - val_acc: 0.4625\n",
      "10000/10000 [==============================] - 1s 67us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment28:\tAdagrad with learning rate @ 0.001\n",
      "=======================================================\n",
      "Time Used: 65.42363599999999\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.5280866161346436\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.4625\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 5s - loss: 2.3392 - acc: 0.1181 - val_loss: 2.2496 - val_acc: 0.1766\n",
      "Epoch 2/20\n",
      " - 3s - loss: 2.2707 - acc: 0.1481 - val_loss: 2.2101 - val_acc: 0.2069\n",
      "Epoch 3/20\n",
      " - 3s - loss: 2.2319 - acc: 0.1700 - val_loss: 2.1825 - val_acc: 0.2267\n",
      "Epoch 4/20\n",
      " - 3s - loss: 2.2070 - acc: 0.1832 - val_loss: 2.1593 - val_acc: 0.2441\n",
      "Epoch 5/20\n",
      " - 3s - loss: 2.1834 - acc: 0.1971 - val_loss: 2.1384 - val_acc: 0.2599\n",
      "Epoch 6/20\n",
      " - 3s - loss: 2.1645 - acc: 0.2090 - val_loss: 2.1196 - val_acc: 0.2667\n",
      "Epoch 7/20\n",
      " - 3s - loss: 2.1470 - acc: 0.2194 - val_loss: 2.1027 - val_acc: 0.2718\n",
      "Epoch 8/20\n",
      " - 3s - loss: 2.1301 - acc: 0.2252 - val_loss: 2.0868 - val_acc: 0.2802\n",
      "Epoch 9/20\n",
      " - 3s - loss: 2.1182 - acc: 0.2308 - val_loss: 2.0726 - val_acc: 0.2833\n",
      "Epoch 10/20\n",
      " - 3s - loss: 2.1044 - acc: 0.2334 - val_loss: 2.0589 - val_acc: 0.2881\n",
      "Epoch 11/20\n",
      " - 3s - loss: 2.0917 - acc: 0.2408 - val_loss: 2.0463 - val_acc: 0.2976\n",
      "Epoch 12/20\n",
      " - 3s - loss: 2.0824 - acc: 0.2475 - val_loss: 2.0347 - val_acc: 0.2988\n",
      "Epoch 13/20\n",
      " - 3s - loss: 2.0718 - acc: 0.2519 - val_loss: 2.0237 - val_acc: 0.3027\n",
      "Epoch 14/20\n",
      " - 3s - loss: 2.0611 - acc: 0.2582 - val_loss: 2.0137 - val_acc: 0.3065\n",
      "Epoch 15/20\n",
      " - 3s - loss: 2.0508 - acc: 0.2608 - val_loss: 2.0039 - val_acc: 0.3116\n",
      "Epoch 16/20\n",
      " - 3s - loss: 2.0462 - acc: 0.2602 - val_loss: 1.9951 - val_acc: 0.3142\n",
      "Epoch 17/20\n",
      " - 3s - loss: 2.0376 - acc: 0.2643 - val_loss: 1.9871 - val_acc: 0.3155\n",
      "Epoch 18/20\n",
      " - 3s - loss: 2.0268 - acc: 0.2713 - val_loss: 1.9787 - val_acc: 0.3180\n",
      "Epoch 19/20\n",
      " - 3s - loss: 2.0195 - acc: 0.2728 - val_loss: 1.9710 - val_acc: 0.3223\n",
      "Epoch 20/20\n",
      " - 3s - loss: 2.0136 - acc: 0.2763 - val_loss: 1.9634 - val_acc: 0.3247\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment29:\tAdadelta with learning rate @ 0.001\n",
      "=======================================================\n",
      "Time Used: 77.96233200000006\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.9634053466796875\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.3247\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 4s - loss: 2.1918 - acc: 0.1874 - val_loss: 2.0434 - val_acc: 0.2678\n",
      "Epoch 2/20\n",
      " - 2s - loss: 2.0387 - acc: 0.2576 - val_loss: 1.9539 - val_acc: 0.3120\n",
      "Epoch 3/20\n",
      " - 2s - loss: 1.9715 - acc: 0.2887 - val_loss: 1.8993 - val_acc: 0.3392\n",
      "Epoch 4/20\n",
      " - 2s - loss: 1.9293 - acc: 0.3099 - val_loss: 1.8636 - val_acc: 0.3499\n",
      "Epoch 5/20\n",
      " - 2s - loss: 1.8947 - acc: 0.3243 - val_loss: 1.8360 - val_acc: 0.3626\n",
      "Epoch 6/20\n",
      " - 2s - loss: 1.8679 - acc: 0.3356 - val_loss: 1.8073 - val_acc: 0.3732\n",
      "Epoch 7/20\n",
      " - 2s - loss: 1.8439 - acc: 0.3494 - val_loss: 1.7887 - val_acc: 0.3815\n",
      "Epoch 8/20\n",
      " - 2s - loss: 1.8239 - acc: 0.3541 - val_loss: 1.7648 - val_acc: 0.3861\n",
      "Epoch 9/20\n",
      " - 2s - loss: 1.8044 - acc: 0.3601 - val_loss: 1.7486 - val_acc: 0.3881\n",
      "Epoch 10/20\n",
      " - 2s - loss: 1.7886 - acc: 0.3664 - val_loss: 1.7335 - val_acc: 0.3959\n",
      "Epoch 11/20\n",
      " - 2s - loss: 1.7722 - acc: 0.3725 - val_loss: 1.7200 - val_acc: 0.4019\n",
      "Epoch 12/20\n",
      " - 2s - loss: 1.7607 - acc: 0.3772 - val_loss: 1.7022 - val_acc: 0.4082\n",
      "Epoch 13/20\n",
      " - 2s - loss: 1.7485 - acc: 0.3804 - val_loss: 1.6892 - val_acc: 0.4099\n",
      "Epoch 14/20\n",
      " - 2s - loss: 1.7340 - acc: 0.3848 - val_loss: 1.6830 - val_acc: 0.4143\n",
      "Epoch 15/20\n",
      " - 2s - loss: 1.7224 - acc: 0.3906 - val_loss: 1.6719 - val_acc: 0.4151\n",
      "Epoch 16/20\n",
      " - 2s - loss: 1.7116 - acc: 0.3964 - val_loss: 1.6566 - val_acc: 0.4198\n",
      "Epoch 17/20\n",
      " - 2s - loss: 1.7023 - acc: 0.4005 - val_loss: 1.6457 - val_acc: 0.4245\n",
      "Epoch 18/20\n",
      " - 2s - loss: 1.6932 - acc: 0.4018 - val_loss: 1.6379 - val_acc: 0.4269\n",
      "Epoch 19/20\n",
      " - 2s - loss: 1.6834 - acc: 0.4047 - val_loss: 1.6325 - val_acc: 0.4302\n",
      "Epoch 20/20\n",
      " - 2s - loss: 1.6755 - acc: 0.4105 - val_loss: 1.6216 - val_acc: 0.4314\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment30:\tSGD with learning rate @ 0.003\n",
      "=======================================================\n",
      "Time Used: 61.44478300000014\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.6216302629470825\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.4314\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 4s - loss: 1.9916 - acc: 0.2731 - val_loss: 1.8170 - val_acc: 0.3495\n",
      "Epoch 2/20\n",
      " - 2s - loss: 1.8109 - acc: 0.3518 - val_loss: 1.7077 - val_acc: 0.3942\n",
      "Epoch 3/20\n",
      " - 2s - loss: 1.7287 - acc: 0.3853 - val_loss: 1.6431 - val_acc: 0.4196\n",
      "Epoch 4/20\n",
      " - 2s - loss: 1.6795 - acc: 0.4032 - val_loss: 1.5915 - val_acc: 0.4370\n",
      "Epoch 5/20\n",
      " - 2s - loss: 1.6431 - acc: 0.4181 - val_loss: 1.5684 - val_acc: 0.4393\n",
      "Epoch 6/20\n",
      " - 2s - loss: 1.6118 - acc: 0.4279 - val_loss: 1.5362 - val_acc: 0.4545\n",
      "Epoch 7/20\n",
      " - 2s - loss: 1.5853 - acc: 0.4374 - val_loss: 1.5046 - val_acc: 0.4649\n",
      "Epoch 8/20\n",
      " - 2s - loss: 1.5576 - acc: 0.4489 - val_loss: 1.4885 - val_acc: 0.4703\n",
      "Epoch 9/20\n",
      " - 2s - loss: 1.5429 - acc: 0.4547 - val_loss: 1.4736 - val_acc: 0.4757\n",
      "Epoch 10/20\n",
      " - 2s - loss: 1.5186 - acc: 0.4610 - val_loss: 1.4740 - val_acc: 0.4784\n",
      "Epoch 11/20\n",
      " - 2s - loss: 1.5037 - acc: 0.4669 - val_loss: 1.4423 - val_acc: 0.4887\n",
      "Epoch 12/20\n",
      " - 2s - loss: 1.4834 - acc: 0.4726 - val_loss: 1.4565 - val_acc: 0.4829\n",
      "Epoch 13/20\n",
      " - 2s - loss: 1.4692 - acc: 0.4786 - val_loss: 1.4548 - val_acc: 0.4799\n",
      "Epoch 14/20\n",
      " - 2s - loss: 1.4595 - acc: 0.4822 - val_loss: 1.4122 - val_acc: 0.4960\n",
      "Epoch 15/20\n",
      " - 2s - loss: 1.4386 - acc: 0.4896 - val_loss: 1.4182 - val_acc: 0.4982\n",
      "Epoch 16/20\n",
      " - 2s - loss: 1.4280 - acc: 0.4922 - val_loss: 1.4069 - val_acc: 0.4989\n",
      "Epoch 17/20\n",
      " - 2s - loss: 1.4199 - acc: 0.4951 - val_loss: 1.3824 - val_acc: 0.5050\n",
      "Epoch 18/20\n",
      " - 2s - loss: 1.4106 - acc: 0.4987 - val_loss: 1.3949 - val_acc: 0.5006\n",
      "Epoch 19/20\n",
      " - 2s - loss: 1.3998 - acc: 0.5005 - val_loss: 1.3699 - val_acc: 0.5128\n",
      "Epoch 20/20\n",
      " - 2s - loss: 1.3845 - acc: 0.5092 - val_loss: 1.3895 - val_acc: 0.5067\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment31:\tMomentum with learning rate @ 0.003\n",
      "=======================================================\n",
      "Time Used: 61.43163400000003\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.3895227848052978\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.5067\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 5s - loss: 10.7523 - acc: 0.1210 - val_loss: 2.1462 - val_acc: 0.1932\n",
      "Epoch 2/20\n",
      " - 3s - loss: 2.0538 - acc: 0.2290 - val_loss: 2.0051 - val_acc: 0.2687\n",
      "Epoch 3/20\n",
      " - 3s - loss: 1.9728 - acc: 0.2713 - val_loss: 1.8744 - val_acc: 0.3274\n",
      "Epoch 4/20\n",
      " - 3s - loss: 1.9343 - acc: 0.2878 - val_loss: 1.8466 - val_acc: 0.3317\n",
      "Epoch 5/20\n",
      " - 3s - loss: 1.9104 - acc: 0.3016 - val_loss: 1.8823 - val_acc: 0.3061\n",
      "Epoch 6/20\n",
      " - 3s - loss: 1.8963 - acc: 0.3072 - val_loss: 1.8133 - val_acc: 0.3395\n",
      "Epoch 7/20\n",
      " - 3s - loss: 1.8861 - acc: 0.3105 - val_loss: 1.8102 - val_acc: 0.3409\n",
      "Epoch 8/20\n",
      " - 3s - loss: 1.8750 - acc: 0.3137 - val_loss: 1.8379 - val_acc: 0.3255\n",
      "Epoch 9/20\n",
      " - 3s - loss: 1.8726 - acc: 0.3160 - val_loss: 1.7848 - val_acc: 0.3554\n",
      "Epoch 10/20\n",
      " - 3s - loss: 1.8612 - acc: 0.3192 - val_loss: 1.8647 - val_acc: 0.2991\n",
      "Epoch 11/20\n",
      " - 3s - loss: 1.8506 - acc: 0.3256 - val_loss: 1.7629 - val_acc: 0.3675\n",
      "Epoch 12/20\n",
      " - 3s - loss: 1.8489 - acc: 0.3265 - val_loss: 1.7566 - val_acc: 0.3577\n",
      "Epoch 13/20\n",
      " - 3s - loss: 1.8396 - acc: 0.3333 - val_loss: 1.7543 - val_acc: 0.3693\n",
      "Epoch 14/20\n",
      " - 3s - loss: 1.8329 - acc: 0.3329 - val_loss: 1.7640 - val_acc: 0.3603\n",
      "Epoch 15/20\n",
      " - 3s - loss: 1.8290 - acc: 0.3353 - val_loss: 1.7303 - val_acc: 0.3616\n",
      "Epoch 16/20\n",
      " - 3s - loss: 1.8237 - acc: 0.3389 - val_loss: 1.7635 - val_acc: 0.3597\n",
      "Epoch 17/20\n",
      " - 3s - loss: 1.8199 - acc: 0.3381 - val_loss: 1.7365 - val_acc: 0.3605\n",
      "Epoch 18/20\n",
      " - 3s - loss: 1.8129 - acc: 0.3418 - val_loss: 1.7905 - val_acc: 0.3378\n",
      "Epoch 19/20\n",
      " - 3s - loss: 1.8130 - acc: 0.3396 - val_loss: 1.7151 - val_acc: 0.4066\n",
      "Epoch 20/20\n",
      " - 3s - loss: 1.8082 - acc: 0.3443 - val_loss: 1.7434 - val_acc: 0.3736\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment32:\tRMSprop with learning rate @ 0.003\n",
      "=======================================================\n",
      "Time Used: 67.8739370000003\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.743436336517334\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.3736\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 5s - loss: 2.0560 - acc: 0.2451 - val_loss: 1.8987 - val_acc: 0.3047\n",
      "Epoch 2/20\n",
      " - 3s - loss: 1.9264 - acc: 0.2849 - val_loss: 1.8512 - val_acc: 0.3026\n",
      "Epoch 3/20\n",
      " - 3s - loss: 1.9104 - acc: 0.2943 - val_loss: 1.8237 - val_acc: 0.3537\n",
      "Epoch 4/20\n",
      " - 3s - loss: 1.8839 - acc: 0.3089 - val_loss: 1.8745 - val_acc: 0.3297\n",
      "Epoch 5/20\n",
      " - 3s - loss: 1.8840 - acc: 0.3099 - val_loss: 1.8141 - val_acc: 0.3585\n",
      "Epoch 6/20\n",
      " - 3s - loss: 1.8695 - acc: 0.3155 - val_loss: 1.7864 - val_acc: 0.3600\n",
      "Epoch 7/20\n",
      " - 3s - loss: 1.8561 - acc: 0.3201 - val_loss: 1.7515 - val_acc: 0.3779\n",
      "Epoch 8/20\n",
      " - 3s - loss: 1.8615 - acc: 0.3209 - val_loss: 1.8189 - val_acc: 0.3260\n",
      "Epoch 9/20\n",
      " - 3s - loss: 1.8566 - acc: 0.3200 - val_loss: 1.7617 - val_acc: 0.3715\n",
      "Epoch 10/20\n",
      " - 3s - loss: 1.8498 - acc: 0.3224 - val_loss: 1.7732 - val_acc: 0.3623\n",
      "Epoch 11/20\n",
      " - 3s - loss: 1.8353 - acc: 0.3321 - val_loss: 1.8086 - val_acc: 0.3315\n",
      "Epoch 12/20\n",
      " - 3s - loss: 1.8439 - acc: 0.3252 - val_loss: 1.7874 - val_acc: 0.3638\n",
      "Epoch 13/20\n",
      " - 3s - loss: 1.8397 - acc: 0.3282 - val_loss: 1.7677 - val_acc: 0.3618\n",
      "Epoch 14/20\n",
      " - 3s - loss: 1.8356 - acc: 0.3275 - val_loss: 1.7505 - val_acc: 0.3654\n",
      "Epoch 15/20\n",
      " - 3s - loss: 1.8359 - acc: 0.3314 - val_loss: 1.7463 - val_acc: 0.3723\n",
      "Epoch 16/20\n",
      " - 3s - loss: 1.8347 - acc: 0.3296 - val_loss: 1.7545 - val_acc: 0.3693\n",
      "Epoch 17/20\n",
      " - 3s - loss: 1.8292 - acc: 0.3326 - val_loss: 1.7413 - val_acc: 0.3777\n",
      "Epoch 18/20\n",
      " - 3s - loss: 1.8369 - acc: 0.3268 - val_loss: 1.7517 - val_acc: 0.3704\n",
      "Epoch 19/20\n",
      " - 3s - loss: 1.8254 - acc: 0.3310 - val_loss: 1.7628 - val_acc: 0.3606\n",
      "Epoch 20/20\n",
      " - 3s - loss: 1.8290 - acc: 0.3375 - val_loss: 1.7536 - val_acc: 0.3756\n",
      "10000/10000 [==============================] - 1s 69us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment33:\tAdam with learning rate @ 0.003\n",
      "=======================================================\n",
      "Time Used: 74.34771300000011\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.7536402839660645\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.3756\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 5s - loss: 2.0584 - acc: 0.2588 - val_loss: 1.8408 - val_acc: 0.3461\n",
      "Epoch 2/20\n",
      " - 3s - loss: 1.8546 - acc: 0.3356 - val_loss: 1.7601 - val_acc: 0.3769\n",
      "Epoch 3/20\n",
      " - 3s - loss: 1.7879 - acc: 0.3633 - val_loss: 1.7048 - val_acc: 0.3933\n",
      "Epoch 4/20\n",
      " - 3s - loss: 1.7476 - acc: 0.3763 - val_loss: 1.6688 - val_acc: 0.4076\n",
      "Epoch 5/20\n",
      " - 3s - loss: 1.7221 - acc: 0.3870 - val_loss: 1.6462 - val_acc: 0.4190\n",
      "Epoch 6/20\n",
      " - 3s - loss: 1.7009 - acc: 0.3965 - val_loss: 1.6212 - val_acc: 0.4321\n",
      "Epoch 7/20\n",
      " - 3s - loss: 1.6822 - acc: 0.4026 - val_loss: 1.6095 - val_acc: 0.4330\n",
      "Epoch 8/20\n",
      " - 3s - loss: 1.6657 - acc: 0.4085 - val_loss: 1.5936 - val_acc: 0.4381\n",
      "Epoch 9/20\n",
      " - 3s - loss: 1.6532 - acc: 0.4148 - val_loss: 1.5764 - val_acc: 0.4447\n",
      "Epoch 10/20\n",
      " - 3s - loss: 1.6336 - acc: 0.4193 - val_loss: 1.5749 - val_acc: 0.4476\n",
      "Epoch 11/20\n",
      " - 3s - loss: 1.6229 - acc: 0.4261 - val_loss: 1.5569 - val_acc: 0.4545\n",
      "Epoch 12/20\n",
      " - 3s - loss: 1.6129 - acc: 0.4292 - val_loss: 1.5471 - val_acc: 0.4571\n",
      "Epoch 13/20\n",
      " - 3s - loss: 1.6044 - acc: 0.4307 - val_loss: 1.5368 - val_acc: 0.4640\n",
      "Epoch 14/20\n",
      " - 3s - loss: 1.5914 - acc: 0.4374 - val_loss: 1.5254 - val_acc: 0.4595\n",
      "Epoch 15/20\n",
      " - 3s - loss: 1.5853 - acc: 0.4399 - val_loss: 1.5201 - val_acc: 0.4687\n",
      "Epoch 16/20\n",
      " - 3s - loss: 1.5752 - acc: 0.4416 - val_loss: 1.5176 - val_acc: 0.4647\n",
      "Epoch 17/20\n",
      " - 3s - loss: 1.5675 - acc: 0.4486 - val_loss: 1.5088 - val_acc: 0.4667\n",
      "Epoch 18/20\n",
      " - 3s - loss: 1.5612 - acc: 0.4484 - val_loss: 1.4994 - val_acc: 0.4715\n",
      "Epoch 19/20\n",
      " - 3s - loss: 1.5525 - acc: 0.4519 - val_loss: 1.4871 - val_acc: 0.4720\n",
      "Epoch 20/20\n",
      " - 3s - loss: 1.5475 - acc: 0.4537 - val_loss: 1.4929 - val_acc: 0.4758\n",
      "10000/10000 [==============================] - 1s 69us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment34:\tAdagrad with learning rate @ 0.003\n",
      "=======================================================\n",
      "Time Used: 66.18453999999974\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.4929234172821044\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.4758\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 5s - loss: 2.2897 - acc: 0.1424 - val_loss: 2.1739 - val_acc: 0.2195\n",
      "Epoch 2/20\n",
      " - 3s - loss: 2.1848 - acc: 0.1906 - val_loss: 2.1120 - val_acc: 0.2550\n",
      "Epoch 3/20\n",
      " - 3s - loss: 2.1341 - acc: 0.2188 - val_loss: 2.0690 - val_acc: 0.2755\n",
      "Epoch 4/20\n",
      " - 3s - loss: 2.0970 - acc: 0.2337 - val_loss: 2.0333 - val_acc: 0.2854\n",
      "Epoch 5/20\n",
      " - 3s - loss: 2.0652 - acc: 0.2500 - val_loss: 2.0036 - val_acc: 0.3012\n",
      "Epoch 6/20\n",
      " - 3s - loss: 2.0394 - acc: 0.2629 - val_loss: 1.9793 - val_acc: 0.3075\n",
      "Epoch 7/20\n",
      " - 3s - loss: 2.0163 - acc: 0.2702 - val_loss: 1.9579 - val_acc: 0.3173\n",
      "Epoch 8/20\n",
      " - 3s - loss: 1.9982 - acc: 0.2799 - val_loss: 1.9392 - val_acc: 0.3229\n",
      "Epoch 9/20\n",
      " - 3s - loss: 1.9803 - acc: 0.2846 - val_loss: 1.9227 - val_acc: 0.3298\n",
      "Epoch 10/20\n",
      " - 3s - loss: 1.9655 - acc: 0.2923 - val_loss: 1.9078 - val_acc: 0.3344\n",
      "Epoch 11/20\n",
      " - 3s - loss: 1.9512 - acc: 0.3015 - val_loss: 1.8938 - val_acc: 0.3390\n",
      "Epoch 12/20\n",
      " - 3s - loss: 1.9384 - acc: 0.3080 - val_loss: 1.8821 - val_acc: 0.3419\n",
      "Epoch 13/20\n",
      " - 3s - loss: 1.9246 - acc: 0.3094 - val_loss: 1.8710 - val_acc: 0.3479\n",
      "Epoch 14/20\n",
      " - 3s - loss: 1.9155 - acc: 0.3145 - val_loss: 1.8604 - val_acc: 0.3488\n",
      "Epoch 15/20\n",
      " - 3s - loss: 1.9058 - acc: 0.3191 - val_loss: 1.8502 - val_acc: 0.3548\n",
      "Epoch 16/20\n",
      " - 3s - loss: 1.8958 - acc: 0.3234 - val_loss: 1.8413 - val_acc: 0.3568\n",
      "Epoch 17/20\n",
      " - 3s - loss: 1.8872 - acc: 0.3261 - val_loss: 1.8326 - val_acc: 0.3622\n",
      "Epoch 18/20\n",
      " - 3s - loss: 1.8794 - acc: 0.3300 - val_loss: 1.8236 - val_acc: 0.3606\n",
      "Epoch 19/20\n",
      " - 3s - loss: 1.8694 - acc: 0.3367 - val_loss: 1.8166 - val_acc: 0.3641\n",
      "Epoch 20/20\n",
      " - 3s - loss: 1.8614 - acc: 0.3387 - val_loss: 1.8095 - val_acc: 0.3655\n",
      "10000/10000 [==============================] - 1s 69us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment35:\tAdadelta with learning rate @ 0.003\n",
      "=======================================================\n",
      "Time Used: 79.11481299999969\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.8095139251708985\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.3655\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 5s - loss: 2.0878 - acc: 0.2358 - val_loss: 1.9120 - val_acc: 0.3310\n",
      "Epoch 2/20\n",
      " - 2s - loss: 1.9150 - acc: 0.3156 - val_loss: 1.8251 - val_acc: 0.3587\n",
      "Epoch 3/20\n",
      " - 2s - loss: 1.8403 - acc: 0.3454 - val_loss: 1.7609 - val_acc: 0.3896\n",
      "Epoch 4/20\n",
      " - 2s - loss: 1.7856 - acc: 0.3658 - val_loss: 1.7093 - val_acc: 0.4004\n",
      "Epoch 5/20\n",
      " - 2s - loss: 1.7497 - acc: 0.3796 - val_loss: 1.6797 - val_acc: 0.4123\n",
      "Epoch 6/20\n",
      " - 2s - loss: 1.7184 - acc: 0.3906 - val_loss: 1.6484 - val_acc: 0.4209\n",
      "Epoch 7/20\n",
      " - 2s - loss: 1.6955 - acc: 0.4009 - val_loss: 1.6572 - val_acc: 0.4200\n",
      "Epoch 8/20\n",
      " - 2s - loss: 1.6719 - acc: 0.4091 - val_loss: 1.6184 - val_acc: 0.4328\n",
      "Epoch 9/20\n",
      " - 2s - loss: 1.6492 - acc: 0.4175 - val_loss: 1.5946 - val_acc: 0.4370\n",
      "Epoch 10/20\n",
      " - 2s - loss: 1.6311 - acc: 0.4242 - val_loss: 1.5794 - val_acc: 0.4463\n",
      "Epoch 11/20\n",
      " - 2s - loss: 1.6180 - acc: 0.4300 - val_loss: 1.5594 - val_acc: 0.4530\n",
      "Epoch 12/20\n",
      " - 2s - loss: 1.6014 - acc: 0.4353 - val_loss: 1.5784 - val_acc: 0.4378\n",
      "Epoch 13/20\n",
      " - 2s - loss: 1.5858 - acc: 0.4414 - val_loss: 1.5311 - val_acc: 0.4652\n",
      "Epoch 14/20\n",
      " - 2s - loss: 1.5731 - acc: 0.4437 - val_loss: 1.5393 - val_acc: 0.4502\n",
      "Epoch 15/20\n",
      " - 2s - loss: 1.5601 - acc: 0.4506 - val_loss: 1.5092 - val_acc: 0.4704\n",
      "Epoch 16/20\n",
      " - 2s - loss: 1.5489 - acc: 0.4524 - val_loss: 1.5019 - val_acc: 0.4719\n",
      "Epoch 17/20\n",
      " - 2s - loss: 1.5402 - acc: 0.4560 - val_loss: 1.5053 - val_acc: 0.4682\n",
      "Epoch 18/20\n",
      " - 2s - loss: 1.5258 - acc: 0.4609 - val_loss: 1.4944 - val_acc: 0.4781\n",
      "Epoch 19/20\n",
      " - 2s - loss: 1.5152 - acc: 0.4657 - val_loss: 1.4860 - val_acc: 0.4710\n",
      "Epoch 20/20\n",
      " - 2s - loss: 1.5055 - acc: 0.4694 - val_loss: 1.4748 - val_acc: 0.4752\n",
      "10000/10000 [==============================] - 1s 69us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment36:\tSGD with learning rate @ 0.01\n",
      "=======================================================\n",
      "Time Used: 62.50948900000003\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.4748447326660157\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.4752\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 5s - loss: 1.9423 - acc: 0.2927 - val_loss: 1.7959 - val_acc: 0.3528\n",
      "Epoch 2/20\n",
      " - 2s - loss: 1.7891 - acc: 0.3574 - val_loss: 1.6984 - val_acc: 0.4014\n",
      "Epoch 3/20\n",
      " - 2s - loss: 1.7284 - acc: 0.3808 - val_loss: 1.6122 - val_acc: 0.4270\n",
      "Epoch 4/20\n",
      " - 2s - loss: 1.6859 - acc: 0.3945 - val_loss: 1.5728 - val_acc: 0.4484\n",
      "Epoch 5/20\n",
      " - 2s - loss: 1.6456 - acc: 0.4131 - val_loss: 1.5538 - val_acc: 0.4482\n",
      "Epoch 6/20\n",
      " - 2s - loss: 1.6151 - acc: 0.4224 - val_loss: 1.5304 - val_acc: 0.4581\n",
      "Epoch 7/20\n",
      " - 2s - loss: 1.5974 - acc: 0.4270 - val_loss: 1.5239 - val_acc: 0.4518\n",
      "Epoch 8/20\n",
      " - 2s - loss: 1.5646 - acc: 0.4379 - val_loss: 1.4927 - val_acc: 0.4637\n",
      "Epoch 9/20\n",
      " - 2s - loss: 1.5508 - acc: 0.4425 - val_loss: 1.4829 - val_acc: 0.4687\n",
      "Epoch 10/20\n",
      " - 2s - loss: 1.5321 - acc: 0.4516 - val_loss: 1.4684 - val_acc: 0.4733\n",
      "Epoch 11/20\n",
      " - 2s - loss: 1.5131 - acc: 0.4580 - val_loss: 1.4474 - val_acc: 0.4808\n",
      "Epoch 12/20\n",
      " - 2s - loss: 1.5006 - acc: 0.4631 - val_loss: 1.4286 - val_acc: 0.4945\n",
      "Epoch 13/20\n",
      " - 2s - loss: 1.4847 - acc: 0.4681 - val_loss: 1.4404 - val_acc: 0.4911\n",
      "Epoch 14/20\n",
      " - 2s - loss: 1.4754 - acc: 0.4727 - val_loss: 1.4328 - val_acc: 0.4943\n",
      "Epoch 15/20\n",
      " - 2s - loss: 1.4682 - acc: 0.4750 - val_loss: 1.4155 - val_acc: 0.4942\n",
      "Epoch 16/20\n",
      " - 2s - loss: 1.4550 - acc: 0.4757 - val_loss: 1.4249 - val_acc: 0.4916\n",
      "Epoch 17/20\n",
      " - 2s - loss: 1.4473 - acc: 0.4808 - val_loss: 1.4216 - val_acc: 0.4910\n",
      "Epoch 18/20\n",
      " - 2s - loss: 1.4324 - acc: 0.4836 - val_loss: 1.4329 - val_acc: 0.4849\n",
      "Epoch 19/20\n",
      " - 2s - loss: 1.4342 - acc: 0.4836 - val_loss: 1.4169 - val_acc: 0.4951\n",
      "Epoch 20/20\n",
      " - 2s - loss: 1.4271 - acc: 0.4876 - val_loss: 1.3929 - val_acc: 0.5011\n",
      "10000/10000 [==============================] - 1s 69us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment37:\tMomentum with learning rate @ 0.01\n",
      "=======================================================\n",
      "Time Used: 62.61198699999977\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.392945088005066\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.5011\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 5s - loss: 14.4777 - acc: 0.0997 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 2/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 3/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 4/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 5/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 6/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 7/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 8/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 9/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 10/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 11/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 12/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 13/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 14/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 15/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 16/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 17/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 18/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 19/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 20/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "10000/10000 [==============================] - 1s 71us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment38:\tRMSprop with learning rate @ 0.01\n",
      "=======================================================\n",
      "Time Used: 68.8817570000001\n",
      "-------------------------------------------------------\n",
      "Test loss: 14.506285720825195\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 6s - loss: 14.4643 - acc: 0.1002 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 2/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 3/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 4/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 5/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 6/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 7/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 8/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 9/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 10/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 11/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 12/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 13/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 14/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 15/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 16/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 17/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 18/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 19/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 20/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "10000/10000 [==============================] - 1s 70us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment39:\tAdam with learning rate @ 0.01\n",
      "=======================================================\n",
      "Time Used: 75.67561799999976\n",
      "-------------------------------------------------------\n",
      "Test loss: 14.506285620117188\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 5s - loss: 8.3441 - acc: 0.1493 - val_loss: 1.9889 - val_acc: 0.2924\n",
      "Epoch 2/20\n",
      " - 3s - loss: 1.9277 - acc: 0.2917 - val_loss: 1.7987 - val_acc: 0.3612\n",
      "Epoch 3/20\n",
      " - 3s - loss: 1.8536 - acc: 0.3233 - val_loss: 1.7645 - val_acc: 0.3680\n",
      "Epoch 4/20\n",
      " - 3s - loss: 1.8178 - acc: 0.3401 - val_loss: 1.7723 - val_acc: 0.3621\n",
      "Epoch 5/20\n",
      " - 3s - loss: 1.7873 - acc: 0.3509 - val_loss: 1.6944 - val_acc: 0.3993\n",
      "Epoch 6/20\n",
      " - 3s - loss: 1.7678 - acc: 0.3616 - val_loss: 1.6813 - val_acc: 0.4063\n",
      "Epoch 7/20\n",
      " - 3s - loss: 1.7424 - acc: 0.3699 - val_loss: 1.6559 - val_acc: 0.4181\n",
      "Epoch 8/20\n",
      " - 3s - loss: 1.7254 - acc: 0.3797 - val_loss: 1.6493 - val_acc: 0.4169\n",
      "Epoch 9/20\n",
      " - 3s - loss: 1.6968 - acc: 0.3904 - val_loss: 1.6284 - val_acc: 0.4269\n",
      "Epoch 10/20\n",
      " - 3s - loss: 1.6850 - acc: 0.3960 - val_loss: 1.6111 - val_acc: 0.4340\n",
      "Epoch 11/20\n",
      " - 3s - loss: 1.6702 - acc: 0.3999 - val_loss: 1.5839 - val_acc: 0.4434\n",
      "Epoch 12/20\n",
      " - 3s - loss: 1.6592 - acc: 0.4056 - val_loss: 1.6213 - val_acc: 0.4290\n",
      "Epoch 13/20\n",
      " - 3s - loss: 1.6485 - acc: 0.4081 - val_loss: 1.5738 - val_acc: 0.4514\n",
      "Epoch 14/20\n",
      " - 3s - loss: 1.6367 - acc: 0.4126 - val_loss: 1.5621 - val_acc: 0.4553\n",
      "Epoch 15/20\n",
      " - 3s - loss: 1.6265 - acc: 0.4170 - val_loss: 1.5507 - val_acc: 0.4549\n",
      "Epoch 16/20\n",
      " - 3s - loss: 1.6172 - acc: 0.4226 - val_loss: 1.5408 - val_acc: 0.4607\n",
      "Epoch 17/20\n",
      " - 3s - loss: 1.6063 - acc: 0.4225 - val_loss: 1.5443 - val_acc: 0.4634\n",
      "Epoch 18/20\n",
      " - 3s - loss: 1.6028 - acc: 0.4261 - val_loss: 1.5224 - val_acc: 0.4672\n",
      "Epoch 19/20\n",
      " - 3s - loss: 1.5947 - acc: 0.4309 - val_loss: 1.5160 - val_acc: 0.4672\n",
      "Epoch 20/20\n",
      " - 3s - loss: 1.5874 - acc: 0.4314 - val_loss: 1.5164 - val_acc: 0.4642\n",
      "10000/10000 [==============================] - 1s 69us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment40:\tAdagrad with learning rate @ 0.01\n",
      "=======================================================\n",
      "Time Used: 67.13117999999986\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.5163957759857178\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.4642\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 6s - loss: 2.1797 - acc: 0.1982 - val_loss: 2.0397 - val_acc: 0.2803\n",
      "Epoch 2/20\n",
      " - 3s - loss: 2.0413 - acc: 0.2621 - val_loss: 1.9564 - val_acc: 0.3157\n",
      "Epoch 3/20\n",
      " - 3s - loss: 1.9781 - acc: 0.2887 - val_loss: 1.9037 - val_acc: 0.3402\n",
      "Epoch 4/20\n",
      " - 3s - loss: 1.9293 - acc: 0.3111 - val_loss: 1.8643 - val_acc: 0.3485\n",
      "Epoch 5/20\n",
      " - 3s - loss: 1.8988 - acc: 0.3230 - val_loss: 1.8351 - val_acc: 0.3688\n",
      "Epoch 6/20\n",
      " - 3s - loss: 1.8726 - acc: 0.3346 - val_loss: 1.8079 - val_acc: 0.3722\n",
      "Epoch 7/20\n",
      " - 3s - loss: 1.8443 - acc: 0.3449 - val_loss: 1.7867 - val_acc: 0.3801\n",
      "Epoch 8/20\n",
      " - 3s - loss: 1.8276 - acc: 0.3516 - val_loss: 1.7663 - val_acc: 0.3870\n",
      "Epoch 9/20\n",
      " - 3s - loss: 1.8086 - acc: 0.3595 - val_loss: 1.7502 - val_acc: 0.3909\n",
      "Epoch 10/20\n",
      " - 3s - loss: 1.7909 - acc: 0.3657 - val_loss: 1.7356 - val_acc: 0.3944\n",
      "Epoch 11/20\n",
      " - 3s - loss: 1.7759 - acc: 0.3713 - val_loss: 1.7204 - val_acc: 0.4013\n",
      "Epoch 12/20\n",
      " - 3s - loss: 1.7657 - acc: 0.3777 - val_loss: 1.7074 - val_acc: 0.4066\n",
      "Epoch 13/20\n",
      " - 3s - loss: 1.7519 - acc: 0.3801 - val_loss: 1.6947 - val_acc: 0.4110\n",
      "Epoch 14/20\n",
      " - 3s - loss: 1.7415 - acc: 0.3830 - val_loss: 1.6842 - val_acc: 0.4121\n",
      "Epoch 15/20\n",
      " - 3s - loss: 1.7278 - acc: 0.3876 - val_loss: 1.6720 - val_acc: 0.4178\n",
      "Epoch 16/20\n",
      " - 3s - loss: 1.7184 - acc: 0.3934 - val_loss: 1.6627 - val_acc: 0.4182\n",
      "Epoch 17/20\n",
      " - 3s - loss: 1.7092 - acc: 0.3958 - val_loss: 1.6533 - val_acc: 0.4257\n",
      "Epoch 18/20\n",
      " - 3s - loss: 1.7005 - acc: 0.3980 - val_loss: 1.6465 - val_acc: 0.4253\n",
      "Epoch 19/20\n",
      " - 3s - loss: 1.6891 - acc: 0.4048 - val_loss: 1.6421 - val_acc: 0.4267\n",
      "Epoch 20/20\n",
      " - 3s - loss: 1.6830 - acc: 0.4066 - val_loss: 1.6293 - val_acc: 0.4314\n",
      "10000/10000 [==============================] - 1s 69us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment41:\tAdadelta with learning rate @ 0.01\n",
      "=======================================================\n",
      "Time Used: 79.80439799999976\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.6292917461395264\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.4314\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 5s - loss: 1.9907 - acc: 0.2757 - val_loss: 1.9310 - val_acc: 0.3044\n",
      "Epoch 2/20\n",
      " - 2s - loss: 1.8203 - acc: 0.3451 - val_loss: 1.7218 - val_acc: 0.3878\n",
      "Epoch 3/20\n",
      " - 2s - loss: 1.7478 - acc: 0.3759 - val_loss: 1.6770 - val_acc: 0.4075\n",
      "Epoch 4/20\n",
      " - 2s - loss: 1.7069 - acc: 0.3931 - val_loss: 1.6095 - val_acc: 0.4265\n",
      "Epoch 5/20\n",
      " - 2s - loss: 1.6687 - acc: 0.4085 - val_loss: 1.6112 - val_acc: 0.4211\n",
      "Epoch 6/20\n",
      " - 2s - loss: 1.6314 - acc: 0.4224 - val_loss: 1.6065 - val_acc: 0.4168\n",
      "Epoch 7/20\n",
      " - 2s - loss: 1.6046 - acc: 0.4297 - val_loss: 1.5265 - val_acc: 0.4554\n",
      "Epoch 8/20\n",
      " - 2s - loss: 1.5867 - acc: 0.4376 - val_loss: 1.5204 - val_acc: 0.4585\n",
      "Epoch 9/20\n",
      " - 2s - loss: 1.5637 - acc: 0.4452 - val_loss: 1.5612 - val_acc: 0.4392\n",
      "Epoch 10/20\n",
      " - 2s - loss: 1.5411 - acc: 0.4570 - val_loss: 1.5131 - val_acc: 0.4566\n",
      "Epoch 11/20\n",
      " - 2s - loss: 1.5286 - acc: 0.4588 - val_loss: 1.4635 - val_acc: 0.4799\n",
      "Epoch 12/20\n",
      " - 2s - loss: 1.5074 - acc: 0.4678 - val_loss: 1.4653 - val_acc: 0.4797\n",
      "Epoch 13/20\n",
      " - 2s - loss: 1.4924 - acc: 0.4710 - val_loss: 1.4736 - val_acc: 0.4735\n",
      "Epoch 14/20\n",
      " - 2s - loss: 1.4790 - acc: 0.4743 - val_loss: 1.4561 - val_acc: 0.4848\n",
      "Epoch 15/20\n",
      " - 2s - loss: 1.4654 - acc: 0.4812 - val_loss: 1.4268 - val_acc: 0.4972\n",
      "Epoch 16/20\n",
      " - 2s - loss: 1.4566 - acc: 0.4840 - val_loss: 1.4552 - val_acc: 0.4877\n",
      "Epoch 17/20\n",
      " - 2s - loss: 1.4397 - acc: 0.4890 - val_loss: 1.4898 - val_acc: 0.4785\n",
      "Epoch 18/20\n",
      " - 2s - loss: 1.4313 - acc: 0.4926 - val_loss: 1.4096 - val_acc: 0.4963\n",
      "Epoch 19/20\n",
      " - 2s - loss: 1.4197 - acc: 0.4957 - val_loss: 1.3926 - val_acc: 0.5069\n",
      "Epoch 20/20\n",
      " - 2s - loss: 1.4052 - acc: 0.5019 - val_loss: 1.3936 - val_acc: 0.5078\n",
      "10000/10000 [==============================] - 1s 72us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment42:\tSGD with learning rate @ 0.03\n",
      "=======================================================\n",
      "Time Used: 63.46529599999985\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.3936080083847047\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.5078\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 5s - loss: 1.9776 - acc: 0.2723 - val_loss: 1.8617 - val_acc: 0.3026\n",
      "Epoch 2/20\n",
      " - 2s - loss: 1.8833 - acc: 0.3099 - val_loss: 1.7799 - val_acc: 0.3660\n",
      "Epoch 3/20\n",
      " - 2s - loss: 1.8186 - acc: 0.3381 - val_loss: 1.7631 - val_acc: 0.3697\n",
      "Epoch 4/20\n",
      " - 2s - loss: 1.8047 - acc: 0.3442 - val_loss: 1.7324 - val_acc: 0.3732\n",
      "Epoch 5/20\n",
      " - 2s - loss: 1.7730 - acc: 0.3550 - val_loss: 1.6744 - val_acc: 0.3966\n",
      "Epoch 6/20\n",
      " - 2s - loss: 1.7549 - acc: 0.3631 - val_loss: 1.7132 - val_acc: 0.3970\n",
      "Epoch 7/20\n",
      " - 2s - loss: 1.7447 - acc: 0.3670 - val_loss: 1.6899 - val_acc: 0.3902\n",
      "Epoch 8/20\n",
      " - 2s - loss: 1.7248 - acc: 0.3750 - val_loss: 1.6533 - val_acc: 0.4102\n",
      "Epoch 9/20\n",
      " - 2s - loss: 1.7083 - acc: 0.3815 - val_loss: 1.6418 - val_acc: 0.4234\n",
      "Epoch 10/20\n",
      " - 2s - loss: 1.6895 - acc: 0.3893 - val_loss: 1.6229 - val_acc: 0.4188\n",
      "Epoch 11/20\n",
      " - 2s - loss: 1.6870 - acc: 0.3903 - val_loss: 1.6273 - val_acc: 0.4223\n",
      "Epoch 12/20\n",
      " - 2s - loss: 1.6828 - acc: 0.3920 - val_loss: 1.6378 - val_acc: 0.4098\n",
      "Epoch 13/20\n",
      " - 2s - loss: 1.6833 - acc: 0.3902 - val_loss: 1.6235 - val_acc: 0.4137\n",
      "Epoch 14/20\n",
      " - 2s - loss: 1.6776 - acc: 0.3969 - val_loss: 1.6198 - val_acc: 0.4285\n",
      "Epoch 15/20\n",
      " - 2s - loss: 1.6615 - acc: 0.4016 - val_loss: 1.6359 - val_acc: 0.4042\n",
      "Epoch 16/20\n",
      " - 2s - loss: 1.6697 - acc: 0.3958 - val_loss: 1.5967 - val_acc: 0.4286\n",
      "Epoch 17/20\n",
      " - 2s - loss: 1.6502 - acc: 0.4046 - val_loss: 1.5884 - val_acc: 0.4312\n",
      "Epoch 18/20\n",
      " - 2s - loss: 1.6395 - acc: 0.4087 - val_loss: 1.5995 - val_acc: 0.4240\n",
      "Epoch 19/20\n",
      " - 2s - loss: 1.6334 - acc: 0.4110 - val_loss: 1.5786 - val_acc: 0.4347\n",
      "Epoch 20/20\n",
      " - 2s - loss: 1.6293 - acc: 0.4149 - val_loss: 1.5761 - val_acc: 0.4331\n",
      "10000/10000 [==============================] - 1s 71us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment43:\tMomentum with learning rate @ 0.03\n",
      "=======================================================\n",
      "Time Used: 63.48842800000011\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.5760615682601928\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.4331\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 6s - loss: 14.4763 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 2/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 3/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 4/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 5/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 6/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 7/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 8/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 9/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 10/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 11/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 12/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 13/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 14/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 15/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 16/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 17/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 18/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 19/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 20/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "10000/10000 [==============================] - 1s 70us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment44:\tRMSprop with learning rate @ 0.03\n",
      "=======================================================\n",
      "Time Used: 69.94691600000033\n",
      "-------------------------------------------------------\n",
      "Test loss: 14.506285720825195\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 6s - loss: 14.4754 - acc: 0.0999 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 2/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 3/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 4/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 5/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 6/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 7/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 8/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 9/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 10/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 11/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 12/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 13/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 14/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 15/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 16/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 17/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 18/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 19/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 20/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "10000/10000 [==============================] - 1s 71us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment45:\tAdam with learning rate @ 0.03\n",
      "=======================================================\n",
      "Time Used: 75.974917\n",
      "-------------------------------------------------------\n",
      "Test loss: 14.506285720825195\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 6s - loss: 14.4773 - acc: 0.0999 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 2/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 3/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 4/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 5/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 6/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 7/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 8/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 9/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 10/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 11/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 12/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 13/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 14/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 15/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 16/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 17/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 18/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 19/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 20/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "10000/10000 [==============================] - 1s 74us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment46:\tAdagrad with learning rate @ 0.03\n",
      "=======================================================\n",
      "Time Used: 68.08816100000013\n",
      "-------------------------------------------------------\n",
      "Test loss: 14.506285656738282\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 6s - loss: 2.0860 - acc: 0.2349 - val_loss: 1.9085 - val_acc: 0.3367\n",
      "Epoch 2/20\n",
      " - 3s - loss: 1.9151 - acc: 0.3135 - val_loss: 1.8190 - val_acc: 0.3668\n",
      "Epoch 3/20\n",
      " - 3s - loss: 1.8443 - acc: 0.3431 - val_loss: 1.7685 - val_acc: 0.3823\n",
      "Epoch 4/20\n",
      " - 3s - loss: 1.7961 - acc: 0.3629 - val_loss: 1.7250 - val_acc: 0.3976\n",
      "Epoch 5/20\n",
      " - 3s - loss: 1.7614 - acc: 0.3748 - val_loss: 1.6918 - val_acc: 0.4119\n",
      "Epoch 6/20\n",
      " - 3s - loss: 1.7296 - acc: 0.3873 - val_loss: 1.6602 - val_acc: 0.4180\n",
      "Epoch 7/20\n",
      " - 3s - loss: 1.7017 - acc: 0.3991 - val_loss: 1.6391 - val_acc: 0.4263\n",
      "Epoch 8/20\n",
      " - 3s - loss: 1.6823 - acc: 0.4063 - val_loss: 1.6195 - val_acc: 0.4334\n",
      "Epoch 9/20\n",
      " - 3s - loss: 1.6631 - acc: 0.4089 - val_loss: 1.6009 - val_acc: 0.4391\n",
      "Epoch 10/20\n",
      " - 3s - loss: 1.6457 - acc: 0.4182 - val_loss: 1.5917 - val_acc: 0.4427\n",
      "Epoch 11/20\n",
      " - 3s - loss: 1.6303 - acc: 0.4220 - val_loss: 1.5769 - val_acc: 0.4474\n",
      "Epoch 12/20\n",
      " - 3s - loss: 1.6148 - acc: 0.4309 - val_loss: 1.5600 - val_acc: 0.4543\n",
      "Epoch 13/20\n",
      " - 3s - loss: 1.6014 - acc: 0.4343 - val_loss: 1.5463 - val_acc: 0.4584\n",
      "Epoch 14/20\n",
      " - 3s - loss: 1.5904 - acc: 0.4392 - val_loss: 1.5384 - val_acc: 0.4627\n",
      "Epoch 15/20\n",
      " - 3s - loss: 1.5793 - acc: 0.4419 - val_loss: 1.5423 - val_acc: 0.4526\n",
      "Epoch 16/20\n",
      " - 3s - loss: 1.5644 - acc: 0.4458 - val_loss: 1.5185 - val_acc: 0.4626\n",
      "Epoch 17/20\n",
      " - 3s - loss: 1.5540 - acc: 0.4518 - val_loss: 1.5191 - val_acc: 0.4665\n",
      "Epoch 18/20\n",
      " - 3s - loss: 1.5471 - acc: 0.4538 - val_loss: 1.5078 - val_acc: 0.4675\n",
      "Epoch 19/20\n",
      " - 3s - loss: 1.5309 - acc: 0.4612 - val_loss: 1.5003 - val_acc: 0.4680\n",
      "Epoch 20/20\n",
      " - 3s - loss: 1.5272 - acc: 0.4624 - val_loss: 1.4802 - val_acc: 0.4754\n",
      "10000/10000 [==============================] - 1s 73us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment47:\tAdadelta with learning rate @ 0.03\n",
      "=======================================================\n",
      "Time Used: 80.69381500000009\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.4802288578033447\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.4754\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 6s - loss: 2.0231 - acc: 0.2616 - val_loss: 1.8228 - val_acc: 0.3376\n",
      "Epoch 2/20\n",
      " - 2s - loss: 1.8320 - acc: 0.3401 - val_loss: 1.7362 - val_acc: 0.3706\n",
      "Epoch 3/20\n",
      " - 2s - loss: 1.7520 - acc: 0.3727 - val_loss: 1.7277 - val_acc: 0.3911\n",
      "Epoch 4/20\n",
      " - 2s - loss: 1.7011 - acc: 0.3908 - val_loss: 1.6255 - val_acc: 0.4226\n",
      "Epoch 5/20\n",
      " - 2s - loss: 1.6588 - acc: 0.4063 - val_loss: 1.6326 - val_acc: 0.4208\n",
      "Epoch 6/20\n",
      " - 2s - loss: 1.6284 - acc: 0.4194 - val_loss: 1.6111 - val_acc: 0.4208\n",
      "Epoch 7/20\n",
      " - 2s - loss: 1.5995 - acc: 0.4288 - val_loss: 1.5660 - val_acc: 0.4508\n",
      "Epoch 8/20\n",
      " - 2s - loss: 1.5774 - acc: 0.4358 - val_loss: 1.5494 - val_acc: 0.4451\n",
      "Epoch 9/20\n",
      " - 2s - loss: 1.5507 - acc: 0.4460 - val_loss: 1.4948 - val_acc: 0.4681\n",
      "Epoch 10/20\n",
      " - 2s - loss: 1.5340 - acc: 0.4509 - val_loss: 1.4879 - val_acc: 0.4665\n",
      "Epoch 11/20\n",
      " - 2s - loss: 1.5209 - acc: 0.4556 - val_loss: 1.4878 - val_acc: 0.4651\n",
      "Epoch 12/20\n",
      " - 2s - loss: 1.4988 - acc: 0.4629 - val_loss: 1.4922 - val_acc: 0.4650\n",
      "Epoch 13/20\n",
      " - 2s - loss: 1.4807 - acc: 0.4716 - val_loss: 1.5136 - val_acc: 0.4544\n",
      "Epoch 14/20\n",
      " - 2s - loss: 1.4662 - acc: 0.4750 - val_loss: 1.4668 - val_acc: 0.4679\n",
      "Epoch 15/20\n",
      " - 2s - loss: 1.4572 - acc: 0.4803 - val_loss: 1.4776 - val_acc: 0.4667\n",
      "Epoch 16/20\n",
      " - 2s - loss: 1.4424 - acc: 0.4823 - val_loss: 1.4569 - val_acc: 0.4708\n",
      "Epoch 17/20\n",
      " - 2s - loss: 1.4312 - acc: 0.4878 - val_loss: 1.4339 - val_acc: 0.4932\n",
      "Epoch 18/20\n",
      " - 2s - loss: 1.4172 - acc: 0.4945 - val_loss: 1.4322 - val_acc: 0.4847\n",
      "Epoch 19/20\n",
      " - 2s - loss: 1.4011 - acc: 0.4982 - val_loss: 1.4785 - val_acc: 0.4793\n",
      "Epoch 20/20\n",
      " - 2s - loss: 1.3991 - acc: 0.5019 - val_loss: 1.3801 - val_acc: 0.5086\n",
      "10000/10000 [==============================] - 1s 75us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment48:\tSGD with learning rate @ 0.1\n",
      "=======================================================\n",
      "Time Used: 65.1138890000002\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.3800915664672853\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.5086\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 6s - loss: 2.1785 - acc: 0.1624 - val_loss: 2.1193 - val_acc: 0.1850\n",
      "Epoch 2/20\n",
      " - 2s - loss: 2.1455 - acc: 0.1751 - val_loss: 2.1493 - val_acc: 0.1791\n",
      "Epoch 3/20\n",
      " - 2s - loss: 2.1719 - acc: 0.1611 - val_loss: 2.1025 - val_acc: 0.1966\n",
      "Epoch 4/20\n",
      " - 2s - loss: 2.1943 - acc: 0.1567 - val_loss: 2.2055 - val_acc: 0.1591\n",
      "Epoch 5/20\n",
      " - 2s - loss: 2.2133 - acc: 0.1476 - val_loss: 2.1345 - val_acc: 0.1752\n",
      "Epoch 6/20\n",
      " - 2s - loss: 2.2029 - acc: 0.1508 - val_loss: 2.2342 - val_acc: 0.1402\n",
      "Epoch 7/20\n",
      " - 2s - loss: 2.2123 - acc: 0.1441 - val_loss: 2.1355 - val_acc: 0.1647\n",
      "Epoch 8/20\n",
      " - 2s - loss: 2.1788 - acc: 0.1558 - val_loss: 2.1309 - val_acc: 0.1693\n",
      "Epoch 9/20\n",
      " - 2s - loss: 2.1840 - acc: 0.1548 - val_loss: 2.1485 - val_acc: 0.1687\n",
      "Epoch 10/20\n",
      " - 2s - loss: 2.1718 - acc: 0.1597 - val_loss: 2.1687 - val_acc: 0.1631\n",
      "Epoch 11/20\n",
      " - 2s - loss: 2.1798 - acc: 0.1552 - val_loss: 2.1325 - val_acc: 0.1655\n",
      "Epoch 12/20\n",
      " - 2s - loss: 2.1731 - acc: 0.1574 - val_loss: 2.1702 - val_acc: 0.1560\n",
      "Epoch 13/20\n",
      " - 2s - loss: 2.1726 - acc: 0.1585 - val_loss: 2.1425 - val_acc: 0.1695\n",
      "Epoch 14/20\n",
      " - 2s - loss: 2.1832 - acc: 0.1599 - val_loss: 2.1504 - val_acc: 0.1708\n",
      "Epoch 15/20\n",
      " - 2s - loss: 2.1624 - acc: 0.1623 - val_loss: 2.1222 - val_acc: 0.1686\n",
      "Epoch 16/20\n",
      " - 2s - loss: 2.1783 - acc: 0.1580 - val_loss: 2.2181 - val_acc: 0.1495\n",
      "Epoch 17/20\n",
      " - 2s - loss: 2.2179 - acc: 0.1479 - val_loss: 2.2439 - val_acc: 0.1464\n",
      "Epoch 18/20\n",
      " - 2s - loss: 2.1976 - acc: 0.1502 - val_loss: 2.1342 - val_acc: 0.1667\n",
      "Epoch 19/20\n",
      " - 2s - loss: 2.1972 - acc: 0.1502 - val_loss: 2.1449 - val_acc: 0.1678\n",
      "Epoch 20/20\n",
      " - 2s - loss: 2.1791 - acc: 0.1567 - val_loss: 2.2580 - val_acc: 0.1270\n",
      "10000/10000 [==============================] - 1s 73us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment49:\tMomentum with learning rate @ 0.1\n",
      "=======================================================\n",
      "Time Used: 65.06299800000033\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.2579662437438963\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.127\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 6s - loss: 14.5102 - acc: 0.0978 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 2/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 3/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 4/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 5/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 6/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 7/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 8/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 9/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 10/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 11/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 12/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 13/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 14/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 15/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 16/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 17/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 18/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 19/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 20/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "10000/10000 [==============================] - 1s 75us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment50:\tRMSprop with learning rate @ 0.1\n",
      "=======================================================\n",
      "Time Used: 71.31673199999977\n",
      "-------------------------------------------------------\n",
      "Test loss: 14.506285668945312\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 7s - loss: 14.4787 - acc: 0.0997 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 2/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 3/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 4/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 5/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 6/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 7/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 8/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 9/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 10/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 11/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 12/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 13/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 14/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 15/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 16/20\n",
      " - 3s - loss: 14.5060 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 17/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 18/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 19/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 20/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "10000/10000 [==============================] - 1s 75us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment51:\tAdam with learning rate @ 0.1\n",
      "=======================================================\n",
      "Time Used: 77.74832000000015\n",
      "-------------------------------------------------------\n",
      "Test loss: 14.506285708618163\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 6s - loss: 14.4645 - acc: 0.1006 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 2/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 3/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 4/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 5/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 6/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 7/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 8/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 9/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 10/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 11/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 12/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 13/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 14/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 15/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 16/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 17/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 18/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 19/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 20/20\n",
      " - 3s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "10000/10000 [==============================] - 1s 74us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment52:\tAdagrad with learning rate @ 0.1\n",
      "=======================================================\n",
      "Time Used: 69.3002120000001\n",
      "-------------------------------------------------------\n",
      "Test loss: 14.506285656738282\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 7s - loss: 1.9929 - acc: 0.2744 - val_loss: 1.8259 - val_acc: 0.3522\n",
      "Epoch 2/20\n",
      " - 3s - loss: 1.8138 - acc: 0.3539 - val_loss: 1.7216 - val_acc: 0.3868\n",
      "Epoch 3/20\n",
      " - 3s - loss: 1.7438 - acc: 0.3796 - val_loss: 1.6533 - val_acc: 0.4136\n",
      "Epoch 4/20\n",
      " - 3s - loss: 1.6954 - acc: 0.3981 - val_loss: 1.6218 - val_acc: 0.4240\n",
      "Epoch 5/20\n",
      " - 3s - loss: 1.6554 - acc: 0.4124 - val_loss: 1.5744 - val_acc: 0.4431\n",
      "Epoch 6/20\n",
      " - 3s - loss: 1.6257 - acc: 0.4245 - val_loss: 1.5925 - val_acc: 0.4379\n",
      "Epoch 7/20\n",
      " - 3s - loss: 1.5973 - acc: 0.4340 - val_loss: 1.6481 - val_acc: 0.4108\n",
      "Epoch 8/20\n",
      " - 3s - loss: 1.5754 - acc: 0.4418 - val_loss: 1.5153 - val_acc: 0.4630\n",
      "Epoch 9/20\n",
      " - 3s - loss: 1.5506 - acc: 0.4512 - val_loss: 1.5286 - val_acc: 0.4482\n",
      "Epoch 10/20\n",
      " - 3s - loss: 1.5347 - acc: 0.4571 - val_loss: 1.5355 - val_acc: 0.4550\n",
      "Epoch 11/20\n",
      " - 3s - loss: 1.5165 - acc: 0.4644 - val_loss: 1.4911 - val_acc: 0.4658\n",
      "Epoch 12/20\n",
      " - 3s - loss: 1.4966 - acc: 0.4711 - val_loss: 1.5056 - val_acc: 0.4656\n",
      "Epoch 13/20\n",
      " - 3s - loss: 1.4814 - acc: 0.4767 - val_loss: 1.4528 - val_acc: 0.4818\n",
      "Epoch 14/20\n",
      " - 3s - loss: 1.4695 - acc: 0.4786 - val_loss: 1.4380 - val_acc: 0.4858\n",
      "Epoch 15/20\n",
      " - 3s - loss: 1.4564 - acc: 0.4857 - val_loss: 1.4286 - val_acc: 0.4940\n",
      "Epoch 16/20\n",
      " - 3s - loss: 1.4458 - acc: 0.4898 - val_loss: 1.4153 - val_acc: 0.4958\n",
      "Epoch 17/20\n",
      " - 3s - loss: 1.4296 - acc: 0.4968 - val_loss: 1.4248 - val_acc: 0.4905\n",
      "Epoch 18/20\n",
      " - 3s - loss: 1.4197 - acc: 0.4990 - val_loss: 1.4435 - val_acc: 0.4853\n",
      "Epoch 19/20\n",
      " - 3s - loss: 1.4105 - acc: 0.5017 - val_loss: 1.4218 - val_acc: 0.4878\n",
      "Epoch 20/20\n",
      " - 3s - loss: 1.3958 - acc: 0.5073 - val_loss: 1.3921 - val_acc: 0.4979\n",
      "10000/10000 [==============================] - 1s 73us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment53:\tAdadelta with learning rate @ 0.1\n",
      "=======================================================\n",
      "Time Used: 81.65389600000026\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.392092668914795\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.4979\n",
      "*******************************************************\n"
     ]
    }
   ],
   "source": [
    "record = experiment_optimizer(list_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PSDOmAw5zp6U"
   },
   "source": [
    "### 4. SUMMRIZE EXPERIMENTS RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DnWnjIN_0UuL"
   },
   "source": [
    "#### 1. Best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Yg_QkZk8hIny",
    "outputId": "c9b41eba-aab7-4afe-d21f-f7343dd54b34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Best Result is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5105, 'Adam with learning rate @ 0.0003')"
      ]
     },
     "execution_count": 109,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_acc, experiment, history = getBest(record)\n",
    "print('The Best Result is:')\n",
    "max_acc, experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "id": "S0JuxvIGiycn",
    "outputId": "f07d9c36-994d-4ba7-8472-c2a87a4a33b4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotAcc(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "id": "PA60iLgxkQLs",
    "outputId": "0f088a4e-bac3-4a59-c606-e3694aea2a83"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotLoss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s9ncqNmU0joA"
   },
   "source": [
    "#### 2. Result summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q4hexsZP4l5h"
   },
   "outputs": [],
   "source": [
    "df_result = get_allResult(record)\n",
    "\n",
    "# add control parameters\n",
    "\n",
    "df_result['activation'] = 'relu'\n",
    "df_result['num_layers'] = 3\n",
    "df_result['num_neurons'] = 256\n",
    "df_result['batch_size'] = 128\n",
    "df_result['dropout_rate'] = 0.2\n",
    "df_result['epochs'] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1723
    },
    "colab_type": "code",
    "id": "4e_34RtuGLy0",
    "outputId": "bff4f9f3-7380-4c0c-b28b-d39b0dfa3d7f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>optimizer</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>activation</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>num_neurons</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Adam with learning rate @ 0.0003</td>\n",
       "      <td>1.389348</td>\n",
       "      <td>0.5105</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>SGD with learning rate @ 0.1</td>\n",
       "      <td>1.380092</td>\n",
       "      <td>0.5086</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>SGD with learning rate @ 0.03</td>\n",
       "      <td>1.393608</td>\n",
       "      <td>0.5078</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Momentum with learning rate @ 0.003</td>\n",
       "      <td>1.389523</td>\n",
       "      <td>0.5067</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>RMSprop with learning rate @ 0.0003</td>\n",
       "      <td>1.400199</td>\n",
       "      <td>0.5050</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Adam with learning rate @ 0.0001</td>\n",
       "      <td>1.388294</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Momentum with learning rate @ 0.01</td>\n",
       "      <td>1.392945</td>\n",
       "      <td>0.5011</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Adadelta with learning rate @ 0.1</td>\n",
       "      <td>1.392093</td>\n",
       "      <td>0.4979</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RMSprop with learning rate @ 0.0001</td>\n",
       "      <td>1.426306</td>\n",
       "      <td>0.4947</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Momentum with learning rate @ 0.001</td>\n",
       "      <td>1.465026</td>\n",
       "      <td>0.4876</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Adam with learning rate @ 3e-05</td>\n",
       "      <td>1.457137</td>\n",
       "      <td>0.4872</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RMSprop with learning rate @ 3e-05</td>\n",
       "      <td>1.483095</td>\n",
       "      <td>0.4784</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Adagrad with learning rate @ 0.003</td>\n",
       "      <td>1.492923</td>\n",
       "      <td>0.4758</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Adadelta with learning rate @ 0.03</td>\n",
       "      <td>1.480229</td>\n",
       "      <td>0.4754</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>SGD with learning rate @ 0.01</td>\n",
       "      <td>1.474845</td>\n",
       "      <td>0.4752</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Adagrad with learning rate @ 0.01</td>\n",
       "      <td>1.516396</td>\n",
       "      <td>0.4642</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>RMSprop with learning rate @ 0.001</td>\n",
       "      <td>1.518208</td>\n",
       "      <td>0.4630</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Adagrad with learning rate @ 0.001</td>\n",
       "      <td>1.528087</td>\n",
       "      <td>0.4625</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>1.537964</td>\n",
       "      <td>0.4605</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adam with learning rate @ 1e-05</td>\n",
       "      <td>1.624268</td>\n",
       "      <td>0.4352</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Momentum with learning rate @ 0.03</td>\n",
       "      <td>1.576062</td>\n",
       "      <td>0.4331</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Adadelta with learning rate @ 0.01</td>\n",
       "      <td>1.629292</td>\n",
       "      <td>0.4314</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>SGD with learning rate @ 0.003</td>\n",
       "      <td>1.621630</td>\n",
       "      <td>0.4314</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Momentum with learning rate @ 0.0003</td>\n",
       "      <td>1.633818</td>\n",
       "      <td>0.4297</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RMSprop with learning rate @ 1e-05</td>\n",
       "      <td>1.631296</td>\n",
       "      <td>0.4251</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Adagrad with learning rate @ 0.0003</td>\n",
       "      <td>1.678336</td>\n",
       "      <td>0.4131</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SGD with learning rate @ 0.001</td>\n",
       "      <td>1.792457</td>\n",
       "      <td>0.3782</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Adam with learning rate @ 0.003</td>\n",
       "      <td>1.753640</td>\n",
       "      <td>0.3756</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>RMSprop with learning rate @ 0.003</td>\n",
       "      <td>1.743436</td>\n",
       "      <td>0.3736</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Momentum with learning rate @ 0.0001</td>\n",
       "      <td>1.805000</td>\n",
       "      <td>0.3713</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Adadelta with learning rate @ 0.003</td>\n",
       "      <td>1.809514</td>\n",
       "      <td>0.3655</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Adagrad with learning rate @ 0.0001</td>\n",
       "      <td>1.836877</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Momentum with learning rate @ 3e-05</td>\n",
       "      <td>1.951523</td>\n",
       "      <td>0.3251</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Adadelta with learning rate @ 0.001</td>\n",
       "      <td>1.963405</td>\n",
       "      <td>0.3247</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SGD with learning rate @ 0.0003</td>\n",
       "      <td>1.969986</td>\n",
       "      <td>0.3181</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Adagrad with learning rate @ 3e-05</td>\n",
       "      <td>1.995356</td>\n",
       "      <td>0.3173</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SGD with learning rate @ 0.0001</td>\n",
       "      <td>2.123397</td>\n",
       "      <td>0.2673</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Momentum with learning rate @ 1e-05</td>\n",
       "      <td>2.112185</td>\n",
       "      <td>0.2641</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Adadelta with learning rate @ 0.0003</td>\n",
       "      <td>2.117975</td>\n",
       "      <td>0.2521</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Adagrad with learning rate @ 1e-05</td>\n",
       "      <td>2.154632</td>\n",
       "      <td>0.2403</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Adadelta with learning rate @ 0.0001</td>\n",
       "      <td>2.217645</td>\n",
       "      <td>0.2059</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SGD with learning rate @ 3e-05</td>\n",
       "      <td>2.220701</td>\n",
       "      <td>0.2006</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGD with learning rate @ 1e-05</td>\n",
       "      <td>2.284520</td>\n",
       "      <td>0.1375</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Momentum with learning rate @ 0.1</td>\n",
       "      <td>2.257966</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Adadelta with learning rate @ 3e-05</td>\n",
       "      <td>2.293115</td>\n",
       "      <td>0.1149</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>RMSprop with learning rate @ 0.03</td>\n",
       "      <td>14.506286</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Adam with learning rate @ 0.03</td>\n",
       "      <td>14.506286</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Adagrad with learning rate @ 0.03</td>\n",
       "      <td>14.506286</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Adam with learning rate @ 0.01</td>\n",
       "      <td>14.506286</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>RMSprop with learning rate @ 0.01</td>\n",
       "      <td>14.506286</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>RMSprop with learning rate @ 0.1</td>\n",
       "      <td>14.506286</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Adam with learning rate @ 0.1</td>\n",
       "      <td>14.506286</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Adagrad with learning rate @ 0.1</td>\n",
       "      <td>14.506286</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Adadelta with learning rate @ 1e-05</td>\n",
       "      <td>2.325728</td>\n",
       "      <td>0.0955</td>\n",
       "      <td>relu</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               optimizer       Loss  Accuracy activation  \\\n",
       "21      Adam with learning rate @ 0.0003   1.389348    0.5105       relu   \n",
       "48          SGD with learning rate @ 0.1   1.380092    0.5086       relu   \n",
       "42         SGD with learning rate @ 0.03   1.393608    0.5078       relu   \n",
       "31   Momentum with learning rate @ 0.003   1.389523    0.5067       relu   \n",
       "20   RMSprop with learning rate @ 0.0003   1.400199    0.5050       relu   \n",
       "15      Adam with learning rate @ 0.0001   1.388294    0.5040       relu   \n",
       "37    Momentum with learning rate @ 0.01   1.392945    0.5011       relu   \n",
       "53     Adadelta with learning rate @ 0.1   1.392093    0.4979       relu   \n",
       "14   RMSprop with learning rate @ 0.0001   1.426306    0.4947       relu   \n",
       "25   Momentum with learning rate @ 0.001   1.465026    0.4876       relu   \n",
       "9        Adam with learning rate @ 3e-05   1.457137    0.4872       relu   \n",
       "8     RMSprop with learning rate @ 3e-05   1.483095    0.4784       relu   \n",
       "34    Adagrad with learning rate @ 0.003   1.492923    0.4758       relu   \n",
       "47    Adadelta with learning rate @ 0.03   1.480229    0.4754       relu   \n",
       "36         SGD with learning rate @ 0.01   1.474845    0.4752       relu   \n",
       "40     Adagrad with learning rate @ 0.01   1.516396    0.4642       relu   \n",
       "26    RMSprop with learning rate @ 0.001   1.518208    0.4630       relu   \n",
       "28    Adagrad with learning rate @ 0.001   1.528087    0.4625       relu   \n",
       "27       Adam with learning rate @ 0.001   1.537964    0.4605       relu   \n",
       "3        Adam with learning rate @ 1e-05   1.624268    0.4352       relu   \n",
       "43    Momentum with learning rate @ 0.03   1.576062    0.4331       relu   \n",
       "41    Adadelta with learning rate @ 0.01   1.629292    0.4314       relu   \n",
       "30        SGD with learning rate @ 0.003   1.621630    0.4314       relu   \n",
       "19  Momentum with learning rate @ 0.0003   1.633818    0.4297       relu   \n",
       "2     RMSprop with learning rate @ 1e-05   1.631296    0.4251       relu   \n",
       "22   Adagrad with learning rate @ 0.0003   1.678336    0.4131       relu   \n",
       "24        SGD with learning rate @ 0.001   1.792457    0.3782       relu   \n",
       "33       Adam with learning rate @ 0.003   1.753640    0.3756       relu   \n",
       "32    RMSprop with learning rate @ 0.003   1.743436    0.3736       relu   \n",
       "13  Momentum with learning rate @ 0.0001   1.805000    0.3713       relu   \n",
       "35   Adadelta with learning rate @ 0.003   1.809514    0.3655       relu   \n",
       "16   Adagrad with learning rate @ 0.0001   1.836877    0.3613       relu   \n",
       "7    Momentum with learning rate @ 3e-05   1.951523    0.3251       relu   \n",
       "29   Adadelta with learning rate @ 0.001   1.963405    0.3247       relu   \n",
       "18       SGD with learning rate @ 0.0003   1.969986    0.3181       relu   \n",
       "10    Adagrad with learning rate @ 3e-05   1.995356    0.3173       relu   \n",
       "12       SGD with learning rate @ 0.0001   2.123397    0.2673       relu   \n",
       "1    Momentum with learning rate @ 1e-05   2.112185    0.2641       relu   \n",
       "23  Adadelta with learning rate @ 0.0003   2.117975    0.2521       relu   \n",
       "4     Adagrad with learning rate @ 1e-05   2.154632    0.2403       relu   \n",
       "17  Adadelta with learning rate @ 0.0001   2.217645    0.2059       relu   \n",
       "6         SGD with learning rate @ 3e-05   2.220701    0.2006       relu   \n",
       "0         SGD with learning rate @ 1e-05   2.284520    0.1375       relu   \n",
       "49     Momentum with learning rate @ 0.1   2.257966    0.1270       relu   \n",
       "11   Adadelta with learning rate @ 3e-05   2.293115    0.1149       relu   \n",
       "44     RMSprop with learning rate @ 0.03  14.506286    0.1000       relu   \n",
       "45        Adam with learning rate @ 0.03  14.506286    0.1000       relu   \n",
       "46     Adagrad with learning rate @ 0.03  14.506286    0.1000       relu   \n",
       "39        Adam with learning rate @ 0.01  14.506286    0.1000       relu   \n",
       "38     RMSprop with learning rate @ 0.01  14.506286    0.1000       relu   \n",
       "50      RMSprop with learning rate @ 0.1  14.506286    0.1000       relu   \n",
       "51         Adam with learning rate @ 0.1  14.506286    0.1000       relu   \n",
       "52      Adagrad with learning rate @ 0.1  14.506286    0.1000       relu   \n",
       "5    Adadelta with learning rate @ 1e-05   2.325728    0.0955       relu   \n",
       "\n",
       "    num_layers  num_neurons  batch_size  dropout_rate  epochs  \n",
       "21           3          256         128           0.2      20  \n",
       "48           3          256         128           0.2      20  \n",
       "42           3          256         128           0.2      20  \n",
       "31           3          256         128           0.2      20  \n",
       "20           3          256         128           0.2      20  \n",
       "15           3          256         128           0.2      20  \n",
       "37           3          256         128           0.2      20  \n",
       "53           3          256         128           0.2      20  \n",
       "14           3          256         128           0.2      20  \n",
       "25           3          256         128           0.2      20  \n",
       "9            3          256         128           0.2      20  \n",
       "8            3          256         128           0.2      20  \n",
       "34           3          256         128           0.2      20  \n",
       "47           3          256         128           0.2      20  \n",
       "36           3          256         128           0.2      20  \n",
       "40           3          256         128           0.2      20  \n",
       "26           3          256         128           0.2      20  \n",
       "28           3          256         128           0.2      20  \n",
       "27           3          256         128           0.2      20  \n",
       "3            3          256         128           0.2      20  \n",
       "43           3          256         128           0.2      20  \n",
       "41           3          256         128           0.2      20  \n",
       "30           3          256         128           0.2      20  \n",
       "19           3          256         128           0.2      20  \n",
       "2            3          256         128           0.2      20  \n",
       "22           3          256         128           0.2      20  \n",
       "24           3          256         128           0.2      20  \n",
       "33           3          256         128           0.2      20  \n",
       "32           3          256         128           0.2      20  \n",
       "13           3          256         128           0.2      20  \n",
       "35           3          256         128           0.2      20  \n",
       "16           3          256         128           0.2      20  \n",
       "7            3          256         128           0.2      20  \n",
       "29           3          256         128           0.2      20  \n",
       "18           3          256         128           0.2      20  \n",
       "10           3          256         128           0.2      20  \n",
       "12           3          256         128           0.2      20  \n",
       "1            3          256         128           0.2      20  \n",
       "23           3          256         128           0.2      20  \n",
       "4            3          256         128           0.2      20  \n",
       "17           3          256         128           0.2      20  \n",
       "6            3          256         128           0.2      20  \n",
       "0            3          256         128           0.2      20  \n",
       "49           3          256         128           0.2      20  \n",
       "11           3          256         128           0.2      20  \n",
       "44           3          256         128           0.2      20  \n",
       "45           3          256         128           0.2      20  \n",
       "46           3          256         128           0.2      20  \n",
       "39           3          256         128           0.2      20  \n",
       "38           3          256         128           0.2      20  \n",
       "50           3          256         128           0.2      20  \n",
       "51           3          256         128           0.2      20  \n",
       "52           3          256         128           0.2      20  \n",
       "5            3          256         128           0.2      20  "
      ]
     },
     "execution_count": 151,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.rename(columns = {\"Experiment\": \"optimizer\"}, inplace = True)\n",
    "df_result.sort_values(by = 'Accuracy', axis = 0, ascending = False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "EXP_Optimizer_LR.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
