{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. IMPORT LIBRARIES AND CIFAR 10, PREPARE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "import keras\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "# dataset\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# modeling tools\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check and Prepare Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape: (50000, 32, 32, 3)\n",
      "test data shape: (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "# check data dimension\n",
    "print('training data shape: {}'.format(x_train.shape))\n",
    "print('test data shape: {}'.format(x_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training labels are: [6, 9, 4, 1, 2, 7, 8, 3, 5, 0]\n",
      "# labels: 10\n"
     ]
    }
   ],
   "source": [
    "# check labels\n",
    "labels = []\n",
    "for y in y_train.flatten():\n",
    "    if y not in labels:\n",
    "        labels.append(y)\n",
    "print('training labels are: {}'.format(labels))\n",
    "print('# labels: {}'.format(len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to one-hot encoded vectors.\n",
    "y_train = to_categorical(y_train, num_classes = 10)\n",
    "y_test = to_categorical(y_test, num_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform images from (32,32,3) to 3072-dimensional vectors (32*32*3)\n",
    "\n",
    "x_train = np.reshape(x_train,(50000,3072))\n",
    "x_test = np.reshape(x_test,(10000,3072))\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalization of pixel values (to [0-1] range)\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. HELP FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotAcc(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model_accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc = 'lower right')\n",
    "    \n",
    "def plotLoss(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model_loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc = 'upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best accuracy result\n",
    "\n",
    "def getBest(record):\n",
    "    max_acc = 0\n",
    "    experiment = None\n",
    "    for value in record.values():\n",
    "        max_acc = max(max_acc, value[1])\n",
    "\n",
    "    for key in record.keys():\n",
    "        if record[key][1] == max_acc:\n",
    "              experiment = key\n",
    "\n",
    "    return max_acc, experiment, record[key][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the summary of all experiments\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def get_allResult(record):\n",
    "    df_result = pd.DataFrame()\n",
    "    length = len(list(record.keys()))\n",
    "    df_result['Experiment'] = list(record.keys())\n",
    "    df_result['Loss'] = [list(record.values())[i][0] for i in range(length)]\n",
    "    df_result['Accuracy'] = [list(record.values())[i][1] for i in range(length)]\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. EXPERIMENTS - TEST WITH #NEURON, #LAYERS, %DROPOUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Set up hyperparameters for different configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[64], 0.2], [[64], 0.4], [[64], 0.6], [[128], 0.2], [[128], 0.4]]\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "# historic configuration: will be exluded in this test\n",
    "# dropout_rate = 0.2\n",
    "# number_of_layer = 3\n",
    "# number_of_neuron = 256\n",
    "# configuration = [256,256]\n",
    "\n",
    "list_layer = [[64], [128], [256], [512], [1024],  \\\n",
    "        [64,64], [128,128], [512,512], [1024,1024], \\\n",
    "        [64,64,64], [128,128,128], [256,256,256], [512,512,512], [1024,1024,1024]]\n",
    "\n",
    "list_dropout_rate = [0.2,0.4,0.6]\n",
    "\n",
    "list_config = []\n",
    "for n_neuron in list_layer:\n",
    "    for dropout_rate in list_dropout_rate:\n",
    "        list_config.append([n_neuron, dropout_rate])\n",
    "        \n",
    "print(list_config[:5])\n",
    "print(len(list_config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Control other conditions and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set conditions\n",
    "n_class = 10\n",
    "input_dimension = 32*32*3\n",
    "\n",
    "# hyper parameters\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "# use the best combo of optimizer and lr from optimizer experiment\n",
    "optimizer = keras.optimizers.Adam(lr = 0.0001)\n",
    "\n",
    "# use the best activation function from activation experiment\n",
    "activation = 'relu'\n",
    "\n",
    "# record hyper parameter information in dictionary                 \n",
    "param_dict = {\n",
    "        'batch_size': 128,\n",
    "        'epochs': 20,\n",
    "        'optimizer': 'Adam with 0.0001 lr',\n",
    "        'activation': 'relu'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Set up experiment function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_config(list_config):\n",
    "    \n",
    "    record = {}            \n",
    "    model = Sequential()\n",
    "    for i, config in enumerate(list_config):   \n",
    "        layer = config[0]\n",
    "        dropout_rate = config[1]\n",
    "        for j, n_neuron in enumerate(layer):\n",
    "\n",
    "            if j == 0:\n",
    "                model.add(Dense(n_neuron, activation = activation, input_dim = input_dimension))\n",
    "                model.add(Dropout(dropout_rate))\n",
    "            else:\n",
    "                model.add(Dense(n_neuron, activation = activation))\n",
    "                model.add(Dropout(dropout_rate))\n",
    "\n",
    "        model.add(Dense(n_class, activation = 'softmax'))\n",
    "\n",
    "        model.compile(optimizer = optimizer,\n",
    "                      loss = 'categorical_crossentropy',\n",
    "                      metrics = ['accuracy'])\n",
    "\n",
    "        start = time.clock()\n",
    "        exp_config = model.fit(x_train, \n",
    "                               y_train, \n",
    "                               epochs = epochs, \n",
    "                               batch_size = batch_size,\n",
    "                               verbose = 2,\n",
    "                               validation_data = (x_test, y_test))\n",
    "        elapsed = (time.clock() - start)\n",
    "        scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "\n",
    "        # record experiment information:\n",
    "        record['layers and dropout_rates are: ' + str(list_config[i])] = [scores[0], scores[1], elapsed, exp_config]\n",
    "\n",
    "        # print experiment name:\n",
    "        print('*******************************************************')\n",
    "        print('=======================================================')\n",
    "        print('Experiment' + str(i) + ':'  + '\\t' + 'layers and dropout_rates are: ' + str(list_config[i]))\n",
    "        print('=======================================================')\n",
    "        # print all used parameters for this model\n",
    "#         for i,j in param_dict.items():\n",
    "#             print(str(i) + '\\t' + str(j)) \n",
    "#             print('-------------------------------------------------------')\n",
    "\n",
    "        # print running time used\n",
    "        print('Time Used: {}'.format(elapsed))\n",
    "        print('-------------------------------------------------------')\n",
    "\n",
    "        # print best loss and accuracy result\n",
    "        print('Test loss:', scores[0])\n",
    "        print('-------------------------------------------------------')\n",
    "        print('Test accuracy:', scores[1])\n",
    "\n",
    "        print('*******************************************************')\n",
    "\n",
    "    return record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 3s - loss: 2.0558 - acc: 0.2365 - val_loss: 1.9109 - val_acc: 0.3252\n",
      "Epoch 2/20\n",
      " - 2s - loss: 1.9382 - acc: 0.2964 - val_loss: 1.8601 - val_acc: 0.3542\n",
      "Epoch 3/20\n",
      " - 2s - loss: 1.8945 - acc: 0.3149 - val_loss: 1.8170 - val_acc: 0.3642\n",
      "Epoch 4/20\n",
      " - 2s - loss: 1.8639 - acc: 0.3290 - val_loss: 1.7979 - val_acc: 0.3740\n",
      "Epoch 5/20\n",
      " - 2s - loss: 1.8450 - acc: 0.3371 - val_loss: 1.7756 - val_acc: 0.3878\n",
      "Epoch 6/20\n",
      " - 2s - loss: 1.8271 - acc: 0.3448 - val_loss: 1.7561 - val_acc: 0.3873\n",
      "Epoch 7/20\n",
      " - 2s - loss: 1.8172 - acc: 0.3539 - val_loss: 1.7385 - val_acc: 0.3985\n",
      "Epoch 8/20\n",
      " - 2s - loss: 1.8053 - acc: 0.3527 - val_loss: 1.7307 - val_acc: 0.3977\n",
      "Epoch 9/20\n",
      " - 2s - loss: 1.7962 - acc: 0.3587 - val_loss: 1.7232 - val_acc: 0.4018\n",
      "Epoch 10/20\n",
      " - 2s - loss: 1.7861 - acc: 0.3646 - val_loss: 1.7141 - val_acc: 0.4100\n",
      "Epoch 11/20\n",
      " - 3s - loss: 1.7778 - acc: 0.3647 - val_loss: 1.7004 - val_acc: 0.4105\n",
      "Epoch 12/20\n",
      " - 2s - loss: 1.7674 - acc: 0.3737 - val_loss: 1.6930 - val_acc: 0.4162\n",
      "Epoch 13/20\n",
      " - 2s - loss: 1.7594 - acc: 0.3744 - val_loss: 1.6829 - val_acc: 0.4168\n",
      "Epoch 14/20\n",
      " - 2s - loss: 1.7479 - acc: 0.3809 - val_loss: 1.6736 - val_acc: 0.4186\n",
      "Epoch 15/20\n",
      " - 2s - loss: 1.7465 - acc: 0.3772 - val_loss: 1.6707 - val_acc: 0.4215\n",
      "Epoch 16/20\n",
      " - 2s - loss: 1.7381 - acc: 0.3820 - val_loss: 1.6619 - val_acc: 0.4239\n",
      "Epoch 17/20\n",
      " - 2s - loss: 1.7313 - acc: 0.3836 - val_loss: 1.6605 - val_acc: 0.4226\n",
      "Epoch 18/20\n",
      " - 2s - loss: 1.7238 - acc: 0.3857 - val_loss: 1.6466 - val_acc: 0.4274\n",
      "Epoch 19/20\n",
      " - 2s - loss: 1.7217 - acc: 0.3901 - val_loss: 1.6437 - val_acc: 0.4254\n",
      "Epoch 20/20\n",
      " - 2s - loss: 1.7174 - acc: 0.3864 - val_loss: 1.6442 - val_acc: 0.4313\n",
      "10000/10000 [==============================] - 0s 37us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment0:\tlayers and dropout_rates are: [[64], 0.2]\n",
      "=======================================================\n",
      "Time Used: 101.737743\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.6441544834136963\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.4313\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 3s - loss: 2.2136 - acc: 0.1792 - val_loss: 2.1208 - val_acc: 0.2156\n",
      "Epoch 2/20\n",
      " - 3s - loss: 2.1088 - acc: 0.2100 - val_loss: 2.0497 - val_acc: 0.2321\n",
      "Epoch 3/20\n",
      " - 2s - loss: 2.0616 - acc: 0.2196 - val_loss: 2.0148 - val_acc: 0.2367\n",
      "Epoch 4/20\n",
      " - 2s - loss: 2.0361 - acc: 0.2262 - val_loss: 1.9855 - val_acc: 0.2506\n",
      "Epoch 5/20\n",
      " - 2s - loss: 2.0125 - acc: 0.2349 - val_loss: 1.9652 - val_acc: 0.2536\n",
      "Epoch 6/20\n",
      " - 3s - loss: 1.9985 - acc: 0.2386 - val_loss: 1.9492 - val_acc: 0.2548\n",
      "Epoch 7/20\n",
      " - 2s - loss: 1.9834 - acc: 0.2425 - val_loss: 1.9306 - val_acc: 0.2613\n",
      "Epoch 8/20\n",
      " - 2s - loss: 1.9727 - acc: 0.2428 - val_loss: 1.9217 - val_acc: 0.2621\n",
      "Epoch 9/20\n",
      " - 2s - loss: 1.9623 - acc: 0.2492 - val_loss: 1.9113 - val_acc: 0.2634\n",
      "Epoch 10/20\n",
      " - 2s - loss: 1.9519 - acc: 0.2507 - val_loss: 1.9011 - val_acc: 0.2662\n",
      "Epoch 11/20\n",
      " - 3s - loss: 1.9428 - acc: 0.2553 - val_loss: 1.8926 - val_acc: 0.2637\n",
      "Epoch 12/20\n",
      " - 3s - loss: 1.9378 - acc: 0.2570 - val_loss: 1.8882 - val_acc: 0.2773\n",
      "Epoch 13/20\n",
      " - 3s - loss: 1.9278 - acc: 0.2612 - val_loss: 1.8755 - val_acc: 0.2828\n",
      "Epoch 14/20\n",
      " - 3s - loss: 1.9222 - acc: 0.2662 - val_loss: 1.8677 - val_acc: 0.2854\n",
      "Epoch 15/20\n",
      " - 3s - loss: 1.9159 - acc: 0.2690 - val_loss: 1.8607 - val_acc: 0.2944\n",
      "Epoch 16/20\n",
      " - 2s - loss: 1.9082 - acc: 0.2712 - val_loss: 1.8587 - val_acc: 0.2930\n",
      "Epoch 17/20\n",
      " - 2s - loss: 1.9044 - acc: 0.2759 - val_loss: 1.8509 - val_acc: 0.2982\n",
      "Epoch 18/20\n",
      " - 3s - loss: 1.8979 - acc: 0.2793 - val_loss: 1.8412 - val_acc: 0.3058\n",
      "Epoch 19/20\n",
      " - 3s - loss: 1.8910 - acc: 0.2840 - val_loss: 1.8400 - val_acc: 0.3052\n",
      "Epoch 20/20\n",
      " - 2s - loss: 1.8889 - acc: 0.2852 - val_loss: 1.8360 - val_acc: 0.3049\n",
      "10000/10000 [==============================] - 1s 60us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment1:\tlayers and dropout_rates are: [[64], 0.4]\n",
      "=======================================================\n",
      "Time Used: 105.79118600000001\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.8360366012573242\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.3049\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 3s - loss: 2.1962 - acc: 0.1697 - val_loss: 2.0911 - val_acc: 0.1995\n",
      "Epoch 2/20\n",
      " - 3s - loss: 2.0733 - acc: 0.2014 - val_loss: 1.9986 - val_acc: 0.2344\n",
      "Epoch 3/20\n",
      " - 3s - loss: 2.0278 - acc: 0.2166 - val_loss: 1.9606 - val_acc: 0.2434\n",
      "Epoch 4/20\n",
      " - 3s - loss: 2.0009 - acc: 0.2210 - val_loss: 1.9352 - val_acc: 0.2486\n",
      "Epoch 5/20\n",
      " - 3s - loss: 1.9811 - acc: 0.2253 - val_loss: 1.9141 - val_acc: 0.2540\n",
      "Epoch 6/20\n",
      " - 2s - loss: 1.9689 - acc: 0.2306 - val_loss: 1.9084 - val_acc: 0.2684\n",
      "Epoch 7/20\n",
      " - 2s - loss: 1.9598 - acc: 0.2366 - val_loss: 1.8881 - val_acc: 0.2757\n",
      "Epoch 8/20\n",
      " - 2s - loss: 1.9471 - acc: 0.2423 - val_loss: 1.8792 - val_acc: 0.2728\n",
      "Epoch 9/20\n",
      " - 3s - loss: 1.9406 - acc: 0.2476 - val_loss: 1.8705 - val_acc: 0.2776\n",
      "Epoch 10/20\n",
      " - 3s - loss: 1.9381 - acc: 0.2511 - val_loss: 1.8601 - val_acc: 0.2833\n",
      "Epoch 11/20\n",
      " - 3s - loss: 1.9265 - acc: 0.2534 - val_loss: 1.8527 - val_acc: 0.2861\n",
      "Epoch 12/20\n",
      " - 3s - loss: 1.9218 - acc: 0.2571 - val_loss: 1.8522 - val_acc: 0.2868\n",
      "Epoch 13/20\n",
      " - 2s - loss: 1.9179 - acc: 0.2585 - val_loss: 1.8446 - val_acc: 0.2937\n",
      "Epoch 14/20\n",
      " - 2s - loss: 1.9134 - acc: 0.2584 - val_loss: 1.8406 - val_acc: 0.2953\n",
      "Epoch 15/20\n",
      " - 2s - loss: 1.9064 - acc: 0.2630 - val_loss: 1.8406 - val_acc: 0.2927\n",
      "Epoch 16/20\n",
      " - 2s - loss: 1.9059 - acc: 0.2643 - val_loss: 1.8324 - val_acc: 0.2977\n",
      "Epoch 17/20\n",
      " - 2s - loss: 1.9024 - acc: 0.2685 - val_loss: 1.8298 - val_acc: 0.3030\n",
      "Epoch 18/20\n",
      " - 2s - loss: 1.8975 - acc: 0.2691 - val_loss: 1.8291 - val_acc: 0.3031\n",
      "Epoch 19/20\n",
      " - 2s - loss: 1.8947 - acc: 0.2699 - val_loss: 1.8203 - val_acc: 0.3046\n",
      "Epoch 20/20\n",
      " - 3s - loss: 1.8936 - acc: 0.2712 - val_loss: 1.8327 - val_acc: 0.3025\n",
      "10000/10000 [==============================] - 0s 49us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment2:\tlayers and dropout_rates are: [[64], 0.6]\n",
      "=======================================================\n",
      "Time Used: 119.50050200000004\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.8326551528930664\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.3025\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 3s - loss: 2.1164 - acc: 0.2085 - val_loss: 1.9682 - val_acc: 0.2710\n",
      "Epoch 2/20\n",
      " - 3s - loss: 1.9589 - acc: 0.2503 - val_loss: 1.9026 - val_acc: 0.2754\n",
      "Epoch 3/20\n",
      " - 2s - loss: 1.9255 - acc: 0.2592 - val_loss: 1.8770 - val_acc: 0.2890\n",
      "Epoch 4/20\n",
      " - 2s - loss: 1.9096 - acc: 0.2664 - val_loss: 1.8577 - val_acc: 0.2909\n",
      "Epoch 5/20\n",
      " - 2s - loss: 1.8953 - acc: 0.2674 - val_loss: 1.8530 - val_acc: 0.2936\n",
      "Epoch 6/20\n",
      " - 3s - loss: 1.8919 - acc: 0.2707 - val_loss: 1.8569 - val_acc: 0.2927\n",
      "Epoch 7/20\n",
      " - 3s - loss: 1.8894 - acc: 0.2731 - val_loss: 1.8430 - val_acc: 0.2956\n",
      "Epoch 8/20\n",
      " - 3s - loss: 1.8834 - acc: 0.2762 - val_loss: 1.8434 - val_acc: 0.2990\n",
      "Epoch 9/20\n",
      " - 2s - loss: 1.8772 - acc: 0.2805 - val_loss: 1.8322 - val_acc: 0.2994\n",
      "Epoch 10/20\n",
      " - 2s - loss: 1.8760 - acc: 0.2776 - val_loss: 1.8342 - val_acc: 0.3040\n",
      "Epoch 11/20\n",
      " - 3s - loss: 1.8757 - acc: 0.2805 - val_loss: 1.8295 - val_acc: 0.3049\n",
      "Epoch 12/20\n",
      " - 3s - loss: 1.8690 - acc: 0.2817 - val_loss: 1.8212 - val_acc: 0.3008\n",
      "Epoch 13/20\n",
      " - 3s - loss: 1.8671 - acc: 0.2822 - val_loss: 1.8189 - val_acc: 0.3070\n",
      "Epoch 14/20\n",
      " - 3s - loss: 1.8682 - acc: 0.2812 - val_loss: 1.8249 - val_acc: 0.3042\n",
      "Epoch 15/20\n",
      " - 3s - loss: 1.8663 - acc: 0.2843 - val_loss: 1.8214 - val_acc: 0.3060\n",
      "Epoch 16/20\n",
      " - 3s - loss: 1.8612 - acc: 0.2876 - val_loss: 1.8151 - val_acc: 0.3079\n",
      "Epoch 17/20\n",
      " - 3s - loss: 1.8612 - acc: 0.2866 - val_loss: 1.8152 - val_acc: 0.3068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20\n",
      " - 2s - loss: 1.8523 - acc: 0.2894 - val_loss: 1.8173 - val_acc: 0.3118\n",
      "Epoch 19/20\n",
      " - 2s - loss: 1.8575 - acc: 0.2871 - val_loss: 1.8121 - val_acc: 0.3102\n",
      "Epoch 20/20\n",
      " - 2s - loss: 1.8541 - acc: 0.2901 - val_loss: 1.8105 - val_acc: 0.3071\n",
      "10000/10000 [==============================] - 0s 32us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment3:\tlayers and dropout_rates are: [[128], 0.2]\n",
      "=======================================================\n",
      "Time Used: 124.401253\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.8104729793548584\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.3071\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 4s - loss: 2.1262 - acc: 0.1727 - val_loss: 2.0253 - val_acc: 0.2103\n",
      "Epoch 2/20\n",
      " - 3s - loss: 2.0098 - acc: 0.2092 - val_loss: 1.9566 - val_acc: 0.2652\n",
      "Epoch 3/20\n",
      " - 3s - loss: 1.9595 - acc: 0.2483 - val_loss: 1.9014 - val_acc: 0.2728\n",
      "Epoch 4/20\n",
      " - 3s - loss: 1.9238 - acc: 0.2568 - val_loss: 1.8756 - val_acc: 0.2836\n",
      "Epoch 5/20\n",
      " - 3s - loss: 1.9025 - acc: 0.2640 - val_loss: 1.8570 - val_acc: 0.2843\n",
      "Epoch 6/20\n",
      " - 3s - loss: 1.8963 - acc: 0.2680 - val_loss: 1.8616 - val_acc: 0.2794\n",
      "Epoch 7/20\n",
      " - 3s - loss: 1.8895 - acc: 0.2654 - val_loss: 1.8453 - val_acc: 0.2854\n",
      "Epoch 8/20\n",
      " - 3s - loss: 1.8851 - acc: 0.2668 - val_loss: 1.8425 - val_acc: 0.2851\n",
      "Epoch 9/20\n",
      " - 3s - loss: 1.8803 - acc: 0.2704 - val_loss: 1.8349 - val_acc: 0.2888\n",
      "Epoch 10/20\n",
      " - 3s - loss: 1.8767 - acc: 0.2724 - val_loss: 1.8407 - val_acc: 0.2926\n",
      "Epoch 11/20\n",
      " - 3s - loss: 1.8734 - acc: 0.2706 - val_loss: 1.8304 - val_acc: 0.2911\n",
      "Epoch 12/20\n",
      " - 3s - loss: 1.8661 - acc: 0.2783 - val_loss: 1.8319 - val_acc: 0.2970\n",
      "Epoch 13/20\n",
      " - 3s - loss: 1.8655 - acc: 0.2740 - val_loss: 1.8193 - val_acc: 0.3018\n",
      "Epoch 14/20\n",
      " - 3s - loss: 1.8669 - acc: 0.2754 - val_loss: 1.8179 - val_acc: 0.3007\n",
      "Epoch 15/20\n",
      " - 2s - loss: 1.8577 - acc: 0.2782 - val_loss: 1.8191 - val_acc: 0.3068\n",
      "Epoch 16/20\n",
      " - 3s - loss: 1.8591 - acc: 0.2776 - val_loss: 1.8192 - val_acc: 0.3062\n",
      "Epoch 17/20\n",
      " - 3s - loss: 1.8547 - acc: 0.2825 - val_loss: 1.8171 - val_acc: 0.3010\n",
      "Epoch 18/20\n",
      " - 3s - loss: 1.8532 - acc: 0.2836 - val_loss: 1.8080 - val_acc: 0.3063\n",
      "Epoch 19/20\n",
      " - 3s - loss: 1.8489 - acc: 0.2866 - val_loss: 1.8081 - val_acc: 0.3092\n",
      "Epoch 20/20\n",
      " - 3s - loss: 1.8466 - acc: 0.2888 - val_loss: 1.8049 - val_acc: 0.3140\n",
      "10000/10000 [==============================] - 1s 58us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment4:\tlayers and dropout_rates are: [[128], 0.4]\n",
      "=======================================================\n",
      "Time Used: 139.441142\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.804887415122986\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.314\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 6s - loss: 2.1155 - acc: 0.2306 - val_loss: 1.9565 - val_acc: 0.2850\n",
      "Epoch 2/20\n",
      " - 4s - loss: 1.9579 - acc: 0.2619 - val_loss: 1.8879 - val_acc: 0.2848\n",
      "Epoch 3/20\n",
      " - 3s - loss: 1.9186 - acc: 0.2659 - val_loss: 1.8635 - val_acc: 0.2895\n",
      "Epoch 4/20\n",
      " - 4s - loss: 1.9097 - acc: 0.2695 - val_loss: 1.8509 - val_acc: 0.2883\n",
      "Epoch 5/20\n",
      " - 4s - loss: 1.8951 - acc: 0.2682 - val_loss: 1.8430 - val_acc: 0.2928\n",
      "Epoch 6/20\n",
      " - 4s - loss: 1.8896 - acc: 0.2720 - val_loss: 1.8378 - val_acc: 0.2934\n",
      "Epoch 7/20\n",
      " - 5s - loss: 1.8901 - acc: 0.2686 - val_loss: 1.8378 - val_acc: 0.2927\n",
      "Epoch 8/20\n",
      " - 3s - loss: 1.8811 - acc: 0.2739 - val_loss: 1.8317 - val_acc: 0.2926\n",
      "Epoch 9/20\n",
      " - 3s - loss: 1.8825 - acc: 0.2727 - val_loss: 1.8326 - val_acc: 0.2908\n",
      "Epoch 10/20\n",
      " - 3s - loss: 1.8767 - acc: 0.2735 - val_loss: 1.8253 - val_acc: 0.2943\n",
      "Epoch 11/20\n",
      " - 3s - loss: 1.8749 - acc: 0.2740 - val_loss: 1.8261 - val_acc: 0.2929\n",
      "Epoch 12/20\n",
      " - 4s - loss: 1.8695 - acc: 0.2762 - val_loss: 1.8224 - val_acc: 0.2935\n",
      "Epoch 13/20\n",
      " - 3s - loss: 1.8671 - acc: 0.2803 - val_loss: 1.8189 - val_acc: 0.2955\n",
      "Epoch 14/20\n",
      " - 4s - loss: 1.8683 - acc: 0.2802 - val_loss: 1.8237 - val_acc: 0.2955\n",
      "Epoch 15/20\n",
      " - 3s - loss: 1.8647 - acc: 0.2802 - val_loss: 1.8213 - val_acc: 0.2980\n",
      "Epoch 16/20\n",
      " - 3s - loss: 1.8590 - acc: 0.2791 - val_loss: 1.8318 - val_acc: 0.2904\n",
      "Epoch 17/20\n",
      " - 3s - loss: 1.8609 - acc: 0.2803 - val_loss: 1.8161 - val_acc: 0.2946\n",
      "Epoch 18/20\n",
      " - 3s - loss: 1.8542 - acc: 0.2841 - val_loss: 1.8143 - val_acc: 0.2978\n",
      "Epoch 19/20\n",
      " - 3s - loss: 1.8497 - acc: 0.2846 - val_loss: 1.8151 - val_acc: 0.2932\n",
      "Epoch 20/20\n",
      " - 3s - loss: 1.8533 - acc: 0.2829 - val_loss: 1.8154 - val_acc: 0.2909\n",
      "10000/10000 [==============================] - 0s 49us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment5:\tlayers and dropout_rates are: [[128], 0.6]\n",
      "=======================================================\n",
      "Time Used: 148.9485259999999\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.8153788166046143\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.2909\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 5s - loss: 2.0510 - acc: 0.2250 - val_loss: 1.9110 - val_acc: 0.2779\n",
      "Epoch 2/20\n",
      " - 3s - loss: 1.9026 - acc: 0.2692 - val_loss: 1.8581 - val_acc: 0.2798\n",
      "Epoch 3/20\n",
      " - 3s - loss: 1.8683 - acc: 0.2779 - val_loss: 1.8414 - val_acc: 0.2843\n",
      "Epoch 4/20\n",
      " - 4s - loss: 1.8607 - acc: 0.2815 - val_loss: 1.8333 - val_acc: 0.2929\n",
      "Epoch 5/20\n",
      " - 3s - loss: 1.8557 - acc: 0.2809 - val_loss: 1.8302 - val_acc: 0.2956\n",
      "Epoch 6/20\n",
      " - 3s - loss: 1.8521 - acc: 0.2822 - val_loss: 1.8363 - val_acc: 0.2916\n",
      "Epoch 7/20\n",
      " - 3s - loss: 1.8492 - acc: 0.2843 - val_loss: 1.8238 - val_acc: 0.2952\n",
      "Epoch 8/20\n",
      " - 4s - loss: 1.8408 - acc: 0.2886 - val_loss: 1.8260 - val_acc: 0.2985\n",
      "Epoch 9/20\n",
      " - 4s - loss: 1.8417 - acc: 0.2880 - val_loss: 1.8200 - val_acc: 0.2966\n",
      "Epoch 10/20\n",
      " - 4s - loss: 1.8401 - acc: 0.2864 - val_loss: 1.8225 - val_acc: 0.2889\n",
      "Epoch 11/20\n",
      " - 4s - loss: 1.8386 - acc: 0.2874 - val_loss: 1.8200 - val_acc: 0.2938\n",
      "Epoch 12/20\n",
      " - 3s - loss: 1.8394 - acc: 0.2876 - val_loss: 1.8195 - val_acc: 0.2940\n",
      "Epoch 13/20\n",
      " - 3s - loss: 1.8368 - acc: 0.2889 - val_loss: 1.8223 - val_acc: 0.2931\n",
      "Epoch 14/20\n",
      " - 4s - loss: 1.8329 - acc: 0.2917 - val_loss: 1.8188 - val_acc: 0.2986\n",
      "Epoch 15/20\n",
      " - 3s - loss: 1.8318 - acc: 0.2917 - val_loss: 1.8160 - val_acc: 0.2959\n",
      "Epoch 16/20\n",
      " - 4s - loss: 1.8333 - acc: 0.2914 - val_loss: 1.8235 - val_acc: 0.2908\n",
      "Epoch 17/20\n",
      " - 3s - loss: 1.8321 - acc: 0.2924 - val_loss: 1.8124 - val_acc: 0.3011\n",
      "Epoch 18/20\n",
      " - 3s - loss: 1.8308 - acc: 0.2900 - val_loss: 1.8214 - val_acc: 0.2952\n",
      "Epoch 19/20\n",
      " - 3s - loss: 1.8301 - acc: 0.2912 - val_loss: 1.8065 - val_acc: 0.3011\n",
      "Epoch 20/20\n",
      " - 3s - loss: 1.8314 - acc: 0.2955 - val_loss: 1.8058 - val_acc: 0.3063\n",
      "10000/10000 [==============================] - 0s 44us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment6:\tlayers and dropout_rates are: [[256], 0.2]\n",
      "=======================================================\n",
      "Time Used: 171.92459900000006\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.8057677703857422\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.3063\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 5s - loss: 2.0561 - acc: 0.2133 - val_loss: 1.9384 - val_acc: 0.2322\n",
      "Epoch 2/20\n",
      " - 4s - loss: 1.9178 - acc: 0.2347 - val_loss: 1.8903 - val_acc: 0.2488\n",
      "Epoch 3/20\n",
      " - 3s - loss: 1.9022 - acc: 0.2387 - val_loss: 1.8740 - val_acc: 0.2570\n",
      "Epoch 4/20\n",
      " - 4s - loss: 1.8859 - acc: 0.2464 - val_loss: 1.8703 - val_acc: 0.2542\n",
      "Epoch 5/20\n",
      " - 3s - loss: 1.8867 - acc: 0.2451 - val_loss: 1.8638 - val_acc: 0.2535\n",
      "Epoch 6/20\n",
      " - 4s - loss: 1.8777 - acc: 0.2463 - val_loss: 1.8616 - val_acc: 0.2649\n",
      "Epoch 7/20\n",
      " - 4s - loss: 1.8761 - acc: 0.2540 - val_loss: 1.8539 - val_acc: 0.2735\n",
      "Epoch 8/20\n",
      " - 3s - loss: 1.8675 - acc: 0.2599 - val_loss: 1.8377 - val_acc: 0.2868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20\n",
      " - 4s - loss: 1.8584 - acc: 0.2735 - val_loss: 1.8294 - val_acc: 0.2882\n",
      "Epoch 10/20\n",
      " - 4s - loss: 1.8509 - acc: 0.2753 - val_loss: 1.8265 - val_acc: 0.2897\n",
      "Epoch 11/20\n",
      " - 4s - loss: 1.8471 - acc: 0.2772 - val_loss: 1.8183 - val_acc: 0.2992\n",
      "Epoch 12/20\n",
      " - 4s - loss: 1.8442 - acc: 0.2794 - val_loss: 1.8144 - val_acc: 0.2990\n",
      "Epoch 13/20\n",
      " - 4s - loss: 1.8438 - acc: 0.2823 - val_loss: 1.8200 - val_acc: 0.2961\n",
      "Epoch 14/20\n",
      " - 4s - loss: 1.8388 - acc: 0.2811 - val_loss: 1.8187 - val_acc: 0.2962\n",
      "Epoch 15/20\n",
      " - 4s - loss: 1.8381 - acc: 0.2841 - val_loss: 1.8129 - val_acc: 0.3049\n",
      "Epoch 16/20\n",
      " - 4s - loss: 1.8343 - acc: 0.2863 - val_loss: 1.8102 - val_acc: 0.2991\n",
      "Epoch 17/20\n",
      " - 4s - loss: 1.8384 - acc: 0.2837 - val_loss: 1.8130 - val_acc: 0.2999\n",
      "Epoch 18/20\n",
      " - 4s - loss: 1.8390 - acc: 0.2827 - val_loss: 1.8133 - val_acc: 0.2998\n",
      "Epoch 19/20\n",
      " - 4s - loss: 1.8345 - acc: 0.2859 - val_loss: 1.8061 - val_acc: 0.3048\n",
      "Epoch 20/20\n",
      " - 4s - loss: 1.8335 - acc: 0.2836 - val_loss: 1.8054 - val_acc: 0.2993\n",
      "10000/10000 [==============================] - 1s 60us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment7:\tlayers and dropout_rates are: [[256], 0.4]\n",
      "=======================================================\n",
      "Time Used: 179.2848879999999\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.8054248935699464\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.2993\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 7s - loss: 2.0728 - acc: 0.2285 - val_loss: 1.9199 - val_acc: 0.2762\n",
      "Epoch 2/20\n",
      " - 4s - loss: 1.9171 - acc: 0.2585 - val_loss: 1.8697 - val_acc: 0.2809\n",
      "Epoch 3/20\n",
      " - 4s - loss: 1.8855 - acc: 0.2693 - val_loss: 1.8429 - val_acc: 0.2874\n",
      "Epoch 4/20\n",
      " - 4s - loss: 1.8692 - acc: 0.2708 - val_loss: 1.8350 - val_acc: 0.2856\n",
      "Epoch 5/20\n",
      " - 5s - loss: 1.8587 - acc: 0.2738 - val_loss: 1.8370 - val_acc: 0.2937\n",
      "Epoch 6/20\n",
      " - 5s - loss: 1.8512 - acc: 0.2802 - val_loss: 1.8255 - val_acc: 0.2895\n",
      "Epoch 7/20\n",
      " - 5s - loss: 1.8455 - acc: 0.2838 - val_loss: 1.8223 - val_acc: 0.2924\n",
      "Epoch 8/20\n",
      " - 5s - loss: 1.8450 - acc: 0.2824 - val_loss: 1.8232 - val_acc: 0.2895\n",
      "Epoch 9/20\n",
      " - 4s - loss: 1.8430 - acc: 0.2817 - val_loss: 1.8197 - val_acc: 0.2904\n",
      "Epoch 10/20\n",
      " - 5s - loss: 1.8363 - acc: 0.2833 - val_loss: 1.8222 - val_acc: 0.2908\n",
      "Epoch 11/20\n",
      " - 4s - loss: 1.8324 - acc: 0.2842 - val_loss: 1.8145 - val_acc: 0.2921\n",
      "Epoch 12/20\n",
      " - 5s - loss: 1.8341 - acc: 0.2851 - val_loss: 1.8126 - val_acc: 0.2975\n",
      "Epoch 13/20\n",
      " - 4s - loss: 1.8361 - acc: 0.2825 - val_loss: 1.8125 - val_acc: 0.2946\n",
      "Epoch 14/20\n",
      " - 5s - loss: 1.8296 - acc: 0.2876 - val_loss: 1.8217 - val_acc: 0.2911\n",
      "Epoch 15/20\n",
      " - 4s - loss: 1.8288 - acc: 0.2881 - val_loss: 1.8161 - val_acc: 0.2933\n",
      "Epoch 16/20\n",
      " - 5s - loss: 1.8285 - acc: 0.2906 - val_loss: 1.8126 - val_acc: 0.2964\n",
      "Epoch 17/20\n",
      " - 4s - loss: 1.8249 - acc: 0.2897 - val_loss: 1.8154 - val_acc: 0.2957\n",
      "Epoch 18/20\n",
      " - 5s - loss: 1.8245 - acc: 0.2899 - val_loss: 1.8153 - val_acc: 0.2975\n",
      "Epoch 19/20\n",
      " - 5s - loss: 1.8242 - acc: 0.2885 - val_loss: 1.8024 - val_acc: 0.2987\n",
      "Epoch 20/20\n",
      " - 4s - loss: 1.8260 - acc: 0.2910 - val_loss: 1.8085 - val_acc: 0.2952\n",
      "10000/10000 [==============================] - 1s 58us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment8:\tlayers and dropout_rates are: [[256], 0.6]\n",
      "=======================================================\n",
      "Time Used: 210.00820399999998\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.8084767501831054\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.2952\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 9s - loss: 2.0656 - acc: 0.2040 - val_loss: 1.9450 - val_acc: 0.2410\n",
      "Epoch 2/20\n",
      " - 5s - loss: 1.9232 - acc: 0.2541 - val_loss: 1.8599 - val_acc: 0.2905\n",
      "Epoch 3/20\n",
      " - 6s - loss: 1.8750 - acc: 0.2779 - val_loss: 1.8472 - val_acc: 0.2905\n",
      "Epoch 4/20\n",
      " - 6s - loss: 1.8639 - acc: 0.2831 - val_loss: 1.8470 - val_acc: 0.2879\n",
      "Epoch 5/20\n",
      " - 5s - loss: 1.8597 - acc: 0.2835 - val_loss: 1.8432 - val_acc: 0.2913\n",
      "Epoch 6/20\n",
      " - 6s - loss: 1.8559 - acc: 0.2846 - val_loss: 1.8429 - val_acc: 0.2933\n",
      "Epoch 7/20\n",
      " - 6s - loss: 1.8438 - acc: 0.2876 - val_loss: 1.8347 - val_acc: 0.3009\n",
      "Epoch 8/20\n",
      " - 7s - loss: 1.8317 - acc: 0.2943 - val_loss: 1.8254 - val_acc: 0.2953\n",
      "Epoch 9/20\n",
      " - 6s - loss: 1.8321 - acc: 0.2916 - val_loss: 1.8373 - val_acc: 0.3064\n",
      "Epoch 10/20\n",
      " - 6s - loss: 1.8320 - acc: 0.2937 - val_loss: 1.8192 - val_acc: 0.2954\n",
      "Epoch 11/20\n",
      " - 6s - loss: 1.8292 - acc: 0.2958 - val_loss: 1.8222 - val_acc: 0.2978\n",
      "Epoch 12/20\n",
      " - 5s - loss: 1.8287 - acc: 0.2922 - val_loss: 1.8230 - val_acc: 0.3001\n",
      "Epoch 13/20\n",
      " - 5s - loss: 1.8247 - acc: 0.2952 - val_loss: 1.8116 - val_acc: 0.3116\n",
      "Epoch 14/20\n",
      " - 6s - loss: 1.8235 - acc: 0.2938 - val_loss: 1.8110 - val_acc: 0.3037\n",
      "Epoch 15/20\n",
      " - 6s - loss: 1.8230 - acc: 0.2951 - val_loss: 1.8163 - val_acc: 0.3047\n",
      "Epoch 16/20\n",
      " - 6s - loss: 1.8227 - acc: 0.2939 - val_loss: 1.8127 - val_acc: 0.3029\n",
      "Epoch 17/20\n",
      " - 6s - loss: 1.8199 - acc: 0.2952 - val_loss: 1.8161 - val_acc: 0.3022\n",
      "Epoch 18/20\n",
      " - 6s - loss: 1.8190 - acc: 0.2976 - val_loss: 1.8117 - val_acc: 0.3070\n",
      "Epoch 19/20\n",
      " - 6s - loss: 1.8186 - acc: 0.2991 - val_loss: 1.8214 - val_acc: 0.3047\n",
      "Epoch 20/20\n",
      " - 6s - loss: 1.8168 - acc: 0.2996 - val_loss: 1.8204 - val_acc: 0.3078\n",
      "10000/10000 [==============================] - 1s 69us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment9:\tlayers and dropout_rates are: [[512], 0.2]\n",
      "=======================================================\n",
      "Time Used: 273.4806920000001\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.8203726732254029\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.3078\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 10s - loss: 2.0201 - acc: 0.2333 - val_loss: 1.8906 - val_acc: 0.2625\n",
      "Epoch 2/20\n",
      " - 7s - loss: 1.8688 - acc: 0.2723 - val_loss: 1.8511 - val_acc: 0.2854\n",
      "Epoch 3/20\n",
      " - 6s - loss: 1.8427 - acc: 0.2850 - val_loss: 1.8320 - val_acc: 0.2905\n",
      "Epoch 4/20\n",
      " - 7s - loss: 1.8302 - acc: 0.2889 - val_loss: 1.8342 - val_acc: 0.2902\n",
      "Epoch 5/20\n",
      " - 7s - loss: 1.8324 - acc: 0.2854 - val_loss: 1.8401 - val_acc: 0.2887\n",
      "Epoch 6/20\n",
      " - 7s - loss: 1.8262 - acc: 0.2913 - val_loss: 1.8202 - val_acc: 0.2946\n",
      "Epoch 7/20\n",
      " - 8s - loss: 1.8241 - acc: 0.2923 - val_loss: 1.8240 - val_acc: 0.2987\n",
      "Epoch 8/20\n",
      " - 7s - loss: 1.8234 - acc: 0.2901 - val_loss: 1.8176 - val_acc: 0.2986\n",
      "Epoch 9/20\n",
      " - 7s - loss: 1.8193 - acc: 0.2952 - val_loss: 1.8324 - val_acc: 0.2950\n",
      "Epoch 10/20\n",
      " - 10s - loss: 1.8197 - acc: 0.2937 - val_loss: 1.8136 - val_acc: 0.3055\n",
      "Epoch 11/20\n",
      " - 7s - loss: 1.8225 - acc: 0.2954 - val_loss: 1.8144 - val_acc: 0.3078\n",
      "Epoch 12/20\n",
      " - 8s - loss: 1.8197 - acc: 0.2950 - val_loss: 1.8169 - val_acc: 0.2997\n",
      "Epoch 13/20\n",
      " - 6s - loss: 1.8174 - acc: 0.2959 - val_loss: 1.8203 - val_acc: 0.3092\n",
      "Epoch 14/20\n",
      " - 6s - loss: 1.8149 - acc: 0.2971 - val_loss: 1.8170 - val_acc: 0.2974\n",
      "Epoch 15/20\n",
      " - 6s - loss: 1.8113 - acc: 0.2971 - val_loss: 1.8162 - val_acc: 0.3117\n",
      "Epoch 16/20\n",
      " - 6s - loss: 1.8142 - acc: 0.2996 - val_loss: 1.8287 - val_acc: 0.2951\n",
      "Epoch 17/20\n",
      " - 7s - loss: 1.8133 - acc: 0.2997 - val_loss: 1.8245 - val_acc: 0.2965\n",
      "Epoch 18/20\n",
      " - 6s - loss: 1.8101 - acc: 0.2993 - val_loss: 1.8114 - val_acc: 0.3096\n",
      "Epoch 19/20\n",
      " - 6s - loss: 1.8137 - acc: 0.2986 - val_loss: 1.8038 - val_acc: 0.3133\n",
      "Epoch 20/20\n",
      " - 6s - loss: 1.8123 - acc: 0.3006 - val_loss: 1.8136 - val_acc: 0.3117\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment10:\tlayers and dropout_rates are: [[512], 0.4]\n",
      "=======================================================\n",
      "Time Used: 309.89238899999987\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.8135937759399414\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.3117\n",
      "*******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 10s - loss: 2.0217 - acc: 0.2446 - val_loss: 1.8643 - val_acc: 0.2814\n",
      "Epoch 2/20\n",
      " - 7s - loss: 1.8650 - acc: 0.2758 - val_loss: 1.8412 - val_acc: 0.2883\n",
      "Epoch 3/20\n",
      " - 7s - loss: 1.8442 - acc: 0.2840 - val_loss: 1.8319 - val_acc: 0.2940\n",
      "Epoch 4/20\n",
      " - 7s - loss: 1.8349 - acc: 0.2859 - val_loss: 1.8283 - val_acc: 0.2946\n",
      "Epoch 5/20\n",
      " - 7s - loss: 1.8285 - acc: 0.2869 - val_loss: 1.8241 - val_acc: 0.2944\n",
      "Epoch 6/20\n",
      " - 7s - loss: 1.8253 - acc: 0.2901 - val_loss: 1.8203 - val_acc: 0.2952\n",
      "Epoch 7/20\n",
      " - 7s - loss: 1.8263 - acc: 0.2886 - val_loss: 1.8171 - val_acc: 0.2961\n",
      "Epoch 8/20\n",
      " - 7s - loss: 1.8283 - acc: 0.2895 - val_loss: 1.8161 - val_acc: 0.2950\n",
      "Epoch 9/20\n",
      " - 7s - loss: 1.8219 - acc: 0.2927 - val_loss: 1.8145 - val_acc: 0.2975\n",
      "Epoch 10/20\n",
      " - 7s - loss: 1.8220 - acc: 0.2925 - val_loss: 1.8242 - val_acc: 0.2943\n",
      "Epoch 11/20\n",
      " - 7s - loss: 1.8194 - acc: 0.2944 - val_loss: 1.8177 - val_acc: 0.2979\n",
      "Epoch 12/20\n",
      " - 6s - loss: 1.8188 - acc: 0.2963 - val_loss: 1.8065 - val_acc: 0.2980\n",
      "Epoch 13/20\n",
      " - 6s - loss: 1.8169 - acc: 0.2951 - val_loss: 1.8193 - val_acc: 0.3019\n",
      "Epoch 14/20\n",
      " - 6s - loss: 1.8173 - acc: 0.2943 - val_loss: 1.8134 - val_acc: 0.3013\n",
      "Epoch 15/20\n",
      " - 6s - loss: 1.8136 - acc: 0.2960 - val_loss: 1.8097 - val_acc: 0.3088\n",
      "Epoch 16/20\n",
      " - 6s - loss: 1.8128 - acc: 0.2949 - val_loss: 1.8096 - val_acc: 0.3033\n",
      "Epoch 17/20\n",
      " - 6s - loss: 1.8147 - acc: 0.2979 - val_loss: 1.8051 - val_acc: 0.3053\n",
      "Epoch 18/20\n",
      " - 6s - loss: 1.8112 - acc: 0.2969 - val_loss: 1.8123 - val_acc: 0.2967\n",
      "Epoch 19/20\n",
      " - 6s - loss: 1.8145 - acc: 0.2980 - val_loss: 1.8117 - val_acc: 0.3106\n",
      "Epoch 20/20\n",
      " - 6s - loss: 1.8104 - acc: 0.2986 - val_loss: 1.8141 - val_acc: 0.3018\n",
      "10000/10000 [==============================] - 1s 72us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment11:\tlayers and dropout_rates are: [[512], 0.6]\n",
      "=======================================================\n",
      "Time Used: 348.1284509999998\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.8141087619781495\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.3018\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 11s - loss: 1.9619 - acc: 0.2562 - val_loss: 1.8724 - val_acc: 0.2790\n",
      "Epoch 2/20\n",
      " - 8s - loss: 1.8576 - acc: 0.2880 - val_loss: 1.8346 - val_acc: 0.2957\n",
      "Epoch 3/20\n",
      " - 8s - loss: 1.8472 - acc: 0.2899 - val_loss: 1.8331 - val_acc: 0.3025\n",
      "Epoch 4/20\n",
      " - 8s - loss: 1.8423 - acc: 0.2899 - val_loss: 1.8321 - val_acc: 0.2964\n",
      "Epoch 5/20\n",
      " - 8s - loss: 1.8318 - acc: 0.2938 - val_loss: 1.8219 - val_acc: 0.2974\n",
      "Epoch 6/20\n",
      " - 8s - loss: 1.8243 - acc: 0.2968 - val_loss: 1.8324 - val_acc: 0.2962\n",
      "Epoch 7/20\n",
      " - 8s - loss: 1.8228 - acc: 0.2979 - val_loss: 1.8310 - val_acc: 0.2966\n",
      "Epoch 8/20\n",
      " - 8s - loss: 1.8112 - acc: 0.2995 - val_loss: 1.8140 - val_acc: 0.3001\n",
      "Epoch 9/20\n",
      " - 8s - loss: 1.8149 - acc: 0.2985 - val_loss: 1.8206 - val_acc: 0.2971\n",
      "Epoch 10/20\n",
      " - 8s - loss: 1.8064 - acc: 0.3000 - val_loss: 1.8226 - val_acc: 0.2992\n",
      "Epoch 11/20\n",
      " - 8s - loss: 1.8101 - acc: 0.2991 - val_loss: 1.8078 - val_acc: 0.2993\n",
      "Epoch 12/20\n",
      " - 8s - loss: 1.8015 - acc: 0.3023 - val_loss: 1.8059 - val_acc: 0.3050\n",
      "Epoch 13/20\n",
      " - 8s - loss: 1.8024 - acc: 0.3038 - val_loss: 1.8173 - val_acc: 0.3029\n",
      "Epoch 14/20\n",
      " - 8s - loss: 1.8040 - acc: 0.3016 - val_loss: 1.8116 - val_acc: 0.3032\n",
      "Epoch 15/20\n",
      " - 8s - loss: 1.8032 - acc: 0.3027 - val_loss: 1.8130 - val_acc: 0.3060\n",
      "Epoch 16/20\n",
      " - 8s - loss: 1.8047 - acc: 0.3015 - val_loss: 1.8016 - val_acc: 0.3112\n",
      "Epoch 17/20\n",
      " - 8s - loss: 1.8005 - acc: 0.3047 - val_loss: 1.8023 - val_acc: 0.3119\n",
      "Epoch 18/20\n",
      " - 8s - loss: 1.8029 - acc: 0.3031 - val_loss: 1.8051 - val_acc: 0.3124\n",
      "Epoch 19/20\n",
      " - 7s - loss: 1.7963 - acc: 0.3078 - val_loss: 1.8042 - val_acc: 0.3143\n",
      "Epoch 20/20\n",
      " - 8s - loss: 1.8027 - acc: 0.3066 - val_loss: 1.7985 - val_acc: 0.3126\n",
      "10000/10000 [==============================] - 1s 79us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment12:\tlayers and dropout_rates are: [[1024], 0.2]\n",
      "=======================================================\n",
      "Time Used: 408.89216899999974\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.798460185623169\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.3126\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 13s - loss: 1.9567 - acc: 0.2620 - val_loss: 1.8387 - val_acc: 0.2898\n",
      "Epoch 2/20\n",
      " - 9s - loss: 1.8387 - acc: 0.2862 - val_loss: 1.8261 - val_acc: 0.3001\n",
      "Epoch 3/20\n",
      " - 9s - loss: 1.8208 - acc: 0.2944 - val_loss: 1.8281 - val_acc: 0.2921\n",
      "Epoch 4/20\n",
      " - 9s - loss: 1.8142 - acc: 0.2953 - val_loss: 1.8162 - val_acc: 0.2966\n",
      "Epoch 5/20\n",
      " - 9s - loss: 1.8095 - acc: 0.2998 - val_loss: 1.8322 - val_acc: 0.2978\n",
      "Epoch 6/20\n",
      " - 9s - loss: 1.8075 - acc: 0.3006 - val_loss: 1.8171 - val_acc: 0.2992\n",
      "Epoch 7/20\n",
      " - 9s - loss: 1.8115 - acc: 0.2992 - val_loss: 1.8084 - val_acc: 0.3025\n",
      "Epoch 8/20\n",
      " - 9s - loss: 1.8032 - acc: 0.3032 - val_loss: 1.8085 - val_acc: 0.3120\n",
      "Epoch 9/20\n",
      " - 9s - loss: 1.8055 - acc: 0.3019 - val_loss: 1.8124 - val_acc: 0.3059\n",
      "Epoch 10/20\n",
      " - 9s - loss: 1.8036 - acc: 0.3028 - val_loss: 1.8057 - val_acc: 0.3173\n",
      "Epoch 11/20\n",
      " - 9s - loss: 1.8005 - acc: 0.3032 - val_loss: 1.8086 - val_acc: 0.3124\n",
      "Epoch 12/20\n",
      " - 9s - loss: 1.8014 - acc: 0.3044 - val_loss: 1.8043 - val_acc: 0.3120\n",
      "Epoch 13/20\n",
      " - 9s - loss: 1.7957 - acc: 0.3037 - val_loss: 1.8078 - val_acc: 0.3111\n",
      "Epoch 14/20\n",
      " - 9s - loss: 1.7994 - acc: 0.3023 - val_loss: 1.8007 - val_acc: 0.3123\n",
      "Epoch 15/20\n",
      " - 9s - loss: 1.7986 - acc: 0.3062 - val_loss: 1.7978 - val_acc: 0.3106\n",
      "Epoch 16/20\n",
      " - 9s - loss: 1.7989 - acc: 0.3064 - val_loss: 1.8242 - val_acc: 0.3004\n",
      "Epoch 17/20\n",
      " - 9s - loss: 1.7992 - acc: 0.3050 - val_loss: 1.7991 - val_acc: 0.3147\n",
      "Epoch 18/20\n",
      " - 9s - loss: 1.7965 - acc: 0.3098 - val_loss: 1.7980 - val_acc: 0.3134\n",
      "Epoch 19/20\n",
      " - 9s - loss: 1.7940 - acc: 0.3078 - val_loss: 1.8083 - val_acc: 0.3118\n",
      "Epoch 20/20\n",
      " - 9s - loss: 1.7951 - acc: 0.3088 - val_loss: 1.7988 - val_acc: 0.3143\n",
      "10000/10000 [==============================] - 1s 87us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment13:\tlayers and dropout_rates are: [[1024], 0.4]\n",
      "=======================================================\n",
      "Time Used: 472.23965099999987\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.79880332736969\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.3143\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 15s - loss: 2.0813 - acc: 0.2177 - val_loss: 1.9066 - val_acc: 0.2726\n",
      "Epoch 2/20\n",
      " - 10s - loss: 1.8526 - acc: 0.2792 - val_loss: 1.8289 - val_acc: 0.2947\n",
      "Epoch 3/20\n",
      " - 10s - loss: 1.8225 - acc: 0.2909 - val_loss: 1.8190 - val_acc: 0.2953\n",
      "Epoch 4/20\n",
      " - 10s - loss: 1.8169 - acc: 0.2944 - val_loss: 1.8250 - val_acc: 0.2927\n",
      "Epoch 5/20\n",
      " - 10s - loss: 1.8088 - acc: 0.2950 - val_loss: 1.8181 - val_acc: 0.2965\n",
      "Epoch 6/20\n",
      " - 10s - loss: 1.8093 - acc: 0.3005 - val_loss: 1.8087 - val_acc: 0.3006\n",
      "Epoch 7/20\n",
      " - 10s - loss: 1.8042 - acc: 0.2977 - val_loss: 1.8113 - val_acc: 0.3047\n",
      "Epoch 8/20\n",
      " - 10s - loss: 1.8057 - acc: 0.3029 - val_loss: 1.8063 - val_acc: 0.3062\n",
      "Epoch 9/20\n",
      " - 10s - loss: 1.8004 - acc: 0.3006 - val_loss: 1.8173 - val_acc: 0.2987\n",
      "Epoch 10/20\n",
      " - 10s - loss: 1.8015 - acc: 0.2999 - val_loss: 1.8137 - val_acc: 0.3034\n",
      "Epoch 11/20\n",
      " - 10s - loss: 1.8021 - acc: 0.3017 - val_loss: 1.8052 - val_acc: 0.3135\n",
      "Epoch 12/20\n",
      " - 10s - loss: 1.8020 - acc: 0.3028 - val_loss: 1.8122 - val_acc: 0.3038\n",
      "Epoch 13/20\n",
      " - 10s - loss: 1.7998 - acc: 0.3027 - val_loss: 1.8177 - val_acc: 0.3085\n",
      "Epoch 14/20\n",
      " - 10s - loss: 1.8009 - acc: 0.3029 - val_loss: 1.8134 - val_acc: 0.3035\n",
      "Epoch 15/20\n",
      " - 10s - loss: 1.8010 - acc: 0.3062 - val_loss: 1.8053 - val_acc: 0.3083\n",
      "Epoch 16/20\n",
      " - 10s - loss: 1.7996 - acc: 0.3046 - val_loss: 1.8099 - val_acc: 0.3048\n",
      "Epoch 17/20\n",
      " - 10s - loss: 1.7924 - acc: 0.3105 - val_loss: 1.8020 - val_acc: 0.3024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20\n",
      " - 10s - loss: 1.7978 - acc: 0.3052 - val_loss: 1.8000 - val_acc: 0.3193\n",
      "Epoch 19/20\n",
      " - 10s - loss: 1.7999 - acc: 0.3071 - val_loss: 1.8078 - val_acc: 0.3067\n",
      "Epoch 20/20\n",
      " - 10s - loss: 1.7937 - acc: 0.3090 - val_loss: 1.7942 - val_acc: 0.3170\n",
      "10000/10000 [==============================] - 1s 95us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment14:\tlayers and dropout_rates are: [[1024], 0.6]\n",
      "=======================================================\n",
      "Time Used: 535.0850809999997\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.7942081130981444\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.317\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 15s - loss: 2.1103 - acc: 0.2077 - val_loss: 1.9356 - val_acc: 0.2444\n",
      "Epoch 2/20\n",
      " - 11s - loss: 1.9223 - acc: 0.2362 - val_loss: 1.8961 - val_acc: 0.2507\n",
      "Epoch 3/20\n",
      " - 11s - loss: 1.9059 - acc: 0.2396 - val_loss: 1.8716 - val_acc: 0.2540\n",
      "Epoch 4/20\n",
      " - 10s - loss: 1.8946 - acc: 0.2443 - val_loss: 1.8652 - val_acc: 0.2680\n",
      "Epoch 5/20\n",
      " - 10s - loss: 1.8790 - acc: 0.2508 - val_loss: 1.8704 - val_acc: 0.2660\n",
      "Epoch 6/20\n",
      " - 10s - loss: 1.8757 - acc: 0.2555 - val_loss: 1.8655 - val_acc: 0.2604\n",
      "Epoch 7/20\n",
      " - 10s - loss: 1.8679 - acc: 0.2601 - val_loss: 1.8556 - val_acc: 0.2790\n",
      "Epoch 8/20\n",
      " - 10s - loss: 1.8688 - acc: 0.2642 - val_loss: 1.8521 - val_acc: 0.2847\n",
      "Epoch 9/20\n",
      " - 10s - loss: 1.8618 - acc: 0.2694 - val_loss: 1.8429 - val_acc: 0.2895\n",
      "Epoch 10/20\n",
      " - 10s - loss: 1.8508 - acc: 0.2815 - val_loss: 1.8365 - val_acc: 0.2893\n",
      "Epoch 11/20\n",
      " - 10s - loss: 1.8389 - acc: 0.2855 - val_loss: 1.8229 - val_acc: 0.2910\n",
      "Epoch 12/20\n",
      " - 10s - loss: 1.8315 - acc: 0.2888 - val_loss: 1.8154 - val_acc: 0.2983\n",
      "Epoch 13/20\n",
      " - 10s - loss: 1.8221 - acc: 0.2922 - val_loss: 1.8117 - val_acc: 0.2937\n",
      "Epoch 14/20\n",
      " - 10s - loss: 1.8272 - acc: 0.2888 - val_loss: 1.8088 - val_acc: 0.2954\n",
      "Epoch 15/20\n",
      " - 10s - loss: 1.8251 - acc: 0.2906 - val_loss: 1.8153 - val_acc: 0.3021\n",
      "Epoch 16/20\n",
      " - 10s - loss: 1.8157 - acc: 0.2938 - val_loss: 1.8120 - val_acc: 0.2936\n",
      "Epoch 17/20\n",
      " - 10s - loss: 1.8217 - acc: 0.2910 - val_loss: 1.8101 - val_acc: 0.2960\n",
      "Epoch 18/20\n",
      " - 10s - loss: 1.8178 - acc: 0.2925 - val_loss: 1.8117 - val_acc: 0.2968\n",
      "Epoch 19/20\n",
      " - 10s - loss: 1.8173 - acc: 0.2946 - val_loss: 1.8067 - val_acc: 0.3103\n",
      "Epoch 20/20\n",
      " - 10s - loss: 1.8120 - acc: 0.2954 - val_loss: 1.8039 - val_acc: 0.3125\n",
      "10000/10000 [==============================] - 1s 101us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment15:\tlayers and dropout_rates are: [[64, 64], 0.2]\n",
      "=======================================================\n",
      "Time Used: 539.3730440000004\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.8038884384155274\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.3125\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 17s - loss: 2.1906 - acc: 0.1823 - val_loss: 2.0727 - val_acc: 0.2278\n",
      "Epoch 2/20\n",
      " - 11s - loss: 2.0306 - acc: 0.2304 - val_loss: 1.9332 - val_acc: 0.2678\n",
      "Epoch 3/20\n",
      " - 11s - loss: 1.9432 - acc: 0.2519 - val_loss: 1.8904 - val_acc: 0.2710\n",
      "Epoch 4/20\n",
      " - 11s - loss: 1.9011 - acc: 0.2610 - val_loss: 1.8492 - val_acc: 0.2847\n",
      "Epoch 5/20\n",
      " - 11s - loss: 1.8972 - acc: 0.2629 - val_loss: 1.8561 - val_acc: 0.2830\n",
      "Epoch 6/20\n",
      " - 11s - loss: 1.8778 - acc: 0.2698 - val_loss: 1.8394 - val_acc: 0.2893\n",
      "Epoch 7/20\n",
      " - 11s - loss: 1.8650 - acc: 0.2748 - val_loss: 1.8268 - val_acc: 0.2926\n",
      "Epoch 8/20\n",
      " - 11s - loss: 1.8566 - acc: 0.2760 - val_loss: 1.8302 - val_acc: 0.2882\n",
      "Epoch 9/20\n",
      " - 10s - loss: 1.8550 - acc: 0.2796 - val_loss: 1.8340 - val_acc: 0.2918\n",
      "Epoch 10/20\n",
      " - 10s - loss: 1.8631 - acc: 0.2741 - val_loss: 1.8311 - val_acc: 0.2871\n",
      "Epoch 11/20\n",
      " - 11s - loss: 1.8587 - acc: 0.2791 - val_loss: 1.8164 - val_acc: 0.2897\n",
      "Epoch 12/20\n",
      " - 11s - loss: 1.8519 - acc: 0.2790 - val_loss: 1.8236 - val_acc: 0.2935\n",
      "Epoch 13/20\n",
      " - 11s - loss: 1.8510 - acc: 0.2793 - val_loss: 1.8224 - val_acc: 0.2888\n",
      "Epoch 14/20\n",
      " - 11s - loss: 1.8437 - acc: 0.2817 - val_loss: 1.8259 - val_acc: 0.2920\n",
      "Epoch 15/20\n",
      " - 11s - loss: 1.8529 - acc: 0.2816 - val_loss: 1.8277 - val_acc: 0.2884\n",
      "Epoch 16/20\n",
      " - 11s - loss: 1.8397 - acc: 0.2842 - val_loss: 1.8227 - val_acc: 0.2905\n",
      "Epoch 17/20\n",
      " - 11s - loss: 1.8369 - acc: 0.2864 - val_loss: 1.8356 - val_acc: 0.2839\n",
      "Epoch 18/20\n",
      " - 11s - loss: 1.8418 - acc: 0.2858 - val_loss: 1.8182 - val_acc: 0.2931\n",
      "Epoch 19/20\n",
      " - 11s - loss: 1.8324 - acc: 0.2873 - val_loss: 1.8241 - val_acc: 0.2872\n",
      "Epoch 20/20\n",
      " - 11s - loss: 1.8355 - acc: 0.2831 - val_loss: 1.8218 - val_acc: 0.2878\n",
      "10000/10000 [==============================] - 1s 112us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment16:\tlayers and dropout_rates are: [[64, 64], 0.4]\n",
      "=======================================================\n",
      "Time Used: 559.9435679999997\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.821775326538086\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.2878\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 17s - loss: 2.2026 - acc: 0.1631 - val_loss: 2.0559 - val_acc: 0.1921\n",
      "Epoch 2/20\n",
      " - 12s - loss: 2.0776 - acc: 0.1903 - val_loss: 1.9999 - val_acc: 0.2023\n",
      "Epoch 3/20\n",
      " - 12s - loss: 2.0383 - acc: 0.1962 - val_loss: 1.9617 - val_acc: 0.2206\n",
      "Epoch 4/20\n",
      " - 12s - loss: 2.0187 - acc: 0.2013 - val_loss: 1.9562 - val_acc: 0.2173\n",
      "Epoch 5/20\n",
      " - 12s - loss: 2.0061 - acc: 0.2020 - val_loss: 1.9615 - val_acc: 0.2057\n",
      "Epoch 6/20\n",
      " - 12s - loss: 2.0075 - acc: 0.2015 - val_loss: 1.9528 - val_acc: 0.2227\n",
      "Epoch 7/20\n",
      " - 12s - loss: 1.9958 - acc: 0.2043 - val_loss: 1.9462 - val_acc: 0.2184\n",
      "Epoch 8/20\n",
      " - 12s - loss: 1.9974 - acc: 0.2043 - val_loss: 1.9632 - val_acc: 0.2203\n",
      "Epoch 9/20\n",
      " - 11s - loss: 1.9845 - acc: 0.2119 - val_loss: 1.9327 - val_acc: 0.2308\n",
      "Epoch 10/20\n",
      " - 11s - loss: 1.9703 - acc: 0.2157 - val_loss: 1.9255 - val_acc: 0.2370\n",
      "Epoch 11/20\n",
      " - 11s - loss: 1.9686 - acc: 0.2134 - val_loss: 1.9178 - val_acc: 0.2292\n",
      "Epoch 12/20\n",
      " - 11s - loss: 1.9595 - acc: 0.2170 - val_loss: 1.9195 - val_acc: 0.2310\n",
      "Epoch 13/20\n",
      " - 11s - loss: 1.9747 - acc: 0.2103 - val_loss: 1.9204 - val_acc: 0.2218\n",
      "Epoch 14/20\n",
      " - 11s - loss: 1.9583 - acc: 0.2170 - val_loss: 1.9186 - val_acc: 0.2268\n",
      "Epoch 15/20\n",
      " - 11s - loss: 1.9728 - acc: 0.2140 - val_loss: 1.9304 - val_acc: 0.2312\n",
      "Epoch 16/20\n",
      " - 11s - loss: 1.9598 - acc: 0.2197 - val_loss: 1.9131 - val_acc: 0.2388\n",
      "Epoch 17/20\n",
      " - 11s - loss: 1.9506 - acc: 0.2238 - val_loss: 1.9150 - val_acc: 0.2321\n",
      "Epoch 18/20\n",
      " - 12s - loss: 1.9464 - acc: 0.2282 - val_loss: 1.9120 - val_acc: 0.2310\n",
      "Epoch 19/20\n",
      " - 11s - loss: 1.9410 - acc: 0.2255 - val_loss: 1.9090 - val_acc: 0.2387\n",
      "Epoch 20/20\n",
      " - 11s - loss: 1.9402 - acc: 0.2314 - val_loss: 1.9146 - val_acc: 0.2262\n",
      "10000/10000 [==============================] - 1s 123us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment17:\tlayers and dropout_rates are: [[64, 64], 0.6]\n",
      "=======================================================\n",
      "Time Used: 582.0858779999999\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.9145619089126586\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.2262\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 18s - loss: 2.1559 - acc: 0.1651 - val_loss: 2.0474 - val_acc: 0.1849\n",
      "Epoch 2/20\n",
      " - 12s - loss: 2.0681 - acc: 0.1749 - val_loss: 2.0409 - val_acc: 0.1864\n",
      "Epoch 3/20\n",
      " - 12s - loss: 2.0614 - acc: 0.1782 - val_loss: 2.0163 - val_acc: 0.1917\n",
      "Epoch 4/20\n",
      " - 12s - loss: 2.0267 - acc: 0.1903 - val_loss: 2.0121 - val_acc: 0.1960\n",
      "Epoch 5/20\n",
      " - 12s - loss: 2.0304 - acc: 0.1857 - val_loss: 2.0104 - val_acc: 0.1906\n",
      "Epoch 6/20\n",
      " - 12s - loss: 2.0119 - acc: 0.1838 - val_loss: 1.9985 - val_acc: 0.1920\n",
      "Epoch 7/20\n",
      " - 12s - loss: 2.0143 - acc: 0.1814 - val_loss: 2.0061 - val_acc: 0.1897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20\n",
      " - 11s - loss: 2.0087 - acc: 0.1886 - val_loss: 1.9981 - val_acc: 0.1966\n",
      "Epoch 9/20\n",
      " - 11s - loss: 2.0076 - acc: 0.1891 - val_loss: 1.9873 - val_acc: 0.1938\n",
      "Epoch 10/20\n",
      " - 12s - loss: 1.9956 - acc: 0.1908 - val_loss: 1.9878 - val_acc: 0.1997\n",
      "Epoch 11/20\n",
      " - 12s - loss: 1.9905 - acc: 0.1955 - val_loss: 1.9971 - val_acc: 0.1983\n",
      "Epoch 12/20\n",
      " - 12s - loss: 2.0091 - acc: 0.1944 - val_loss: 2.0154 - val_acc: 0.2087\n",
      "Epoch 13/20\n",
      " - 12s - loss: 2.0240 - acc: 0.1899 - val_loss: 1.9875 - val_acc: 0.2003\n",
      "Epoch 14/20\n",
      " - 12s - loss: 2.0087 - acc: 0.1967 - val_loss: 1.9848 - val_acc: 0.1975\n",
      "Epoch 15/20\n",
      " - 12s - loss: 1.9950 - acc: 0.2032 - val_loss: 1.9831 - val_acc: 0.2029\n",
      "Epoch 16/20\n",
      " - 12s - loss: 1.9867 - acc: 0.2085 - val_loss: 1.9779 - val_acc: 0.2064\n",
      "Epoch 17/20\n",
      " - 13s - loss: 1.9820 - acc: 0.2106 - val_loss: 1.9725 - val_acc: 0.2095\n",
      "Epoch 18/20\n",
      " - 12s - loss: 1.9881 - acc: 0.2087 - val_loss: 1.9589 - val_acc: 0.2137\n",
      "Epoch 19/20\n",
      " - 11s - loss: 1.9676 - acc: 0.2108 - val_loss: 1.9589 - val_acc: 0.2072\n",
      "Epoch 20/20\n",
      " - 12s - loss: 1.9689 - acc: 0.2082 - val_loss: 1.9634 - val_acc: 0.2056\n",
      "10000/10000 [==============================] - 1s 132us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment18:\tlayers and dropout_rates are: [[128, 128], 0.2]\n",
      "=======================================================\n",
      "Time Used: 603.9077470000002\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.9633658824920655\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.2056\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 20s - loss: 2.2072 - acc: 0.1495 - val_loss: 2.1659 - val_acc: 0.1618\n",
      "Epoch 2/20\n",
      " - 13s - loss: 2.2370 - acc: 0.1333 - val_loss: 2.2838 - val_acc: 0.1124\n",
      "Epoch 3/20\n",
      " - 13s - loss: 2.2811 - acc: 0.1128 - val_loss: 2.2887 - val_acc: 0.1099\n",
      "Epoch 4/20\n",
      " - 13s - loss: 2.2297 - acc: 0.1400 - val_loss: 2.1194 - val_acc: 0.1690\n",
      "Epoch 5/20\n",
      " - 13s - loss: 2.1831 - acc: 0.1497 - val_loss: 2.0831 - val_acc: 0.1743\n",
      "Epoch 6/20\n",
      " - 13s - loss: 2.1278 - acc: 0.1659 - val_loss: 2.0636 - val_acc: 0.1800\n",
      "Epoch 7/20\n",
      " - 13s - loss: 2.1148 - acc: 0.1661 - val_loss: 2.1499 - val_acc: 0.1567\n",
      "Epoch 8/20\n",
      " - 13s - loss: 2.0997 - acc: 0.1648 - val_loss: 2.0626 - val_acc: 0.1725\n",
      "Epoch 9/20\n",
      " - 13s - loss: 2.0989 - acc: 0.1678 - val_loss: 2.0381 - val_acc: 0.1862\n",
      "Epoch 10/20\n",
      " - 13s - loss: 2.0578 - acc: 0.1744 - val_loss: 2.0181 - val_acc: 0.1847\n",
      "Epoch 11/20\n",
      " - 13s - loss: 2.0511 - acc: 0.1745 - val_loss: 2.0796 - val_acc: 0.1712\n",
      "Epoch 12/20\n",
      " - 13s - loss: 2.0663 - acc: 0.1731 - val_loss: 2.0013 - val_acc: 0.1912\n",
      "Epoch 13/20\n",
      " - 13s - loss: 2.0318 - acc: 0.1779 - val_loss: 1.9937 - val_acc: 0.1901\n",
      "Epoch 14/20\n",
      " - 13s - loss: 2.0234 - acc: 0.1833 - val_loss: 1.9877 - val_acc: 0.1942\n",
      "Epoch 15/20\n",
      " - 13s - loss: 2.0162 - acc: 0.1855 - val_loss: 1.9828 - val_acc: 0.1984\n",
      "Epoch 16/20\n",
      " - 13s - loss: 2.0159 - acc: 0.1841 - val_loss: 1.9787 - val_acc: 0.1984\n",
      "Epoch 17/20\n",
      " - 13s - loss: 2.0044 - acc: 0.1875 - val_loss: 2.0272 - val_acc: 0.1895\n",
      "Epoch 18/20\n",
      " - 12s - loss: 2.0389 - acc: 0.1830 - val_loss: 1.9968 - val_acc: 0.1922\n",
      "Epoch 19/20\n",
      " - 12s - loss: 2.0230 - acc: 0.1848 - val_loss: 1.9720 - val_acc: 0.2007\n",
      "Epoch 20/20\n",
      " - 12s - loss: 1.9915 - acc: 0.1897 - val_loss: 1.9810 - val_acc: 0.1980\n",
      "10000/10000 [==============================] - 1s 142us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment19:\tlayers and dropout_rates are: [[128, 128], 0.4]\n",
      "=======================================================\n",
      "Time Used: 662.9428699999999\n",
      "-------------------------------------------------------\n",
      "Test loss: 1.9810445934295655\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.198\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 21s - loss: 2.2014 - acc: 0.1550 - val_loss: 2.0701 - val_acc: 0.1780\n",
      "Epoch 2/20\n",
      " - 13s - loss: 2.0838 - acc: 0.1715 - val_loss: 2.0133 - val_acc: 0.1836\n",
      "Epoch 3/20\n",
      " - 14s - loss: 2.0797 - acc: 0.1720 - val_loss: 2.0276 - val_acc: 0.1829\n",
      "Epoch 4/20\n",
      " - 13s - loss: 2.1254 - acc: 0.1621 - val_loss: 2.1024 - val_acc: 0.1701\n",
      "Epoch 5/20\n",
      " - 13s - loss: 2.0831 - acc: 0.1688 - val_loss: 2.0329 - val_acc: 0.1812\n",
      "Epoch 6/20\n",
      " - 13s - loss: 2.0487 - acc: 0.1740 - val_loss: 2.0044 - val_acc: 0.1962\n",
      "Epoch 7/20\n",
      " - 13s - loss: 2.0641 - acc: 0.1734 - val_loss: 2.1232 - val_acc: 0.1706\n",
      "Epoch 8/20\n",
      " - 13s - loss: 2.0856 - acc: 0.1690 - val_loss: 2.0207 - val_acc: 0.1831\n",
      "Epoch 9/20\n",
      " - 13s - loss: 2.0364 - acc: 0.1770 - val_loss: 2.0146 - val_acc: 0.1862\n",
      "Epoch 10/20\n",
      " - 13s - loss: 2.0634 - acc: 0.1763 - val_loss: 2.0782 - val_acc: 0.1645\n",
      "Epoch 11/20\n",
      " - 13s - loss: 2.0629 - acc: 0.1728 - val_loss: 2.0724 - val_acc: 0.1748\n",
      "Epoch 12/20\n",
      " - 13s - loss: 2.0846 - acc: 0.1701 - val_loss: 2.0564 - val_acc: 0.1760\n",
      "Epoch 13/20\n",
      " - 13s - loss: 2.1578 - acc: 0.1583 - val_loss: 2.1096 - val_acc: 0.1727\n",
      "Epoch 14/20\n",
      " - 12s - loss: 2.1230 - acc: 0.1615 - val_loss: 2.0993 - val_acc: 0.1671\n",
      "Epoch 15/20\n",
      " - 13s - loss: 2.1039 - acc: 0.1636 - val_loss: 2.0540 - val_acc: 0.1755\n",
      "Epoch 16/20\n",
      " - 13s - loss: 2.0847 - acc: 0.1726 - val_loss: 2.0438 - val_acc: 0.1788\n",
      "Epoch 17/20\n",
      " - 13s - loss: 2.0756 - acc: 0.1688 - val_loss: 2.0417 - val_acc: 0.1828\n",
      "Epoch 18/20\n",
      " - 13s - loss: 2.0647 - acc: 0.1729 - val_loss: 2.0348 - val_acc: 0.1805\n",
      "Epoch 19/20\n",
      " - 13s - loss: 2.0548 - acc: 0.1785 - val_loss: 2.0382 - val_acc: 0.1799\n",
      "Epoch 20/20\n",
      " - 13s - loss: 2.0561 - acc: 0.1779 - val_loss: 2.0148 - val_acc: 0.1902\n",
      "10000/10000 [==============================] - 2s 154us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment20:\tlayers and dropout_rates are: [[128, 128], 0.6]\n",
      "=======================================================\n",
      "Time Used: 671.9614959999999\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.0147647804260256\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1902\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 25s - loss: 2.1140 - acc: 0.1652 - val_loss: 2.1318 - val_acc: 0.1665\n",
      "Epoch 2/20\n",
      " - 16s - loss: 2.1243 - acc: 0.1618 - val_loss: 2.2290 - val_acc: 0.1320\n",
      "Epoch 3/20\n",
      " - 16s - loss: 2.2096 - acc: 0.1375 - val_loss: 2.1609 - val_acc: 0.1481\n",
      "Epoch 4/20\n",
      " - 16s - loss: 2.2095 - acc: 0.1495 - val_loss: 2.1424 - val_acc: 0.1687\n",
      "Epoch 5/20\n",
      " - 16s - loss: 2.1767 - acc: 0.1556 - val_loss: 2.1166 - val_acc: 0.1641\n",
      "Epoch 6/20\n",
      " - 16s - loss: 2.1777 - acc: 0.1586 - val_loss: 2.1112 - val_acc: 0.1733\n",
      "Epoch 7/20\n",
      " - 15s - loss: 2.1722 - acc: 0.1633 - val_loss: 2.1110 - val_acc: 0.1727\n",
      "Epoch 8/20\n",
      " - 16s - loss: 2.1471 - acc: 0.1612 - val_loss: 2.0918 - val_acc: 0.1744\n",
      "Epoch 9/20\n",
      " - 16s - loss: 2.1751 - acc: 0.1573 - val_loss: 2.1753 - val_acc: 0.1560\n",
      "Epoch 10/20\n",
      " - 16s - loss: 2.1902 - acc: 0.1522 - val_loss: 2.1162 - val_acc: 0.1710\n",
      "Epoch 11/20\n",
      " - 16s - loss: 2.1366 - acc: 0.1646 - val_loss: 2.0978 - val_acc: 0.1664\n",
      "Epoch 12/20\n",
      " - 16s - loss: 2.1163 - acc: 0.1641 - val_loss: 2.0755 - val_acc: 0.1675\n",
      "Epoch 13/20\n",
      " - 16s - loss: 2.1298 - acc: 0.1704 - val_loss: 2.1005 - val_acc: 0.1767\n",
      "Epoch 14/20\n",
      " - 16s - loss: 2.1221 - acc: 0.1676 - val_loss: 2.0947 - val_acc: 0.1782\n",
      "Epoch 15/20\n",
      " - 15s - loss: 2.2067 - acc: 0.1455 - val_loss: 2.2455 - val_acc: 0.1336\n",
      "Epoch 16/20\n",
      " - 16s - loss: 2.2417 - acc: 0.1330 - val_loss: 2.2361 - val_acc: 0.1366\n",
      "Epoch 17/20\n",
      " - 16s - loss: 2.2309 - acc: 0.1373 - val_loss: 2.1867 - val_acc: 0.1526\n",
      "Epoch 18/20\n",
      " - 16s - loss: 2.1436 - acc: 0.1626 - val_loss: 2.1123 - val_acc: 0.1746\n",
      "Epoch 19/20\n",
      " - 16s - loss: 2.1453 - acc: 0.1652 - val_loss: 2.1409 - val_acc: 0.1686\n",
      "Epoch 20/20\n",
      " - 16s - loss: 2.1445 - acc: 0.1665 - val_loss: 2.0951 - val_acc: 0.1755\n",
      "10000/10000 [==============================] - 2s 183us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment21:\tlayers and dropout_rates are: [[512, 512], 0.2]\n",
      "=======================================================\n",
      "Time Used: 874.9987600000004\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.095114179611206\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1755\n",
      "*******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 30s - loss: 2.1947 - acc: 0.1499 - val_loss: 2.2316 - val_acc: 0.1517\n",
      "Epoch 2/20\n",
      " - 19s - loss: 2.2274 - acc: 0.1403 - val_loss: 2.2525 - val_acc: 0.1296\n",
      "Epoch 3/20\n",
      " - 19s - loss: 2.2057 - acc: 0.1456 - val_loss: 2.1852 - val_acc: 0.1548\n",
      "Epoch 4/20\n",
      " - 19s - loss: 2.1735 - acc: 0.1532 - val_loss: 2.1789 - val_acc: 0.1409\n",
      "Epoch 5/20\n",
      " - 19s - loss: 2.1625 - acc: 0.1476 - val_loss: 2.1502 - val_acc: 0.1506\n",
      "Epoch 6/20\n",
      " - 19s - loss: 2.1498 - acc: 0.1499 - val_loss: 2.0944 - val_acc: 0.1678\n",
      "Epoch 7/20\n",
      " - 20s - loss: 2.1386 - acc: 0.1567 - val_loss: 2.1237 - val_acc: 0.1577\n",
      "Epoch 8/20\n",
      " - 19s - loss: 2.1125 - acc: 0.1633 - val_loss: 2.0732 - val_acc: 0.1751\n",
      "Epoch 9/20\n",
      " - 19s - loss: 2.1473 - acc: 0.1595 - val_loss: 2.2444 - val_acc: 0.1372\n",
      "Epoch 10/20\n",
      " - 19s - loss: 2.2308 - acc: 0.1416 - val_loss: 2.1619 - val_acc: 0.1670\n",
      "Epoch 11/20\n",
      " - 19s - loss: 2.2001 - acc: 0.1530 - val_loss: 2.1050 - val_acc: 0.1846\n",
      "Epoch 12/20\n",
      " - 19s - loss: 2.1985 - acc: 0.1515 - val_loss: 2.1085 - val_acc: 0.1763\n",
      "Epoch 13/20\n",
      " - 19s - loss: 2.1958 - acc: 0.1519 - val_loss: 2.2627 - val_acc: 0.1209\n",
      "Epoch 14/20\n",
      " - 19s - loss: 2.2527 - acc: 0.1281 - val_loss: 2.2519 - val_acc: 0.1258\n",
      "Epoch 15/20\n",
      " - 19s - loss: 2.2495 - acc: 0.1297 - val_loss: 2.2505 - val_acc: 0.1288\n",
      "Epoch 16/20\n",
      " - 19s - loss: 2.2464 - acc: 0.1304 - val_loss: 2.2484 - val_acc: 0.1292\n",
      "Epoch 17/20\n",
      " - 19s - loss: 2.2473 - acc: 0.1304 - val_loss: 2.2446 - val_acc: 0.1353\n",
      "Epoch 18/20\n",
      " - 19s - loss: 2.2429 - acc: 0.1329 - val_loss: 2.2466 - val_acc: 0.1305\n",
      "Epoch 19/20\n",
      " - 19s - loss: 2.2405 - acc: 0.1353 - val_loss: 2.2430 - val_acc: 0.1355\n",
      "Epoch 20/20\n",
      " - 19s - loss: 2.2387 - acc: 0.1358 - val_loss: 2.2355 - val_acc: 0.1381\n",
      "10000/10000 [==============================] - 2s 205us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment22:\tlayers and dropout_rates are: [[512, 512], 0.4]\n",
      "=======================================================\n",
      "Time Used: 1079.1153700000013\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.2354582698822023\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1381\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 33s - loss: 2.2613 - acc: 0.1244 - val_loss: 2.2906 - val_acc: 0.1072\n",
      "Epoch 2/20\n",
      " - 23s - loss: 2.2706 - acc: 0.1181 - val_loss: 2.2012 - val_acc: 0.1469\n",
      "Epoch 3/20\n",
      " - 22s - loss: 2.1802 - acc: 0.1514 - val_loss: 2.1359 - val_acc: 0.1614\n",
      "Epoch 4/20\n",
      " - 21s - loss: 2.2269 - acc: 0.1271 - val_loss: 2.2545 - val_acc: 0.1184\n",
      "Epoch 5/20\n",
      " - 22s - loss: 2.2500 - acc: 0.1231 - val_loss: 2.2514 - val_acc: 0.1189\n",
      "Epoch 6/20\n",
      " - 22s - loss: 2.2366 - acc: 0.1273 - val_loss: 2.2033 - val_acc: 0.1363\n",
      "Epoch 7/20\n",
      " - 22s - loss: 2.2044 - acc: 0.1400 - val_loss: 2.1906 - val_acc: 0.1422\n",
      "Epoch 8/20\n",
      " - 22s - loss: 2.1990 - acc: 0.1426 - val_loss: 2.1880 - val_acc: 0.1435\n",
      "Epoch 9/20\n",
      " - 22s - loss: 2.1722 - acc: 0.1521 - val_loss: 2.2102 - val_acc: 0.1357\n",
      "Epoch 10/20\n",
      " - 22s - loss: 2.1831 - acc: 0.1526 - val_loss: 2.1117 - val_acc: 0.1692\n",
      "Epoch 11/20\n",
      " - 22s - loss: 2.1707 - acc: 0.1592 - val_loss: 2.1194 - val_acc: 0.1672\n",
      "Epoch 12/20\n",
      " - 22s - loss: 2.1715 - acc: 0.1568 - val_loss: 2.1125 - val_acc: 0.1713\n",
      "Epoch 13/20\n",
      " - 21s - loss: 2.1613 - acc: 0.1591 - val_loss: 2.1203 - val_acc: 0.1681\n",
      "Epoch 14/20\n",
      " - 22s - loss: 2.2553 - acc: 0.1201 - val_loss: 2.2806 - val_acc: 0.1082\n",
      "Epoch 15/20\n",
      " - 22s - loss: 2.2677 - acc: 0.1155 - val_loss: 2.2847 - val_acc: 0.1067\n",
      "Epoch 16/20\n",
      " - 22s - loss: 2.2718 - acc: 0.1135 - val_loss: 2.2843 - val_acc: 0.1066\n",
      "Epoch 17/20\n",
      " - 22s - loss: 2.2720 - acc: 0.1127 - val_loss: 2.2850 - val_acc: 0.1064\n",
      "Epoch 18/20\n",
      " - 22s - loss: 2.2718 - acc: 0.1119 - val_loss: 2.2848 - val_acc: 0.1067\n",
      "Epoch 19/20\n",
      " - 22s - loss: 2.2649 - acc: 0.1140 - val_loss: 2.2571 - val_acc: 0.1170\n",
      "Epoch 20/20\n",
      " - 22s - loss: 2.2484 - acc: 0.1217 - val_loss: 2.2577 - val_acc: 0.1159\n",
      "10000/10000 [==============================] - 2s 231us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment23:\tlayers and dropout_rates are: [[512, 512], 0.6]\n",
      "=======================================================\n",
      "Time Used: 1273.2023480000007\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.2576748638153075\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1159\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 45s - loss: 2.1951 - acc: 0.1506 - val_loss: 2.1615 - val_acc: 0.1586\n",
      "Epoch 2/20\n",
      " - 33s - loss: 2.1979 - acc: 0.1486 - val_loss: 2.2223 - val_acc: 0.1373\n",
      "Epoch 3/20\n",
      " - 32s - loss: 2.2335 - acc: 0.1363 - val_loss: 2.2280 - val_acc: 0.1388\n",
      "Epoch 4/20\n",
      " - 33s - loss: 2.2021 - acc: 0.1454 - val_loss: 2.1375 - val_acc: 0.1604\n",
      "Epoch 5/20\n",
      " - 33s - loss: 2.1685 - acc: 0.1588 - val_loss: 2.1363 - val_acc: 0.1640\n",
      "Epoch 6/20\n",
      " - 33s - loss: 2.1740 - acc: 0.1563 - val_loss: 2.1335 - val_acc: 0.1638\n",
      "Epoch 7/20\n",
      " - 32s - loss: 2.1762 - acc: 0.1550 - val_loss: 2.1305 - val_acc: 0.1637\n",
      "Epoch 8/20\n",
      " - 33s - loss: 2.1946 - acc: 0.1532 - val_loss: 2.2306 - val_acc: 0.1301\n",
      "Epoch 9/20\n",
      " - 33s - loss: 2.2351 - acc: 0.1328 - val_loss: 2.2197 - val_acc: 0.1348\n",
      "Epoch 10/20\n",
      " - 33s - loss: 2.2356 - acc: 0.1319 - val_loss: 2.2548 - val_acc: 0.1225\n",
      "Epoch 11/20\n",
      " - 33s - loss: 2.2487 - acc: 0.1267 - val_loss: 2.2574 - val_acc: 0.1177\n",
      "Epoch 12/20\n",
      " - 33s - loss: 2.2467 - acc: 0.1252 - val_loss: 2.2496 - val_acc: 0.1241\n",
      "Epoch 13/20\n",
      " - 33s - loss: 2.2164 - acc: 0.1356 - val_loss: 2.1916 - val_acc: 0.1433\n",
      "Epoch 14/20\n",
      " - 33s - loss: 2.2040 - acc: 0.1437 - val_loss: 2.2301 - val_acc: 0.1284\n",
      "Epoch 15/20\n",
      " - 33s - loss: 2.2421 - acc: 0.1247 - val_loss: 2.2659 - val_acc: 0.1167\n",
      "Epoch 16/20\n",
      " - 33s - loss: 2.2450 - acc: 0.1229 - val_loss: 2.2550 - val_acc: 0.1211\n",
      "Epoch 17/20\n",
      " - 33s - loss: 2.2218 - acc: 0.1351 - val_loss: 2.1986 - val_acc: 0.1401\n",
      "Epoch 18/20\n",
      " - 33s - loss: 2.2137 - acc: 0.1323 - val_loss: 2.2263 - val_acc: 0.1311\n",
      "Epoch 19/20\n",
      " - 33s - loss: 2.2075 - acc: 0.1351 - val_loss: 2.1960 - val_acc: 0.1389\n",
      "Epoch 20/20\n",
      " - 33s - loss: 2.2193 - acc: 0.1309 - val_loss: 2.1647 - val_acc: 0.1505\n",
      "10000/10000 [==============================] - 3s 294us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment24:\tlayers and dropout_rates are: [[1024, 1024], 0.2]\n",
      "=======================================================\n",
      "Time Used: 2037.860134999999\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.1647074211120607\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1505\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 57s - loss: 2.1959 - acc: 0.1443 - val_loss: 2.1815 - val_acc: 0.1436\n",
      "Epoch 2/20\n",
      " - 44s - loss: 2.1893 - acc: 0.1445 - val_loss: 2.1950 - val_acc: 0.1440\n",
      "Epoch 3/20\n",
      " - 43s - loss: 2.2422 - acc: 0.1245 - val_loss: 2.2962 - val_acc: 0.1030\n",
      "Epoch 4/20\n",
      " - 43s - loss: 2.2807 - acc: 0.1091 - val_loss: 2.2830 - val_acc: 0.1093\n",
      "Epoch 5/20\n",
      " - 44s - loss: 2.2700 - acc: 0.1170 - val_loss: 2.2831 - val_acc: 0.1090\n",
      "Epoch 6/20\n",
      " - 42s - loss: 2.2552 - acc: 0.1242 - val_loss: 2.2618 - val_acc: 0.1194\n",
      "Epoch 7/20\n",
      " - 43s - loss: 2.2350 - acc: 0.1331 - val_loss: 2.1227 - val_acc: 0.1730\n",
      "Epoch 8/20\n",
      " - 43s - loss: 2.1724 - acc: 0.1581 - val_loss: 2.1134 - val_acc: 0.1724\n",
      "Epoch 9/20\n",
      " - 44s - loss: 2.1660 - acc: 0.1540 - val_loss: 2.1370 - val_acc: 0.1600\n",
      "Epoch 10/20\n",
      " - 42s - loss: 2.1572 - acc: 0.1569 - val_loss: 2.1255 - val_acc: 0.1648\n",
      "Epoch 11/20\n",
      " - 42s - loss: 2.1790 - acc: 0.1477 - val_loss: 2.1563 - val_acc: 0.1567\n",
      "Epoch 12/20\n",
      " - 43s - loss: 2.1840 - acc: 0.1473 - val_loss: 2.1538 - val_acc: 0.1572\n",
      "Epoch 13/20\n",
      " - 42s - loss: 2.2038 - acc: 0.1381 - val_loss: 2.1669 - val_acc: 0.1528\n",
      "Epoch 14/20\n",
      " - 43s - loss: 2.1982 - acc: 0.1386 - val_loss: 2.1824 - val_acc: 0.1458\n",
      "Epoch 15/20\n",
      " - 43s - loss: 2.1734 - acc: 0.1516 - val_loss: 2.1093 - val_acc: 0.1672\n",
      "Epoch 16/20\n",
      " - 44s - loss: 2.1951 - acc: 0.1400 - val_loss: 2.2174 - val_acc: 0.1337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20\n",
      " - 43s - loss: 2.1983 - acc: 0.1414 - val_loss: 2.1263 - val_acc: 0.1651\n",
      "Epoch 18/20\n",
      " - 43s - loss: 2.2011 - acc: 0.1404 - val_loss: 2.1940 - val_acc: 0.1445\n",
      "Epoch 19/20\n",
      " - 43s - loss: 2.2007 - acc: 0.1418 - val_loss: 2.1879 - val_acc: 0.1446\n",
      "Epoch 20/20\n",
      " - 43s - loss: 2.2113 - acc: 0.1364 - val_loss: 2.2854 - val_acc: 0.1086\n",
      "10000/10000 [==============================] - 4s 379us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment25:\tlayers and dropout_rates are: [[1024, 1024], 0.4]\n",
      "=======================================================\n",
      "Time Used: 2719.049793\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.285413084411621\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1086\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 68s - loss: 2.2019 - acc: 0.1490 - val_loss: 2.1321 - val_acc: 0.1653\n",
      "Epoch 2/20\n",
      " - 53s - loss: 2.1639 - acc: 0.1586 - val_loss: 2.1143 - val_acc: 0.1727\n",
      "Epoch 3/20\n",
      " - 53s - loss: 2.1969 - acc: 0.1477 - val_loss: 2.2833 - val_acc: 0.1173\n",
      "Epoch 4/20\n",
      " - 53s - loss: 2.2722 - acc: 0.1187 - val_loss: 2.2754 - val_acc: 0.1188\n",
      "Epoch 5/20\n",
      " - 54s - loss: 2.2491 - acc: 0.1323 - val_loss: 2.2447 - val_acc: 0.1347\n",
      "Epoch 6/20\n",
      " - 53s - loss: 2.2243 - acc: 0.1434 - val_loss: 2.1850 - val_acc: 0.1676\n",
      "Epoch 7/20\n",
      " - 53s - loss: 2.1928 - acc: 0.1573 - val_loss: 2.1726 - val_acc: 0.1670\n",
      "Epoch 8/20\n",
      " - 54s - loss: 2.1973 - acc: 0.1553 - val_loss: 2.1726 - val_acc: 0.1654\n",
      "Epoch 9/20\n",
      " - 53s - loss: 2.1904 - acc: 0.1584 - val_loss: 2.1679 - val_acc: 0.1675\n",
      "Epoch 10/20\n",
      " - 54s - loss: 2.1886 - acc: 0.1597 - val_loss: 2.1671 - val_acc: 0.1678\n",
      "Epoch 11/20\n",
      " - 53s - loss: 2.1857 - acc: 0.1602 - val_loss: 2.1648 - val_acc: 0.1675\n",
      "Epoch 12/20\n",
      " - 53s - loss: 2.1831 - acc: 0.1593 - val_loss: 2.1593 - val_acc: 0.1687\n",
      "Epoch 13/20\n",
      " - 53s - loss: 2.1794 - acc: 0.1651 - val_loss: 2.1582 - val_acc: 0.1702\n",
      "Epoch 14/20\n",
      " - 54s - loss: 2.1797 - acc: 0.1634 - val_loss: 2.1556 - val_acc: 0.1713\n",
      "Epoch 15/20\n",
      " - 53s - loss: 2.1728 - acc: 0.1613 - val_loss: 2.1493 - val_acc: 0.1715\n",
      "Epoch 16/20\n",
      " - 54s - loss: 2.1673 - acc: 0.1646 - val_loss: 2.1100 - val_acc: 0.1764\n",
      "Epoch 17/20\n",
      " - 52s - loss: 2.1542 - acc: 0.1640 - val_loss: 2.1100 - val_acc: 0.1763\n",
      "Epoch 18/20\n",
      " - 52s - loss: 2.1606 - acc: 0.1598 - val_loss: 2.1076 - val_acc: 0.1737\n",
      "Epoch 19/20\n",
      " - 52s - loss: 2.1465 - acc: 0.1637 - val_loss: 2.1056 - val_acc: 0.1804\n",
      "Epoch 20/20\n",
      " - 52s - loss: 2.1443 - acc: 0.1654 - val_loss: 2.1019 - val_acc: 0.1750\n",
      "10000/10000 [==============================] - 4s 407us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment26:\tlayers and dropout_rates are: [[1024, 1024], 0.6]\n",
      "=======================================================\n",
      "Time Used: 3401.1686550000013\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.1018564804077147\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.175\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 69s - loss: 2.2274 - acc: 0.1365 - val_loss: 2.1911 - val_acc: 0.1377\n",
      "Epoch 2/20\n",
      " - 53s - loss: 2.2118 - acc: 0.1384 - val_loss: 2.1885 - val_acc: 0.1388\n",
      "Epoch 3/20\n",
      " - 54s - loss: 2.2133 - acc: 0.1361 - val_loss: 2.1922 - val_acc: 0.1371\n",
      "Epoch 4/20\n",
      " - 54s - loss: 2.1914 - acc: 0.1442 - val_loss: 2.1346 - val_acc: 0.1594\n",
      "Epoch 5/20\n",
      " - 54s - loss: 2.1961 - acc: 0.1523 - val_loss: 2.1607 - val_acc: 0.1673\n",
      "Epoch 6/20\n",
      " - 53s - loss: 2.1746 - acc: 0.1560 - val_loss: 2.1342 - val_acc: 0.1641\n",
      "Epoch 7/20\n",
      " - 54s - loss: 2.2020 - acc: 0.1465 - val_loss: 2.1594 - val_acc: 0.1528\n",
      "Epoch 8/20\n",
      " - 52s - loss: 2.1702 - acc: 0.1523 - val_loss: 2.1224 - val_acc: 0.1659\n",
      "Epoch 9/20\n",
      " - 53s - loss: 2.2172 - acc: 0.1332 - val_loss: 2.2232 - val_acc: 0.1296\n",
      "Epoch 10/20\n",
      " - 54s - loss: 2.2238 - acc: 0.1306 - val_loss: 2.2624 - val_acc: 0.1156\n",
      "Epoch 11/20\n",
      " - 54s - loss: 2.2616 - acc: 0.1150 - val_loss: 2.2612 - val_acc: 0.1152\n",
      "Epoch 12/20\n",
      " - 53s - loss: 2.2521 - acc: 0.1203 - val_loss: 2.2490 - val_acc: 0.1207\n",
      "Epoch 13/20\n",
      " - 53s - loss: 2.2468 - acc: 0.1208 - val_loss: 2.2511 - val_acc: 0.1197\n",
      "Epoch 14/20\n",
      " - 53s - loss: 2.2453 - acc: 0.1231 - val_loss: 2.2364 - val_acc: 0.1257\n",
      "Epoch 15/20\n",
      " - 53s - loss: 2.2327 - acc: 0.1257 - val_loss: 2.1900 - val_acc: 0.1451\n",
      "Epoch 16/20\n",
      " - 52s - loss: 2.1700 - acc: 0.1511 - val_loss: 2.1330 - val_acc: 0.1679\n",
      "Epoch 17/20\n",
      " - 53s - loss: 2.1471 - acc: 0.1618 - val_loss: 2.1277 - val_acc: 0.1689\n",
      "Epoch 18/20\n",
      " - 54s - loss: 2.1494 - acc: 0.1614 - val_loss: 2.1527 - val_acc: 0.1604\n",
      "Epoch 19/20\n",
      " - 54s - loss: 2.1605 - acc: 0.1578 - val_loss: 2.1273 - val_acc: 0.1683\n",
      "Epoch 20/20\n",
      " - 53s - loss: 2.1445 - acc: 0.1657 - val_loss: 2.1260 - val_acc: 0.1683\n",
      "10000/10000 [==============================] - 5s 467us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment27:\tlayers and dropout_rates are: [[64, 64, 64], 0.2]\n",
      "=======================================================\n",
      "Time Used: 3400.8607359999987\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.125989817047119\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1683\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 73s - loss: 2.2354 - acc: 0.1427 - val_loss: 2.2078 - val_acc: 0.1445\n",
      "Epoch 2/20\n",
      " - 54s - loss: 2.2254 - acc: 0.1388 - val_loss: 2.2179 - val_acc: 0.1384\n",
      "Epoch 3/20\n",
      " - 54s - loss: 2.2153 - acc: 0.1442 - val_loss: 2.1632 - val_acc: 0.1583\n",
      "Epoch 4/20\n",
      " - 54s - loss: 2.2249 - acc: 0.1369 - val_loss: 2.1993 - val_acc: 0.1457\n",
      "Epoch 5/20\n",
      " - 53s - loss: 2.1974 - acc: 0.1520 - val_loss: 2.1428 - val_acc: 0.1675\n",
      "Epoch 6/20\n",
      " - 54s - loss: 2.1910 - acc: 0.1508 - val_loss: 2.2304 - val_acc: 0.1326\n",
      "Epoch 7/20\n",
      " - 55s - loss: 2.2416 - acc: 0.1275 - val_loss: 2.2239 - val_acc: 0.1317\n",
      "Epoch 8/20\n",
      " - 54s - loss: 2.2215 - acc: 0.1380 - val_loss: 2.1672 - val_acc: 0.1534\n",
      "Epoch 9/20\n",
      " - 54s - loss: 2.1984 - acc: 0.1461 - val_loss: 2.1705 - val_acc: 0.1521\n",
      "Epoch 10/20\n",
      " - 54s - loss: 2.1960 - acc: 0.1480 - val_loss: 2.1712 - val_acc: 0.1514\n",
      "Epoch 11/20\n",
      " - 54s - loss: 2.2006 - acc: 0.1437 - val_loss: 2.1986 - val_acc: 0.1426\n",
      "Epoch 12/20\n",
      " - 54s - loss: 2.2095 - acc: 0.1430 - val_loss: 2.1964 - val_acc: 0.1430\n",
      "Epoch 13/20\n",
      " - 54s - loss: 2.2189 - acc: 0.1364 - val_loss: 2.2171 - val_acc: 0.1341\n",
      "Epoch 14/20\n",
      " - 54s - loss: 2.2258 - acc: 0.1320 - val_loss: 2.2172 - val_acc: 0.1340\n",
      "Epoch 15/20\n",
      " - 54s - loss: 2.2647 - acc: 0.1164 - val_loss: 2.3029 - val_acc: 0.1002\n",
      "Epoch 16/20\n",
      " - 54s - loss: 2.3004 - acc: 0.1026 - val_loss: 2.3015 - val_acc: 0.0998\n",
      "Epoch 17/20\n",
      " - 54s - loss: 2.2992 - acc: 0.0999 - val_loss: 2.3015 - val_acc: 0.0998\n",
      "Epoch 18/20\n",
      " - 54s - loss: 2.2993 - acc: 0.1027 - val_loss: 2.3015 - val_acc: 0.1002\n",
      "Epoch 19/20\n",
      " - 54s - loss: 2.2998 - acc: 0.1032 - val_loss: 2.3014 - val_acc: 0.1002\n",
      "Epoch 20/20\n",
      " - 54s - loss: 2.2994 - acc: 0.0983 - val_loss: 2.3014 - val_acc: 0.0998\n",
      "10000/10000 [==============================] - 5s 464us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment28:\tlayers and dropout_rates are: [[64, 64, 64], 0.4]\n",
      "=======================================================\n",
      "Time Used: 3442.6925520000004\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.301416826248169\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.0998\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 75s - loss: 2.3032 - acc: 0.1010 - val_loss: 2.3025 - val_acc: 0.1005\n",
      "Epoch 2/20\n",
      " - 56s - loss: 2.3009 - acc: 0.1004 - val_loss: 2.2942 - val_acc: 0.1112\n",
      "Epoch 3/20\n",
      " - 55s - loss: 2.2824 - acc: 0.1105 - val_loss: 2.2816 - val_acc: 0.1111\n",
      "Epoch 4/20\n",
      " - 55s - loss: 2.2778 - acc: 0.1131 - val_loss: 2.2793 - val_acc: 0.1111\n",
      "Epoch 5/20\n",
      " - 55s - loss: 2.2762 - acc: 0.1131 - val_loss: 2.2786 - val_acc: 0.1096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20\n",
      " - 55s - loss: 2.2744 - acc: 0.1154 - val_loss: 2.2691 - val_acc: 0.1148\n",
      "Epoch 7/20\n",
      " - 55s - loss: 2.2659 - acc: 0.1198 - val_loss: 2.2678 - val_acc: 0.1149\n",
      "Epoch 8/20\n",
      " - 56s - loss: 2.2668 - acc: 0.1193 - val_loss: 2.2682 - val_acc: 0.1149\n",
      "Epoch 9/20\n",
      " - 54s - loss: 2.2546 - acc: 0.1214 - val_loss: 2.1903 - val_acc: 0.1522\n",
      "Epoch 10/20\n",
      " - 55s - loss: 2.2207 - acc: 0.1530 - val_loss: 2.1909 - val_acc: 0.1697\n",
      "Epoch 11/20\n",
      " - 55s - loss: 2.2049 - acc: 0.1568 - val_loss: 2.1801 - val_acc: 0.1697\n",
      "Epoch 12/20\n",
      " - 55s - loss: 2.1975 - acc: 0.1532 - val_loss: 2.1710 - val_acc: 0.1697\n",
      "Epoch 13/20\n",
      " - 55s - loss: 2.1913 - acc: 0.1565 - val_loss: 2.1678 - val_acc: 0.1698\n",
      "Epoch 14/20\n",
      " - 54s - loss: 2.1859 - acc: 0.1597 - val_loss: 2.1612 - val_acc: 0.1697\n",
      "Epoch 15/20\n",
      " - 55s - loss: 2.1855 - acc: 0.1556 - val_loss: 2.1560 - val_acc: 0.1706\n",
      "Epoch 16/20\n",
      " - 55s - loss: 2.1808 - acc: 0.1578 - val_loss: 2.1570 - val_acc: 0.1706\n",
      "Epoch 17/20\n",
      " - 55s - loss: 2.1818 - acc: 0.1553 - val_loss: 2.1572 - val_acc: 0.1706\n",
      "Epoch 18/20\n",
      " - 54s - loss: 2.1773 - acc: 0.1589 - val_loss: 2.1588 - val_acc: 0.1706\n",
      "Epoch 19/20\n",
      " - 55s - loss: 2.1778 - acc: 0.1583 - val_loss: 2.1580 - val_acc: 0.1706\n",
      "Epoch 20/20\n",
      " - 54s - loss: 2.1785 - acc: 0.1592 - val_loss: 2.1579 - val_acc: 0.1706\n",
      "10000/10000 [==============================] - 5s 499us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment29:\tlayers and dropout_rates are: [[64, 64, 64], 0.6]\n",
      "=======================================================\n",
      "Time Used: 3480.659639999998\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.157907646560669\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1706\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 77s - loss: 2.1772 - acc: 0.1569 - val_loss: 2.1264 - val_acc: 0.1691\n",
      "Epoch 2/20\n",
      " - 55s - loss: 2.1598 - acc: 0.1588 - val_loss: 2.1256 - val_acc: 0.1694\n",
      "Epoch 3/20\n",
      " - 57s - loss: 2.2031 - acc: 0.1411 - val_loss: 2.2040 - val_acc: 0.1412\n",
      "Epoch 4/20\n",
      " - 56s - loss: 2.2094 - acc: 0.1382 - val_loss: 2.2042 - val_acc: 0.1412\n",
      "Epoch 5/20\n",
      " - 56s - loss: 2.2056 - acc: 0.1399 - val_loss: 2.2037 - val_acc: 0.1412\n",
      "Epoch 6/20\n",
      " - 57s - loss: 2.2151 - acc: 0.1371 - val_loss: 2.2164 - val_acc: 0.1349\n",
      "Epoch 7/20\n",
      " - 55s - loss: 2.2177 - acc: 0.1346 - val_loss: 2.2159 - val_acc: 0.1355\n",
      "Epoch 8/20\n",
      " - 56s - loss: 2.2200 - acc: 0.1354 - val_loss: 2.2159 - val_acc: 0.1355\n",
      "Epoch 9/20\n",
      " - 56s - loss: 2.2127 - acc: 0.1365 - val_loss: 2.2112 - val_acc: 0.1380\n",
      "Epoch 10/20\n",
      " - 58s - loss: 2.2114 - acc: 0.1388 - val_loss: 2.2107 - val_acc: 0.1384\n",
      "Epoch 11/20\n",
      " - 68s - loss: 2.2093 - acc: 0.1398 - val_loss: 2.2096 - val_acc: 0.1384\n",
      "Epoch 12/20\n",
      " - 68s - loss: 2.2121 - acc: 0.1363 - val_loss: 2.2098 - val_acc: 0.1387\n",
      "Epoch 13/20\n",
      " - 64s - loss: 2.2116 - acc: 0.1401 - val_loss: 2.2094 - val_acc: 0.1386\n",
      "Epoch 14/20\n",
      " - 58s - loss: 2.2097 - acc: 0.1391 - val_loss: 2.2095 - val_acc: 0.1386\n",
      "Epoch 15/20\n",
      " - 74s - loss: 2.2109 - acc: 0.1368 - val_loss: 2.2100 - val_acc: 0.1386\n",
      "Epoch 16/20\n",
      " - 61s - loss: 2.2097 - acc: 0.1390 - val_loss: 2.2101 - val_acc: 0.1382\n",
      "Epoch 17/20\n",
      " - 60s - loss: 2.2107 - acc: 0.1401 - val_loss: 2.2103 - val_acc: 0.1382\n",
      "Epoch 18/20\n",
      " - 67s - loss: 2.2113 - acc: 0.1354 - val_loss: 2.2103 - val_acc: 0.1385\n",
      "Epoch 19/20\n",
      " - 63s - loss: 2.2115 - acc: 0.1353 - val_loss: 2.2111 - val_acc: 0.1385\n",
      "Epoch 20/20\n",
      " - 62s - loss: 2.2101 - acc: 0.1368 - val_loss: 2.2094 - val_acc: 0.1383\n",
      "10000/10000 [==============================] - 6s 637us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment30:\tlayers and dropout_rates are: [[128, 128, 128], 0.2]\n",
      "=======================================================\n",
      "Time Used: 3531.2878329999985\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.209431689453125\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1383\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 97s - loss: 2.2354 - acc: 0.1299 - val_loss: 2.2121 - val_acc: 0.1386\n",
      "Epoch 2/20\n",
      " - 64s - loss: 2.2192 - acc: 0.1379 - val_loss: 2.2107 - val_acc: 0.1386\n",
      "Epoch 3/20\n",
      " - 65s - loss: 2.2176 - acc: 0.1364 - val_loss: 2.2108 - val_acc: 0.1386\n",
      "Epoch 4/20\n",
      " - 65s - loss: 2.2155 - acc: 0.1357 - val_loss: 2.2106 - val_acc: 0.1386\n",
      "Epoch 5/20\n",
      " - 65s - loss: 2.2152 - acc: 0.1332 - val_loss: 2.2108 - val_acc: 0.1386\n",
      "Epoch 6/20\n",
      " - 63s - loss: 2.2140 - acc: 0.1359 - val_loss: 2.2105 - val_acc: 0.1386\n",
      "Epoch 7/20\n",
      " - 66s - loss: 2.2175 - acc: 0.1332 - val_loss: 2.2126 - val_acc: 0.1377\n",
      "Epoch 8/20\n",
      " - 64s - loss: 2.2179 - acc: 0.1335 - val_loss: 2.2123 - val_acc: 0.1377\n",
      "Epoch 9/20\n",
      " - 69s - loss: 2.2181 - acc: 0.1338 - val_loss: 2.2123 - val_acc: 0.1377\n",
      "Epoch 10/20\n",
      " - 65s - loss: 2.2155 - acc: 0.1387 - val_loss: 2.2128 - val_acc: 0.1377\n",
      "Epoch 11/20\n",
      " - 69s - loss: 2.2161 - acc: 0.1372 - val_loss: 2.2120 - val_acc: 0.1377\n",
      "Epoch 12/20\n",
      " - 74s - loss: 2.2171 - acc: 0.1347 - val_loss: 2.2133 - val_acc: 0.1374\n",
      "Epoch 13/20\n",
      " - 73s - loss: 2.2162 - acc: 0.1370 - val_loss: 2.2130 - val_acc: 0.1374\n",
      "Epoch 14/20\n",
      " - 59s - loss: 2.2178 - acc: 0.1344 - val_loss: 2.2131 - val_acc: 0.1374\n",
      "Epoch 15/20\n",
      " - 58s - loss: 2.2163 - acc: 0.1368 - val_loss: 2.2130 - val_acc: 0.1374\n",
      "Epoch 16/20\n",
      " - 59s - loss: 2.2165 - acc: 0.1366 - val_loss: 2.2123 - val_acc: 0.1376\n",
      "Epoch 17/20\n",
      " - 57s - loss: 2.2176 - acc: 0.1355 - val_loss: 2.2127 - val_acc: 0.1376\n",
      "Epoch 18/20\n",
      " - 59s - loss: 2.2127 - acc: 0.1382 - val_loss: 2.2128 - val_acc: 0.1376\n",
      "Epoch 19/20\n",
      " - 59s - loss: 2.2156 - acc: 0.1355 - val_loss: 2.2124 - val_acc: 0.1377\n",
      "Epoch 20/20\n",
      " - 59s - loss: 2.2134 - acc: 0.1347 - val_loss: 2.2120 - val_acc: 0.1377\n",
      "10000/10000 [==============================] - 6s 556us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment31:\tlayers and dropout_rates are: [[128, 128, 128], 0.4]\n",
      "=======================================================\n",
      "Time Used: 3590.064365000002\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.2120123008728028\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1377\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 86s - loss: 2.2636 - acc: 0.1217 - val_loss: 2.2209 - val_acc: 0.1354\n",
      "Epoch 2/20\n",
      " - 60s - loss: 2.2345 - acc: 0.1338 - val_loss: 2.2151 - val_acc: 0.1377\n",
      "Epoch 3/20\n",
      " - 59s - loss: 2.2271 - acc: 0.1322 - val_loss: 2.2139 - val_acc: 0.1372\n",
      "Epoch 4/20\n",
      " - 60s - loss: 2.2275 - acc: 0.1361 - val_loss: 2.2144 - val_acc: 0.1372\n",
      "Epoch 5/20\n",
      " - 60s - loss: 2.2261 - acc: 0.1335 - val_loss: 2.2136 - val_acc: 0.1372\n",
      "Epoch 6/20\n",
      " - 60s - loss: 2.2244 - acc: 0.1364 - val_loss: 2.2146 - val_acc: 0.1370\n",
      "Epoch 7/20\n",
      " - 60s - loss: 2.2247 - acc: 0.1355 - val_loss: 2.2140 - val_acc: 0.1370\n",
      "Epoch 8/20\n",
      " - 60s - loss: 2.2221 - acc: 0.1362 - val_loss: 2.2149 - val_acc: 0.1370\n",
      "Epoch 9/20\n",
      " - 61s - loss: 2.2240 - acc: 0.1339 - val_loss: 2.2151 - val_acc: 0.1370\n",
      "Epoch 10/20\n",
      " - 62s - loss: 2.2226 - acc: 0.1353 - val_loss: 2.2140 - val_acc: 0.1370\n",
      "Epoch 11/20\n",
      " - 74s - loss: 2.2218 - acc: 0.1353 - val_loss: 2.2141 - val_acc: 0.1375\n",
      "Epoch 12/20\n",
      " - 72s - loss: 2.2202 - acc: 0.1345 - val_loss: 2.1959 - val_acc: 0.1452\n",
      "Epoch 13/20\n",
      " - 61s - loss: 2.2077 - acc: 0.1391 - val_loss: 2.1958 - val_acc: 0.1455\n",
      "Epoch 14/20\n",
      " - 61s - loss: 2.2050 - acc: 0.1428 - val_loss: 2.1489 - val_acc: 0.1669\n",
      "Epoch 15/20\n",
      " - 61s - loss: 2.1782 - acc: 0.1585 - val_loss: 2.1390 - val_acc: 0.1708\n",
      "Epoch 16/20\n",
      " - 61s - loss: 2.1735 - acc: 0.1593 - val_loss: 2.1356 - val_acc: 0.1708\n",
      "Epoch 17/20\n",
      " - 62s - loss: 2.1741 - acc: 0.1606 - val_loss: 2.1374 - val_acc: 0.1708\n",
      "Epoch 18/20\n",
      " - 61s - loss: 2.1701 - acc: 0.1579 - val_loss: 2.1353 - val_acc: 0.1708\n",
      "Epoch 19/20\n",
      " - 62s - loss: 2.1725 - acc: 0.1584 - val_loss: 2.1369 - val_acc: 0.1708\n",
      "Epoch 20/20\n",
      " - 60s - loss: 2.1707 - acc: 0.1573 - val_loss: 2.1363 - val_acc: 0.1708\n",
      "10000/10000 [==============================] - 7s 730us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment32:\tlayers and dropout_rates are: [[128, 128, 128], 0.6]\n",
      "=======================================================\n",
      "Time Used: 3640.2470810000013\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.1363009994506834\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1708\n",
      "*******************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 94s - loss: 2.1696 - acc: 0.1586 - val_loss: 2.1271 - val_acc: 0.1699\n",
      "Epoch 2/20\n",
      " - 65s - loss: 2.1597 - acc: 0.1595 - val_loss: 2.1254 - val_acc: 0.1699\n",
      "Epoch 3/20\n",
      " - 64s - loss: 2.1582 - acc: 0.1611 - val_loss: 2.1268 - val_acc: 0.1699\n",
      "Epoch 4/20\n",
      " - 64s - loss: 2.1604 - acc: 0.1599 - val_loss: 2.1261 - val_acc: 0.1708\n",
      "Epoch 5/20\n",
      " - 65s - loss: 2.1632 - acc: 0.1595 - val_loss: 2.1272 - val_acc: 0.1708\n",
      "Epoch 6/20\n",
      " - 64s - loss: 2.1604 - acc: 0.1602 - val_loss: 2.1267 - val_acc: 0.1699\n",
      "Epoch 7/20\n",
      " - 64s - loss: 2.1598 - acc: 0.1594 - val_loss: 2.1282 - val_acc: 0.1699\n",
      "Epoch 8/20\n",
      " - 65s - loss: 2.1582 - acc: 0.1617 - val_loss: 2.1254 - val_acc: 0.1708\n",
      "Epoch 9/20\n",
      " - 64s - loss: 2.1607 - acc: 0.1597 - val_loss: 2.1287 - val_acc: 0.1699\n",
      "Epoch 10/20\n",
      " - 66s - loss: 2.1581 - acc: 0.1636 - val_loss: 2.1264 - val_acc: 0.1708\n",
      "Epoch 11/20\n",
      " - 78s - loss: 2.1598 - acc: 0.1609 - val_loss: 2.1272 - val_acc: 0.1708\n",
      "Epoch 12/20\n",
      " - 67s - loss: 2.1592 - acc: 0.1607 - val_loss: 2.1261 - val_acc: 0.1708\n",
      "Epoch 13/20\n",
      " - 67s - loss: 2.1589 - acc: 0.1627 - val_loss: 2.1265 - val_acc: 0.1708\n",
      "Epoch 14/20\n",
      " - 67s - loss: 2.1619 - acc: 0.1631 - val_loss: 2.1287 - val_acc: 0.1633\n",
      "Epoch 15/20\n",
      " - 67s - loss: 2.1595 - acc: 0.1617 - val_loss: 2.1271 - val_acc: 0.1708\n",
      "Epoch 16/20\n",
      " - 67s - loss: 2.1579 - acc: 0.1610 - val_loss: 2.1277 - val_acc: 0.1708\n",
      "Epoch 17/20\n",
      " - 64s - loss: 2.1585 - acc: 0.1605 - val_loss: 2.1259 - val_acc: 0.1699\n",
      "Epoch 18/20\n",
      " - 62s - loss: 2.1595 - acc: 0.1625 - val_loss: 2.1274 - val_acc: 0.1708\n",
      "Epoch 19/20\n",
      " - 61s - loss: 2.1577 - acc: 0.1637 - val_loss: 2.1267 - val_acc: 0.1708\n",
      "Epoch 20/20\n",
      " - 61s - loss: 2.1590 - acc: 0.1625 - val_loss: 2.1262 - val_acc: 0.1708\n",
      "10000/10000 [==============================] - 7s 718us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment33:\tlayers and dropout_rates are: [[256, 256, 256], 0.2]\n",
      "=======================================================\n",
      "Time Used: 3798.5061050000004\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.1261708751678468\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1708\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 98s - loss: 2.1799 - acc: 0.1548 - val_loss: 2.1284 - val_acc: 0.1708\n",
      "Epoch 2/20\n",
      " - 65s - loss: 2.1633 - acc: 0.1582 - val_loss: 2.1275 - val_acc: 0.1708\n",
      "Epoch 3/20\n",
      " - 67s - loss: 2.1667 - acc: 0.1563 - val_loss: 2.1287 - val_acc: 0.1708\n",
      "Epoch 4/20\n",
      " - 76s - loss: 2.1656 - acc: 0.1569 - val_loss: 2.1295 - val_acc: 0.1708\n",
      "Epoch 5/20\n",
      " - 72s - loss: 2.1632 - acc: 0.1622 - val_loss: 2.1300 - val_acc: 0.1708\n",
      "Epoch 6/20\n",
      " - 71s - loss: 2.1632 - acc: 0.1596 - val_loss: 2.1290 - val_acc: 0.1708\n",
      "Epoch 7/20\n",
      " - 72s - loss: 2.1637 - acc: 0.1607 - val_loss: 2.1289 - val_acc: 0.1699\n",
      "Epoch 8/20\n",
      " - 73s - loss: 2.1641 - acc: 0.1605 - val_loss: 2.1287 - val_acc: 0.1699\n",
      "Epoch 9/20\n",
      " - 68s - loss: 2.1642 - acc: 0.1574 - val_loss: 2.1298 - val_acc: 0.1708\n",
      "Epoch 10/20\n",
      " - 69s - loss: 2.1615 - acc: 0.1592 - val_loss: 2.1290 - val_acc: 0.1708\n",
      "Epoch 11/20\n",
      " - 72s - loss: 2.1606 - acc: 0.1608 - val_loss: 2.1268 - val_acc: 0.1708\n",
      "Epoch 12/20\n",
      " - 73s - loss: 2.1622 - acc: 0.1624 - val_loss: 2.1274 - val_acc: 0.1708\n",
      "Epoch 13/20\n",
      " - 72s - loss: 2.1611 - acc: 0.1601 - val_loss: 2.1286 - val_acc: 0.1699\n",
      "Epoch 14/20\n",
      " - 73s - loss: 2.1619 - acc: 0.1576 - val_loss: 2.1302 - val_acc: 0.1699\n",
      "Epoch 15/20\n",
      " - 75s - loss: 2.1618 - acc: 0.1611 - val_loss: 2.1283 - val_acc: 0.1699\n",
      "Epoch 16/20\n",
      " - 74s - loss: 2.1613 - acc: 0.1621 - val_loss: 2.1287 - val_acc: 0.1708\n",
      "Epoch 17/20\n",
      " - 72s - loss: 2.1612 - acc: 0.1599 - val_loss: 2.1277 - val_acc: 0.1708\n",
      "Epoch 18/20\n",
      " - 75s - loss: 2.1605 - acc: 0.1610 - val_loss: 2.1257 - val_acc: 0.1682\n",
      "Epoch 19/20\n",
      " - 77s - loss: 2.1582 - acc: 0.1609 - val_loss: 2.1290 - val_acc: 0.1708\n",
      "Epoch 20/20\n",
      " - 71s - loss: 2.1591 - acc: 0.1613 - val_loss: 2.1281 - val_acc: 0.1699\n",
      "10000/10000 [==============================] - 7s 672us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment34:\tlayers and dropout_rates are: [[256, 256, 256], 0.4]\n",
      "=======================================================\n",
      "Time Used: 3941.8738630000007\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.1280705322265625\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1699\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 119s - loss: 2.1989 - acc: 0.1524 - val_loss: 2.1326 - val_acc: 0.1708\n",
      "Epoch 2/20\n",
      " - 78s - loss: 2.1735 - acc: 0.1583 - val_loss: 2.1303 - val_acc: 0.1708\n",
      "Epoch 3/20\n",
      " - 82s - loss: 2.1722 - acc: 0.1579 - val_loss: 2.1312 - val_acc: 0.1708\n",
      "Epoch 4/20\n",
      " - 79s - loss: 2.1717 - acc: 0.1555 - val_loss: 2.1315 - val_acc: 0.1708\n",
      "Epoch 5/20\n",
      " - 76s - loss: 2.1712 - acc: 0.1556 - val_loss: 2.1333 - val_acc: 0.1708\n",
      "Epoch 6/20\n",
      " - 76s - loss: 2.1685 - acc: 0.1575 - val_loss: 2.1294 - val_acc: 0.1708\n",
      "Epoch 7/20\n",
      " - 78s - loss: 2.1680 - acc: 0.1560 - val_loss: 2.1291 - val_acc: 0.1708\n",
      "Epoch 8/20\n",
      " - 76s - loss: 2.1694 - acc: 0.1612 - val_loss: 2.1307 - val_acc: 0.1708\n",
      "Epoch 9/20\n",
      " - 78s - loss: 2.1657 - acc: 0.1559 - val_loss: 2.1316 - val_acc: 0.1708\n",
      "Epoch 10/20\n",
      " - 73s - loss: 2.1689 - acc: 0.1589 - val_loss: 2.1308 - val_acc: 0.1708\n",
      "Epoch 11/20\n",
      " - 72s - loss: 2.1694 - acc: 0.1584 - val_loss: 2.1309 - val_acc: 0.1708\n",
      "Epoch 12/20\n",
      " - 76s - loss: 2.1649 - acc: 0.1592 - val_loss: 2.1298 - val_acc: 0.1708\n",
      "Epoch 13/20\n",
      " - 77s - loss: 2.1661 - acc: 0.1597 - val_loss: 2.1312 - val_acc: 0.1699\n",
      "Epoch 14/20\n",
      " - 72s - loss: 2.1634 - acc: 0.1603 - val_loss: 2.1297 - val_acc: 0.1699\n",
      "Epoch 15/20\n",
      " - 76s - loss: 2.1654 - acc: 0.1602 - val_loss: 2.1288 - val_acc: 0.1699\n",
      "Epoch 16/20\n",
      " - 74s - loss: 2.1634 - acc: 0.1580 - val_loss: 2.1283 - val_acc: 0.1708\n",
      "Epoch 17/20\n",
      " - 73s - loss: 2.1622 - acc: 0.1627 - val_loss: 2.1307 - val_acc: 0.1699\n",
      "Epoch 18/20\n",
      " - 75s - loss: 2.1644 - acc: 0.1611 - val_loss: 2.1304 - val_acc: 0.1708\n",
      "Epoch 19/20\n",
      " - 74s - loss: 2.1649 - acc: 0.1612 - val_loss: 2.1289 - val_acc: 0.1708\n",
      "Epoch 20/20\n",
      " - 75s - loss: 2.1639 - acc: 0.1607 - val_loss: 2.1288 - val_acc: 0.1708\n",
      "10000/10000 [==============================] - 7s 740us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment35:\tlayers and dropout_rates are: [[256, 256, 256], 0.6]\n",
      "=======================================================\n",
      "Time Used: 4089.1838949999947\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.128767169570923\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1708\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 136s - loss: 2.1634 - acc: 0.1618 - val_loss: 2.1287 - val_acc: 0.1616\n",
      "Epoch 2/20\n",
      " - 90s - loss: 2.1603 - acc: 0.1596 - val_loss: 2.1278 - val_acc: 0.1708\n",
      "Epoch 3/20\n",
      " - 79s - loss: 2.1573 - acc: 0.1611 - val_loss: 2.1282 - val_acc: 0.1708\n",
      "Epoch 4/20\n",
      " - 78s - loss: 2.1570 - acc: 0.1652 - val_loss: 2.1262 - val_acc: 0.1708\n",
      "Epoch 5/20\n",
      " - 80s - loss: 2.1578 - acc: 0.1613 - val_loss: 2.1264 - val_acc: 0.1699\n",
      "Epoch 6/20\n",
      " - 90s - loss: 2.1574 - acc: 0.1621 - val_loss: 2.1276 - val_acc: 0.1682\n",
      "Epoch 7/20\n",
      " - 87s - loss: 2.1592 - acc: 0.1598 - val_loss: 2.1268 - val_acc: 0.1708\n",
      "Epoch 8/20\n",
      " - 83s - loss: 2.1586 - acc: 0.1612 - val_loss: 2.1266 - val_acc: 0.1699\n",
      "Epoch 9/20\n",
      " - 82s - loss: 2.1583 - acc: 0.1631 - val_loss: 2.1273 - val_acc: 0.1699\n",
      "Epoch 10/20\n",
      " - 87s - loss: 2.1563 - acc: 0.1612 - val_loss: 2.1274 - val_acc: 0.1699\n",
      "Epoch 11/20\n",
      " - 81s - loss: 2.1572 - acc: 0.1628 - val_loss: 2.1260 - val_acc: 0.1699\n",
      "Epoch 12/20\n",
      " - 84s - loss: 2.1576 - acc: 0.1628 - val_loss: 2.1250 - val_acc: 0.1708\n",
      "Epoch 13/20\n",
      " - 92s - loss: 2.1575 - acc: 0.1631 - val_loss: 2.1254 - val_acc: 0.1708\n",
      "Epoch 14/20\n",
      " - 80s - loss: 2.1578 - acc: 0.1633 - val_loss: 2.1273 - val_acc: 0.1699\n",
      "Epoch 15/20\n",
      " - 73s - loss: 2.1573 - acc: 0.1626 - val_loss: 2.1263 - val_acc: 0.1708\n",
      "Epoch 16/20\n",
      " - 71s - loss: 2.1574 - acc: 0.1650 - val_loss: 2.1266 - val_acc: 0.1699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20\n",
      " - 72s - loss: 2.1578 - acc: 0.1616 - val_loss: 2.1270 - val_acc: 0.1699\n",
      "Epoch 18/20\n",
      " - 71s - loss: 2.1581 - acc: 0.1637 - val_loss: 2.1266 - val_acc: 0.1708\n",
      "Epoch 19/20\n",
      " - 79s - loss: 2.1556 - acc: 0.1608 - val_loss: 2.1302 - val_acc: 0.1699\n",
      "Epoch 20/20\n",
      " - 83s - loss: 2.1568 - acc: 0.1613 - val_loss: 2.1270 - val_acc: 0.1708\n",
      "10000/10000 [==============================] - 9s 905us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment36:\tlayers and dropout_rates are: [[512, 512, 512], 0.2]\n",
      "=======================================================\n",
      "Time Used: 4461.7858019999985\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.1270097160339354\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1708\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 144s - loss: 2.1717 - acc: 0.1561 - val_loss: 2.1299 - val_acc: 0.1708\n",
      "Epoch 2/20\n",
      " - 97s - loss: 2.1629 - acc: 0.1582 - val_loss: 2.1288 - val_acc: 0.1708\n",
      "Epoch 3/20\n",
      " - 101s - loss: 2.1620 - acc: 0.1598 - val_loss: 2.1275 - val_acc: 0.1699\n",
      "Epoch 4/20\n",
      " - 104s - loss: 2.1592 - acc: 0.1601 - val_loss: 2.1268 - val_acc: 0.1708\n",
      "Epoch 5/20\n",
      " - 85s - loss: 2.1618 - acc: 0.1582 - val_loss: 2.1292 - val_acc: 0.1699\n",
      "Epoch 6/20\n",
      " - 87s - loss: 2.1611 - acc: 0.1600 - val_loss: 2.1282 - val_acc: 0.1708\n",
      "Epoch 7/20\n",
      " - 90s - loss: 2.1596 - acc: 0.1622 - val_loss: 2.1273 - val_acc: 0.1708\n",
      "Epoch 8/20\n",
      " - 79s - loss: 2.1581 - acc: 0.1620 - val_loss: 2.1289 - val_acc: 0.1699\n",
      "Epoch 9/20\n",
      " - 76s - loss: 2.1599 - acc: 0.1614 - val_loss: 2.1272 - val_acc: 0.1708\n",
      "Epoch 10/20\n",
      " - 77s - loss: 2.1586 - acc: 0.1620 - val_loss: 2.1283 - val_acc: 0.1708\n",
      "Epoch 11/20\n",
      " - 77s - loss: 2.1584 - acc: 0.1635 - val_loss: 2.1292 - val_acc: 0.1708\n",
      "Epoch 12/20\n",
      " - 77s - loss: 2.1595 - acc: 0.1599 - val_loss: 2.1269 - val_acc: 0.1708\n",
      "Epoch 13/20\n",
      " - 97s - loss: 2.1573 - acc: 0.1624 - val_loss: 2.1275 - val_acc: 0.1682\n",
      "Epoch 14/20\n",
      " - 92s - loss: 2.1602 - acc: 0.1623 - val_loss: 2.1273 - val_acc: 0.1708\n",
      "Epoch 15/20\n",
      " - 92s - loss: 2.1599 - acc: 0.1629 - val_loss: 2.1281 - val_acc: 0.1708\n",
      "Epoch 16/20\n",
      " - 87s - loss: 2.1584 - acc: 0.1612 - val_loss: 2.1278 - val_acc: 0.1699\n",
      "Epoch 17/20\n",
      " - 88s - loss: 2.1576 - acc: 0.1616 - val_loss: 2.1292 - val_acc: 0.1699\n",
      "Epoch 18/20\n",
      " - 89s - loss: 2.1590 - acc: 0.1603 - val_loss: 2.1293 - val_acc: 0.1699\n",
      "Epoch 19/20\n",
      " - 89s - loss: 2.1599 - acc: 0.1639 - val_loss: 2.1258 - val_acc: 0.1633\n",
      "Epoch 20/20\n",
      " - 97s - loss: 2.1612 - acc: 0.1607 - val_loss: 2.1284 - val_acc: 0.1708\n",
      "10000/10000 [==============================] - 10s 960us/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment37:\tlayers and dropout_rates are: [[512, 512, 512], 0.4]\n",
      "=======================================================\n",
      "Time Used: 4816.555141000004\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.128408865737915\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1708\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 159s - loss: 2.1812 - acc: 0.1538 - val_loss: 2.1284 - val_acc: 0.1708\n",
      "Epoch 2/20\n",
      " - 91s - loss: 2.1664 - acc: 0.1568 - val_loss: 2.1299 - val_acc: 0.1699\n",
      "Epoch 3/20\n",
      " - 84s - loss: 2.1688 - acc: 0.1544 - val_loss: 2.1308 - val_acc: 0.1708\n",
      "Epoch 4/20\n",
      " - 84s - loss: 2.1656 - acc: 0.1587 - val_loss: 2.1302 - val_acc: 0.1708\n",
      "Epoch 5/20\n",
      " - 87s - loss: 2.1638 - acc: 0.1603 - val_loss: 2.1290 - val_acc: 0.1708\n",
      "Epoch 6/20\n",
      " - 84s - loss: 2.1646 - acc: 0.1589 - val_loss: 2.1294 - val_acc: 0.1708\n",
      "Epoch 7/20\n",
      " - 87s - loss: 2.1644 - acc: 0.1611 - val_loss: 2.1304 - val_acc: 0.1708\n",
      "Epoch 8/20\n",
      " - 96s - loss: 2.1636 - acc: 0.1575 - val_loss: 2.1299 - val_acc: 0.1708\n",
      "Epoch 9/20\n",
      " - 95s - loss: 2.1633 - acc: 0.1566 - val_loss: 2.1294 - val_acc: 0.1708\n",
      "Epoch 10/20\n",
      " - 91s - loss: 2.1617 - acc: 0.1613 - val_loss: 2.1298 - val_acc: 0.1708\n",
      "Epoch 11/20\n",
      " - 94s - loss: 2.1626 - acc: 0.1583 - val_loss: 2.1296 - val_acc: 0.1699\n",
      "Epoch 12/20\n",
      " - 94s - loss: 2.1615 - acc: 0.1608 - val_loss: 2.1286 - val_acc: 0.1699\n",
      "Epoch 13/20\n",
      " - 103s - loss: 2.1608 - acc: 0.1611 - val_loss: 2.1312 - val_acc: 0.1708\n",
      "Epoch 14/20\n",
      " - 90s - loss: 2.1615 - acc: 0.1617 - val_loss: 2.1279 - val_acc: 0.1708\n",
      "Epoch 15/20\n",
      " - 98s - loss: 2.1598 - acc: 0.1616 - val_loss: 2.1273 - val_acc: 0.1708\n",
      "Epoch 16/20\n",
      " - 92s - loss: 2.1607 - acc: 0.1599 - val_loss: 2.1323 - val_acc: 0.1708\n",
      "Epoch 17/20\n",
      " - 90s - loss: 2.1595 - acc: 0.1610 - val_loss: 2.1285 - val_acc: 0.1708\n",
      "Epoch 18/20\n",
      " - 95s - loss: 2.1627 - acc: 0.1603 - val_loss: 2.1283 - val_acc: 0.1699\n",
      "Epoch 19/20\n",
      " - 99s - loss: 2.1618 - acc: 0.1585 - val_loss: 2.1315 - val_acc: 0.1708\n",
      "Epoch 20/20\n",
      " - 99s - loss: 2.1616 - acc: 0.1600 - val_loss: 2.1285 - val_acc: 0.1708\n",
      "10000/10000 [==============================] - 13s 1ms/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment38:\tlayers and dropout_rates are: [[512, 512, 512], 0.6]\n",
      "=======================================================\n",
      "Time Used: 5182.7402319999965\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.1285130348205565\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1708\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 180s - loss: 2.1625 - acc: 0.1594 - val_loss: 2.1275 - val_acc: 0.1708\n",
      "Epoch 2/20\n",
      " - 129s - loss: 2.1566 - acc: 0.1627 - val_loss: 2.1284 - val_acc: 0.1642\n",
      "Epoch 3/20\n",
      " - 124s - loss: 2.1578 - acc: 0.1621 - val_loss: 2.1276 - val_acc: 0.1699\n",
      "Epoch 4/20\n",
      " - 146s - loss: 2.1594 - acc: 0.1610 - val_loss: 2.1262 - val_acc: 0.1682\n",
      "Epoch 5/20\n",
      " - 141s - loss: 2.1578 - acc: 0.1616 - val_loss: 2.1269 - val_acc: 0.1699\n",
      "Epoch 6/20\n",
      " - 153s - loss: 2.1589 - acc: 0.1601 - val_loss: 2.1264 - val_acc: 0.1682\n",
      "Epoch 7/20\n",
      " - 149s - loss: 2.1573 - acc: 0.1626 - val_loss: 2.1278 - val_acc: 0.1699\n",
      "Epoch 8/20\n",
      " - 145s - loss: 2.1561 - acc: 0.1622 - val_loss: 2.1262 - val_acc: 0.1642\n",
      "Epoch 9/20\n",
      " - 148s - loss: 2.1571 - acc: 0.1613 - val_loss: 2.1276 - val_acc: 0.1682\n",
      "Epoch 10/20\n",
      " - 133s - loss: 2.1556 - acc: 0.1636 - val_loss: 2.1282 - val_acc: 0.1699\n",
      "Epoch 11/20\n",
      " - 144s - loss: 2.1569 - acc: 0.1634 - val_loss: 2.1273 - val_acc: 0.1708\n",
      "Epoch 12/20\n",
      " - 123s - loss: 2.1581 - acc: 0.1604 - val_loss: 2.1255 - val_acc: 0.1708\n",
      "Epoch 13/20\n",
      " - 116s - loss: 2.1589 - acc: 0.1625 - val_loss: 2.1254 - val_acc: 0.1708\n",
      "Epoch 14/20\n",
      " - 136s - loss: 2.1577 - acc: 0.1620 - val_loss: 2.1259 - val_acc: 0.1708\n",
      "Epoch 15/20\n",
      " - 135s - loss: 2.1578 - acc: 0.1584 - val_loss: 2.1283 - val_acc: 0.1708\n",
      "Epoch 16/20\n",
      " - 132s - loss: 2.1576 - acc: 0.1646 - val_loss: 2.1253 - val_acc: 0.1708\n",
      "Epoch 17/20\n",
      " - 129s - loss: 2.1558 - acc: 0.1630 - val_loss: 2.1269 - val_acc: 0.1699\n",
      "Epoch 18/20\n",
      " - 129s - loss: 2.1581 - acc: 0.1618 - val_loss: 2.1255 - val_acc: 0.1708\n",
      "Epoch 19/20\n",
      " - 139s - loss: 2.1564 - acc: 0.1640 - val_loss: 2.1250 - val_acc: 0.1699\n",
      "Epoch 20/20\n",
      " - 126s - loss: 2.1580 - acc: 0.1612 - val_loss: 2.1275 - val_acc: 0.1682\n",
      "10000/10000 [==============================] - 12s 1ms/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment39:\tlayers and dropout_rates are: [[1024, 1024, 1024], 0.2]\n",
      "=======================================================\n",
      "Time Used: 6567.081699999995\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.1274575855255127\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1682\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 228s - loss: 2.1678 - acc: 0.1575 - val_loss: 2.1296 - val_acc: 0.1708\n",
      "Epoch 2/20\n",
      " - 171s - loss: 2.1597 - acc: 0.1593 - val_loss: 2.1278 - val_acc: 0.1699\n",
      "Epoch 3/20\n",
      " - 149s - loss: 2.1617 - acc: 0.1591 - val_loss: 2.1277 - val_acc: 0.1708\n",
      "Epoch 4/20\n",
      " - 147s - loss: 2.1595 - acc: 0.1624 - val_loss: 2.1280 - val_acc: 0.1708\n",
      "Epoch 5/20\n",
      " - 194s - loss: 2.1602 - acc: 0.1607 - val_loss: 2.1284 - val_acc: 0.1708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20\n",
      " - 174s - loss: 2.1589 - acc: 0.1601 - val_loss: 2.1273 - val_acc: 0.1699\n",
      "Epoch 7/20\n",
      " - 132s - loss: 2.1554 - acc: 0.1606 - val_loss: 2.1262 - val_acc: 0.1708\n",
      "Epoch 8/20\n",
      " - 151s - loss: 2.1587 - acc: 0.1610 - val_loss: 2.1278 - val_acc: 0.1708\n",
      "Epoch 9/20\n",
      " - 151s - loss: 2.1570 - acc: 0.1603 - val_loss: 2.1258 - val_acc: 0.1699\n",
      "Epoch 10/20\n",
      " - 141s - loss: 2.1601 - acc: 0.1593 - val_loss: 2.1298 - val_acc: 0.1699\n",
      "Epoch 11/20\n",
      " - 149s - loss: 2.1575 - acc: 0.1618 - val_loss: 2.1262 - val_acc: 0.1699\n",
      "Epoch 12/20\n",
      " - 138s - loss: 2.1585 - acc: 0.1618 - val_loss: 2.1273 - val_acc: 0.1708\n",
      "Epoch 13/20\n",
      " - 162s - loss: 2.1616 - acc: 0.1597 - val_loss: 2.1263 - val_acc: 0.1708\n",
      "Epoch 14/20\n",
      " - 148s - loss: 2.1591 - acc: 0.1618 - val_loss: 2.1261 - val_acc: 0.1699\n",
      "Epoch 15/20\n",
      " - 148s - loss: 2.1585 - acc: 0.1630 - val_loss: 2.1291 - val_acc: 0.1682\n",
      "Epoch 16/20\n",
      " - 149s - loss: 2.1588 - acc: 0.1614 - val_loss: 2.1292 - val_acc: 0.1642\n",
      "Epoch 17/20\n",
      " - 131s - loss: 2.1574 - acc: 0.1631 - val_loss: 2.1288 - val_acc: 0.1708\n",
      "Epoch 18/20\n",
      " - 146s - loss: 2.1577 - acc: 0.1596 - val_loss: 2.1289 - val_acc: 0.1699\n",
      "Epoch 19/20\n",
      " - 138s - loss: 2.1601 - acc: 0.1632 - val_loss: 2.1278 - val_acc: 0.1708\n",
      "Epoch 20/20\n",
      " - 135s - loss: 2.1572 - acc: 0.1623 - val_loss: 2.1253 - val_acc: 0.1708\n",
      "10000/10000 [==============================] - 15s 1ms/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment40:\tlayers and dropout_rates are: [[1024, 1024, 1024], 0.4]\n",
      "=======================================================\n",
      "Time Used: 7720.766034999993\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.1253053916931153\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1708\n",
      "*******************************************************\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 219s - loss: 2.1742 - acc: 0.1560 - val_loss: 2.1298 - val_acc: 0.1708\n",
      "Epoch 2/20\n",
      " - 167s - loss: 2.1600 - acc: 0.1582 - val_loss: 2.1284 - val_acc: 0.1699\n",
      "Epoch 3/20\n",
      " - 167s - loss: 2.1622 - acc: 0.1588 - val_loss: 2.1292 - val_acc: 0.1699\n",
      "Epoch 4/20\n",
      " - 156s - loss: 2.1632 - acc: 0.1579 - val_loss: 2.1279 - val_acc: 0.1708\n",
      "Epoch 5/20\n",
      " - 164s - loss: 2.1597 - acc: 0.1601 - val_loss: 2.1301 - val_acc: 0.1708\n",
      "Epoch 6/20\n",
      " - 173s - loss: 2.1625 - acc: 0.1600 - val_loss: 2.1279 - val_acc: 0.1699\n",
      "Epoch 7/20\n",
      " - 175s - loss: 2.1615 - acc: 0.1619 - val_loss: 2.1278 - val_acc: 0.1708\n",
      "Epoch 8/20\n",
      " - 147s - loss: 2.1623 - acc: 0.1586 - val_loss: 2.1275 - val_acc: 0.1708\n",
      "Epoch 9/20\n",
      " - 148s - loss: 2.1603 - acc: 0.1605 - val_loss: 2.1282 - val_acc: 0.1699\n",
      "Epoch 10/20\n",
      " - 145s - loss: 2.1581 - acc: 0.1604 - val_loss: 2.1278 - val_acc: 0.1708\n",
      "Epoch 11/20\n",
      " - 145s - loss: 2.1597 - acc: 0.1644 - val_loss: 2.1278 - val_acc: 0.1708\n",
      "Epoch 12/20\n",
      " - 144s - loss: 2.1596 - acc: 0.1621 - val_loss: 2.1270 - val_acc: 0.1708\n",
      "Epoch 13/20\n",
      " - 142s - loss: 2.1589 - acc: 0.1633 - val_loss: 2.1298 - val_acc: 0.1708\n",
      "Epoch 14/20\n",
      " - 174s - loss: 2.1611 - acc: 0.1615 - val_loss: 2.1280 - val_acc: 0.1708\n",
      "Epoch 15/20\n",
      " - 165s - loss: 2.1628 - acc: 0.1602 - val_loss: 2.1306 - val_acc: 0.1708\n",
      "Epoch 16/20\n",
      " - 161s - loss: 2.1613 - acc: 0.1609 - val_loss: 2.1302 - val_acc: 0.1682\n",
      "Epoch 17/20\n",
      " - 156s - loss: 2.1585 - acc: 0.1621 - val_loss: 2.1285 - val_acc: 0.1699\n",
      "Epoch 18/20\n",
      " - 170s - loss: 2.1572 - acc: 0.1626 - val_loss: 2.1282 - val_acc: 0.1708\n",
      "Epoch 19/20\n",
      " - 158s - loss: 2.1573 - acc: 0.1636 - val_loss: 2.1280 - val_acc: 0.1708\n",
      "Epoch 20/20\n",
      " - 170s - loss: 2.1566 - acc: 0.1618 - val_loss: 2.1276 - val_acc: 0.1708\n",
      "10000/10000 [==============================] - 21s 2ms/step\n",
      "*******************************************************\n",
      "=======================================================\n",
      "Experiment41:\tlayers and dropout_rates are: [[1024, 1024, 1024], 0.6]\n",
      "=======================================================\n",
      "Time Used: 8980.636012000003\n",
      "-------------------------------------------------------\n",
      "Test loss: 2.127583786392212\n",
      "-------------------------------------------------------\n",
      "Test accuracy: 0.1708\n",
      "*******************************************************\n"
     ]
    }
   ],
   "source": [
    "record = experiment_config(list_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. SUMMARIZE EXPERIMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Best Result is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4313, 'layers and dropout_rates are: [[64], 0.2]')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_acc, experiment, history = getBest(record)\n",
    "print('The Best Result is:')\n",
    "max_acc, experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotAcc(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotLoss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Result summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = get_allResult(record)\n",
    "\n",
    "# add control parameters\n",
    "\n",
    "df_result['activation'] = 'relu'\n",
    "df_result['optimizer'] = 'Adam with learning rate @ 0.001'\n",
    "df_result['batch_size'] = 128\n",
    "\n",
    "# df_result['num_layers'] = 3\n",
    "# df_result['num_neurons'] = 256\n",
    "# df_result['dropout_rate'] = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Config</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>activation</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>layers and dropout_rates are: [[64], 0.2]</td>\n",
       "      <td>1.644154</td>\n",
       "      <td>0.4313</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>layers and dropout_rates are: [[1024], 0.6]</td>\n",
       "      <td>1.794208</td>\n",
       "      <td>0.3170</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>layers and dropout_rates are: [[1024], 0.4]</td>\n",
       "      <td>1.798803</td>\n",
       "      <td>0.3143</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>layers and dropout_rates are: [[128], 0.4]</td>\n",
       "      <td>1.804887</td>\n",
       "      <td>0.3140</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>layers and dropout_rates are: [[1024], 0.2]</td>\n",
       "      <td>1.798460</td>\n",
       "      <td>0.3126</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>layers and dropout_rates are: [[64, 64], 0.2]</td>\n",
       "      <td>1.803888</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>layers and dropout_rates are: [[512], 0.4]</td>\n",
       "      <td>1.813594</td>\n",
       "      <td>0.3117</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>layers and dropout_rates are: [[512], 0.2]</td>\n",
       "      <td>1.820373</td>\n",
       "      <td>0.3078</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>layers and dropout_rates are: [[128], 0.2]</td>\n",
       "      <td>1.810473</td>\n",
       "      <td>0.3071</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>layers and dropout_rates are: [[256], 0.2]</td>\n",
       "      <td>1.805768</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>layers and dropout_rates are: [[64], 0.4]</td>\n",
       "      <td>1.836037</td>\n",
       "      <td>0.3049</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>layers and dropout_rates are: [[64], 0.6]</td>\n",
       "      <td>1.832655</td>\n",
       "      <td>0.3025</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>layers and dropout_rates are: [[512], 0.6]</td>\n",
       "      <td>1.814109</td>\n",
       "      <td>0.3018</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>layers and dropout_rates are: [[256], 0.4]</td>\n",
       "      <td>1.805425</td>\n",
       "      <td>0.2993</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>layers and dropout_rates are: [[256], 0.6]</td>\n",
       "      <td>1.808477</td>\n",
       "      <td>0.2952</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>layers and dropout_rates are: [[128], 0.6]</td>\n",
       "      <td>1.815379</td>\n",
       "      <td>0.2909</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>layers and dropout_rates are: [[64, 64], 0.4]</td>\n",
       "      <td>1.821775</td>\n",
       "      <td>0.2878</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>layers and dropout_rates are: [[64, 64], 0.6]</td>\n",
       "      <td>1.914562</td>\n",
       "      <td>0.2262</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>layers and dropout_rates are: [[128, 128], 0.2]</td>\n",
       "      <td>1.963366</td>\n",
       "      <td>0.2056</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>layers and dropout_rates are: [[128, 128], 0.4]</td>\n",
       "      <td>1.981045</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>layers and dropout_rates are: [[128, 128], 0.6]</td>\n",
       "      <td>2.014765</td>\n",
       "      <td>0.1902</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>layers and dropout_rates are: [[512, 512], 0.2]</td>\n",
       "      <td>2.095114</td>\n",
       "      <td>0.1755</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>layers and dropout_rates are: [[1024, 1024], 0.6]</td>\n",
       "      <td>2.101856</td>\n",
       "      <td>0.1750</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>layers and dropout_rates are: [[128, 128, 128]...</td>\n",
       "      <td>2.136301</td>\n",
       "      <td>0.1708</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>layers and dropout_rates are: [[256, 256, 256]...</td>\n",
       "      <td>2.126171</td>\n",
       "      <td>0.1708</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>layers and dropout_rates are: [[1024, 1024, 10...</td>\n",
       "      <td>2.125305</td>\n",
       "      <td>0.1708</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>layers and dropout_rates are: [[512, 512, 512]...</td>\n",
       "      <td>2.128513</td>\n",
       "      <td>0.1708</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>layers and dropout_rates are: [[512, 512, 512]...</td>\n",
       "      <td>2.128409</td>\n",
       "      <td>0.1708</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>layers and dropout_rates are: [[512, 512, 512]...</td>\n",
       "      <td>2.127010</td>\n",
       "      <td>0.1708</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>layers and dropout_rates are: [[256, 256, 256]...</td>\n",
       "      <td>2.128767</td>\n",
       "      <td>0.1708</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>layers and dropout_rates are: [[1024, 1024, 10...</td>\n",
       "      <td>2.127584</td>\n",
       "      <td>0.1708</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>layers and dropout_rates are: [[64, 64, 64], 0.6]</td>\n",
       "      <td>2.157908</td>\n",
       "      <td>0.1706</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>layers and dropout_rates are: [[256, 256, 256]...</td>\n",
       "      <td>2.128071</td>\n",
       "      <td>0.1699</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>layers and dropout_rates are: [[64, 64, 64], 0.2]</td>\n",
       "      <td>2.125990</td>\n",
       "      <td>0.1683</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>layers and dropout_rates are: [[1024, 1024, 10...</td>\n",
       "      <td>2.127458</td>\n",
       "      <td>0.1682</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>layers and dropout_rates are: [[1024, 1024], 0.2]</td>\n",
       "      <td>2.164707</td>\n",
       "      <td>0.1505</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>layers and dropout_rates are: [[128, 128, 128]...</td>\n",
       "      <td>2.209432</td>\n",
       "      <td>0.1383</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>layers and dropout_rates are: [[512, 512], 0.4]</td>\n",
       "      <td>2.235458</td>\n",
       "      <td>0.1381</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>layers and dropout_rates are: [[128, 128, 128]...</td>\n",
       "      <td>2.212012</td>\n",
       "      <td>0.1377</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>layers and dropout_rates are: [[512, 512], 0.6]</td>\n",
       "      <td>2.257675</td>\n",
       "      <td>0.1159</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>layers and dropout_rates are: [[1024, 1024], 0.4]</td>\n",
       "      <td>2.285413</td>\n",
       "      <td>0.1086</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>layers and dropout_rates are: [[64, 64, 64], 0.4]</td>\n",
       "      <td>2.301417</td>\n",
       "      <td>0.0998</td>\n",
       "      <td>relu</td>\n",
       "      <td>Adam with learning rate @ 0.001</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Config      Loss  Accuracy  \\\n",
       "0           layers and dropout_rates are: [[64], 0.2]  1.644154    0.4313   \n",
       "14        layers and dropout_rates are: [[1024], 0.6]  1.794208    0.3170   \n",
       "13        layers and dropout_rates are: [[1024], 0.4]  1.798803    0.3143   \n",
       "4          layers and dropout_rates are: [[128], 0.4]  1.804887    0.3140   \n",
       "12        layers and dropout_rates are: [[1024], 0.2]  1.798460    0.3126   \n",
       "15      layers and dropout_rates are: [[64, 64], 0.2]  1.803888    0.3125   \n",
       "10         layers and dropout_rates are: [[512], 0.4]  1.813594    0.3117   \n",
       "9          layers and dropout_rates are: [[512], 0.2]  1.820373    0.3078   \n",
       "3          layers and dropout_rates are: [[128], 0.2]  1.810473    0.3071   \n",
       "6          layers and dropout_rates are: [[256], 0.2]  1.805768    0.3063   \n",
       "1           layers and dropout_rates are: [[64], 0.4]  1.836037    0.3049   \n",
       "2           layers and dropout_rates are: [[64], 0.6]  1.832655    0.3025   \n",
       "11         layers and dropout_rates are: [[512], 0.6]  1.814109    0.3018   \n",
       "7          layers and dropout_rates are: [[256], 0.4]  1.805425    0.2993   \n",
       "8          layers and dropout_rates are: [[256], 0.6]  1.808477    0.2952   \n",
       "5          layers and dropout_rates are: [[128], 0.6]  1.815379    0.2909   \n",
       "16      layers and dropout_rates are: [[64, 64], 0.4]  1.821775    0.2878   \n",
       "17      layers and dropout_rates are: [[64, 64], 0.6]  1.914562    0.2262   \n",
       "18    layers and dropout_rates are: [[128, 128], 0.2]  1.963366    0.2056   \n",
       "19    layers and dropout_rates are: [[128, 128], 0.4]  1.981045    0.1980   \n",
       "20    layers and dropout_rates are: [[128, 128], 0.6]  2.014765    0.1902   \n",
       "21    layers and dropout_rates are: [[512, 512], 0.2]  2.095114    0.1755   \n",
       "26  layers and dropout_rates are: [[1024, 1024], 0.6]  2.101856    0.1750   \n",
       "32  layers and dropout_rates are: [[128, 128, 128]...  2.136301    0.1708   \n",
       "33  layers and dropout_rates are: [[256, 256, 256]...  2.126171    0.1708   \n",
       "40  layers and dropout_rates are: [[1024, 1024, 10...  2.125305    0.1708   \n",
       "38  layers and dropout_rates are: [[512, 512, 512]...  2.128513    0.1708   \n",
       "37  layers and dropout_rates are: [[512, 512, 512]...  2.128409    0.1708   \n",
       "36  layers and dropout_rates are: [[512, 512, 512]...  2.127010    0.1708   \n",
       "35  layers and dropout_rates are: [[256, 256, 256]...  2.128767    0.1708   \n",
       "41  layers and dropout_rates are: [[1024, 1024, 10...  2.127584    0.1708   \n",
       "29  layers and dropout_rates are: [[64, 64, 64], 0.6]  2.157908    0.1706   \n",
       "34  layers and dropout_rates are: [[256, 256, 256]...  2.128071    0.1699   \n",
       "27  layers and dropout_rates are: [[64, 64, 64], 0.2]  2.125990    0.1683   \n",
       "39  layers and dropout_rates are: [[1024, 1024, 10...  2.127458    0.1682   \n",
       "24  layers and dropout_rates are: [[1024, 1024], 0.2]  2.164707    0.1505   \n",
       "30  layers and dropout_rates are: [[128, 128, 128]...  2.209432    0.1383   \n",
       "22    layers and dropout_rates are: [[512, 512], 0.4]  2.235458    0.1381   \n",
       "31  layers and dropout_rates are: [[128, 128, 128]...  2.212012    0.1377   \n",
       "23    layers and dropout_rates are: [[512, 512], 0.6]  2.257675    0.1159   \n",
       "25  layers and dropout_rates are: [[1024, 1024], 0.4]  2.285413    0.1086   \n",
       "28  layers and dropout_rates are: [[64, 64, 64], 0.4]  2.301417    0.0998   \n",
       "\n",
       "   activation                        optimizer  batch_size  \n",
       "0        relu  Adam with learning rate @ 0.001         128  \n",
       "14       relu  Adam with learning rate @ 0.001         128  \n",
       "13       relu  Adam with learning rate @ 0.001         128  \n",
       "4        relu  Adam with learning rate @ 0.001         128  \n",
       "12       relu  Adam with learning rate @ 0.001         128  \n",
       "15       relu  Adam with learning rate @ 0.001         128  \n",
       "10       relu  Adam with learning rate @ 0.001         128  \n",
       "9        relu  Adam with learning rate @ 0.001         128  \n",
       "3        relu  Adam with learning rate @ 0.001         128  \n",
       "6        relu  Adam with learning rate @ 0.001         128  \n",
       "1        relu  Adam with learning rate @ 0.001         128  \n",
       "2        relu  Adam with learning rate @ 0.001         128  \n",
       "11       relu  Adam with learning rate @ 0.001         128  \n",
       "7        relu  Adam with learning rate @ 0.001         128  \n",
       "8        relu  Adam with learning rate @ 0.001         128  \n",
       "5        relu  Adam with learning rate @ 0.001         128  \n",
       "16       relu  Adam with learning rate @ 0.001         128  \n",
       "17       relu  Adam with learning rate @ 0.001         128  \n",
       "18       relu  Adam with learning rate @ 0.001         128  \n",
       "19       relu  Adam with learning rate @ 0.001         128  \n",
       "20       relu  Adam with learning rate @ 0.001         128  \n",
       "21       relu  Adam with learning rate @ 0.001         128  \n",
       "26       relu  Adam with learning rate @ 0.001         128  \n",
       "32       relu  Adam with learning rate @ 0.001         128  \n",
       "33       relu  Adam with learning rate @ 0.001         128  \n",
       "40       relu  Adam with learning rate @ 0.001         128  \n",
       "38       relu  Adam with learning rate @ 0.001         128  \n",
       "37       relu  Adam with learning rate @ 0.001         128  \n",
       "36       relu  Adam with learning rate @ 0.001         128  \n",
       "35       relu  Adam with learning rate @ 0.001         128  \n",
       "41       relu  Adam with learning rate @ 0.001         128  \n",
       "29       relu  Adam with learning rate @ 0.001         128  \n",
       "34       relu  Adam with learning rate @ 0.001         128  \n",
       "27       relu  Adam with learning rate @ 0.001         128  \n",
       "39       relu  Adam with learning rate @ 0.001         128  \n",
       "24       relu  Adam with learning rate @ 0.001         128  \n",
       "30       relu  Adam with learning rate @ 0.001         128  \n",
       "22       relu  Adam with learning rate @ 0.001         128  \n",
       "31       relu  Adam with learning rate @ 0.001         128  \n",
       "23       relu  Adam with learning rate @ 0.001         128  \n",
       "25       relu  Adam with learning rate @ 0.001         128  \n",
       "28       relu  Adam with learning rate @ 0.001         128  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result.rename(columns = {\"Experiment\": \"Config\"}, inplace = True)\n",
    "df_result.sort_values(by = 'Accuracy', axis = 0, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv('exe_config.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
